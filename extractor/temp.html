<p>According to
    <code><a href="https://en.wikipedia.org/wiki/Differential_privacy">https://en.wikipedia.org/wiki/Differential_privacy</a></code>,
    <i>differential privacy (DP)</i> is a system for publicly sharing information about a dataset by describing the
    patterns within groups of individuals within the dataset while withholding information about each individual in the
    dataset. For example, training a machine learning model for medical diagnosis, we would like to have machine
    learning algorithms that do not memorize sensitive information about the training set, such as the specific medical
    histories of individual patients. Differential privacy is a notion that allows quantifying the degree of privacy
    protection provided by an algorithm for the underlying (sensitive) dataset it operates on. Through differential
    privacy, we can design machine learning algorithms that responsibly train models on private data.</p>
<aside>\n<div class="top hr">
        <hr />
    </div>\n<section class="feature3">\n<p id="c06-para-0039"><img alt="" role="presentation" src="images/note.png" />
            You can use both the techniques together, federated learning with differential privacy, to securely train a
            model with PII data sitting in distributed silos.</p> <span aria-label="113" epub:type="pagebreak"
            id="Page_113" role="doc-pagebreak"></span>\n<div class="bottom hr">
            <hr />
        </div>\n</section>\n</aside>