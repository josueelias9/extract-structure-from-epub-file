---
marp: true
theme: default
paginate: true
backgroundColor: #fff
color: #333
---

---

<!-- _class: lead -->
<!-- _paginate: false -->

# Book Summary Presentation

### AI-Generated Book Summary

---

---

<!-- _class: lead -->

## 📚 Table of Contents

1. **Table of Contents** (4 sections)
2. **Official Google Cloud CertifiedProfessional Machine Learning EngineerStudy Guide** (0 sections)
3. **Acknowledgments** (0 sections)
4. **About the Author** (0 sections)
5. **About the Technical Editors** (2 sections)
6. **Introduction** (9 sections)
7. **Chapter 1Framing ML Problems** (7 sections)
8. **Chapter 2Exploring Data and Building Data Pipelines** (11 sections)
9. **Chapter 3Feature Engineering** (9 sections)
10. **Chapter 4Choosing the Right ML Infrastructure** (8 sections)
11. **Chapter 5Architecting ML Solutions** (8 sections)
12. **Chapter 6Building Secure ML Pipelines** (6 sections)
13. **Chapter 7Model Building** (9 sections)
14. **Chapter 8Model Training and Hyperparameter Tuning** (10 sections)
15. **Chapter 9Model Explainability on Vertex AI** (4 sections)
16. **Chapter 10Scaling Models in Production** (9 sections)
17. **Chapter 11Designing ML Training Pipelines** (7 sections)
18. **Chapter 12Model Monitoring, Tracking, and Auditing Metadata** (9 sections)
19. **Chapter 13Maintaining ML Solutions** (8 sections)
20. **Chapter 14BigQuery ML** (9 sections)
21. **AppendixAnswers to Review Questions** (14 sections)
22. **Index** (24 sections)
23. **Online Test Bank** (1 sections)
24. **WILEY END USER LICENSE AGREEMENT** (0 sections)

---

<!-- _class: lead -->

# 1. Table of Contents

---

---

### 1. Summary: Table of Contents

This book covers the Google Cloud Professional Machine Learning Engineer certification, providing guidance on framing machine learning problems, building data pipelines, feature engineering, and architecting scalable ML solutions. It also explores secure ML pipeline design, model training and hyperparameter tuning, model explainability, scaling models in production, and maintaining ML solutions using Vertex AI and BigQuery ML.

---

---

## 1.1. List of Tables

---

---

### 1.1. Summary: List of Tables

Here is a summary of the table of contents in two sentences:

The book covers various machine learning (ML) topics, including data types, classification metrics, outlier detection, AutoML algorithms, and workflow mapping to Google Cloud services, as well as data storage, encryption, and hyperparameter tuning options. It also discusses explainable techniques, feature selection, ML orchestration, and retraining strategies for Vertex AI and other tools such as BigQuery ML, Kubeflow Pipelines, and Cloud Composer.

---

---

## 1.2. List of Illustrations

---

---

### 1.2. Summary: List of Illustrations

Here is a concise two-sentence summary:

This book covers the basics of Machine Learning (ML) and its applications in Google Cloud, including data preparation, model selection, and deployment using TensorFlow, Vertex AI, and Kubeflow Pipelines. It also introduces MLOps, a set of practices that combine ML and software engineering to build, deploy, and manage machine learning models at scale, with a focus on automating the workflow through tools like Vertex AI and Kubeflow.

---

---

## 1.3. Guide

---

---

### 1.3. Summary: Guide

Cover Table of Contents Title Page Copyright Dedication Acknowledgments About the Author About the Technical Editors Introduction Begin Reading Appendix: Answers to Review Questions Index End User License Agreement

---

---

## 1.4. Pages

---

---

### 1.4. Summary: Pages

There is no summary provided for the given content, which appears to be a list of numbers.

---

---

<!-- _class: lead -->

# 2. Official Google Cloud CertifiedProfessional Machine Learning EngineerStudy Guide

---

---

<!-- _class: lead -->

# 3. Acknowledgments

---

---

<!-- _class: lead -->

# 4. About the Author

---

---

<!-- _class: lead -->

# 5. About the Technical Editors

---

---

## 5.1. About the Technical Proofreader

---

---

### 5.1. Summary: About the Technical Proofreader

Adam Vincent is a multi-certified Google Cloud expert who creates educational courses on machine learning and has a passion for automating processes. In his free time, he enjoys gaming, reading, and hiking.

---

---

## 5.2. Google Technical Reviewer

---

---

### 5.2. Summary: Google Technical Reviewer

Wiley and the authors wish to thank the Google Technical Reviewer Emma Freeman for her thorough review of the proofs for this book.

---

---

<!-- _class: lead -->

# 6. Introduction

---

---

## 6.1. Google Cloud Professional Machine Learning Engineer Certification

---

---

### 6.1. Summary: Google Cloud Professional Machine Learning Engineer Certification

A Professional Machine Learning Engineer designs and productionizes machine learning models to solve business challenges using Google Cloud technologies, considering responsible AI throughout the development process. The engineer should be proficient in model architecture, data pipelines, metrics interpretation, and application development, with a focus on creating scalable solutions for optimal performance.

---

---

### 6.1.1. Why Become Professional ML Engineer (PMLE) Certified?

---

---

### 6.1.1. Summary: Why Become Professional ML Engineer (PMLE) Certified?

Obtaining the Professional Machine Learning Engineer (PMLE) certification can significantly increase your marketability and provide proof of professional achievement, as it demonstrates expertise in AI/ML tools on the Google Cloud Platform. The PMLE certification also recognizes Google's leadership in open-source AI and ML, and can raise customer confidence in a certified professional's ability to design secure, cost-effective, and scalable ML solutions.

---

---

### 6.1.2. How to Become Certified

---

---

### 6.1.2. Summary: How to Become Certified

The Google Cloud Professional Solutions Architect exam requires 3+ years of industry experience, including design and management expertise with the platform, and consists of 50-60 multiple-choice questions to be completed in 2 hours. Exam registration options include online proctored or on-site, proctored testing at a nearby center.

---

---

## 6.2. Who Should Buy This Book

---

---

### 6.2. Summary: Who Should Buy This Book

This book provides a comprehensive guide to mastering Machine Learning (ML) on Google Cloud Platform, covering topics from data preparation to deployment and model selection, with a focus on best practices for designing secure ML environments. It serves as a professional-level study guide for the Professional Machine Learning Engineer exam, assuming prior knowledge of Google Cloud basics and machine learning fundamentals.

---

---

## 6.3. How This Book Is Organized

---

---

### 6.3. Summary: How This Book Is Organized

The book consists of 14 chapters covering topics from translating business challenges into machine learning (ML) use cases to designing reliable and scalable ML solutions for production, including model building, training, and deployment, as well as data engineering, security, and monitoring. It also covers BigQuery ML algorithms and provides guidance on choosing the right ML infrastructure and services for various use cases.

---

---

### 6.3.1. Chapter Features

---

---

### 6.3.1. Summary: Chapter Features

The book provides a flexible study plan with objectives listed at the beginning of each chapter, followed by review questions, assessment tests, and exam essentials to help prepare for the Google Cloud certification exam. To get the most out of the book, it's recommended to read chapters from start to finish and then review key concepts and test your understanding with the provided materials.

---

---

## 6.4. Bonus Digital Contents

---

---

### 6.4. Summary: Bonus Digital Contents

The online learning environment includes digital test engine with practice tests, including 30-question assessment test and 100+ review questions per chapter, two bonus exams, and electronic flash cards to review exam objectives. A fully searchable glossary of key terms is also included, providing access to the book's definitions in PDF format.

---

---

### 6.4.1. Interactive Online Learning Environment and Test Bank

---

---

### 6.4.1. Summary: Interactive Online Learning Environment and Test Bank

You can access all these resources at www.wiley.com/go/sybextestprep .

---

---

## 6.5. Conventions Used in This Book

---

---

### 6.5. Summary: Conventions Used in This Book

This book employs various typographic styles to clarify key terms, configuration files, and other technical information, including italicized text for emphasized words, monospaced font for file names and URLs. The book also uses conventions such as notes for peripheral information, tips for saving time or overcoming limitations, to facilitate quick understanding of complex topics.

---

---

## 6.6. Google Cloud Professional ML Engineer Objective Map

---

---

### 6.6. Summary: Google Cloud Professional ML Engineer Objective Map

This book covers the development of low-code machine learning (ML) solutions, including objective chapters on architecting solutions, collaborating within teams, scaling prototypes, serving and scaling models, automating and orchestrating ML pipelines, monitoring ML solutions, exploring data preprocessing, model prototyping using Jupyter notebooks, tracking and running ML experiments, building models, choosing appropriate hardware for training, and determining an appropriate retraining policy.

---

---

## 6.7. How to Contact the Publisher

---

---

### 6.7. Summary: How to Contact the Publisher

Customers can report errors in this book by emailing the Customer Service Team at wileysupport@wiley.com with "Possible Book Errata Submission" as the subject. The team will review and investigate any submitted corrections to ensure accuracy.

---

---

## 6.8. Assessment Test

---

---

### 6.8. Summary: Assessment Test (Part 1/4)

Here are the concise summaries of the provided content: 1. To predict user lifetime value over 30 days without data leakage, create a training set from day 1-29 and validation set from day 30. Use AUC PR and AUC ROC metrics. 2. For an imbalanced dataset, focus on positive class using AUC PR and AUC ROC metrics. 3. Create a feature cross by swapping two or more features. 4. Use Cloud Pub/Sub for data streaming and Cloud Dataflow for data transformation. 5. Recommend BigQuery ML for team preference in SQL coding. 6. Vertex AI managed datasets benefit from integrated data labeling, lineage tracking, and splitting of data into sets. 7. To handle sensitive data, use Cloud DLP or Vision AI. 8. Use L1 regularization to reduce features while avoiding exploding gradients. 9. To avoid exploding

---

---

### 6.8. Summary: Assessment Test (Part 2/4)

gradients, use batch normalization or lower learning rate. 10. For real-time prediction, use Dataproc connectors with Pub/Sub Lite Spark connector or BigQuery Spark connector. 11. Vertex AI Vizier is not for non-ML use cases. 12. Not a tool to track metrics when training neural networks: Vertex AI TensorBoard Profiler. 13. Use XRAI (eXplanation with Ranked Area Integrals) technique to select features with structured datasets. 14. A TensorFlow SavedModel is created when calling `tf.saved_model.save()`. 15. To set up real-time prediction using Vertex AI, import the model, deploy, create an endpoint, and enable monitoring. 16. For tracking lineage in Kubeflow pipelines, use Vertex AI Model Registry or Vertex AI Artifact Registry. 17. Not a recommended way to invoke a Kubeflow pipeline:

---

---

### 6.8. Summary: Assessment Test (Part 3/4)

Directly using BigQuery. 18. To identify pets and landmarks in photos, use Vision AI or Vertex AI AutoML model. 19. For training large models on TPU, use a TPU pod or multiple TPU slices. 20. For predicting lobster age based on size and color, deploy the model on Vertex AI, expose a REST endpoint, enable monitoring, and port to BigQuery for batch prediction. 21. Container logging provides information like time stamp and latency for each request. 22. To scale machine learning models using Vertex AI, automate training using pipelines or move to custom models to match MLOps maturity level. 23. Not a reason to use Vertex AI Feature Store: It extracts features from images and videos but not for all data types. 24. For predicting revenue by agents in BigQuery ML, build an ARIMA+ model and make

---

---

### 6.8. Summary: Assessment Test (Part 4/4)

predictions in BigQuery. 25. To ensure accurate predictions using the built model, deploy as part of CD, run predictions on BigQuery ML, or export the model to Vertex AI for serving. 26. The statement about Vertex AI and BigQuery ML being incorrect is: Vertex AI supports all models trained on BigQuery ML.

---

---

## 6.9. Answers to Assessment Test

---

---

### 6.9. Summary: Answers to Assessment Test (Part 1/4)

Here are the concise summaries of each chapter: A: Use time-based split for time-series data to avoid data leakage and precision-recall curves for imbalanced classes. B: Create feature crosses to combine features and use BigQuery ML for machine learning. A, True: Cloud Pub/Sub and Cloud Dataflow create a pipeline for streaming data and transformation. A: BigQuery ML is used for machine learning with SQL queries. C: Use managed datasets for integrated data labeling, lineage, and automatic features. A: Cloud DLP obscures PII data using mentioned techniques. B: Image masking requires separate tools like Vision AI and Python code. L1 and L2 regularization reduce features and train models, respectively. A, B: Batch normalization and learning rate help prevent exploding gradients. D: Use Cloud

---

---

### 6.9. Summary: Answers to Assessment Test (Part 2/4)

Storage connector to move data directly to BigQuery for Hadoop/Spark jobs. C: Random search algorithms improve performance by using fewer trails. B: Vertex AI Vizier optimizes complex models with many parameters as an independent service. D: Vertex AI hyperparameter tuning is not for tracking metrics, but rather for tuning hyperparameters. A: Sampled Shapley method explains tabular or structured datasets with Explainable AI. C: Feature importance explains the features using a score (importance). A: SavedModel contains a complete TensorFlow program after model training. B, C, A: Import models to Model Registry in Vertex AI and deploy before creating an endpoint. C: Record metadata and artifacts produced by ML system with Vertex ML Metadata. D: Kubeflow pipelines are not compatible with

---

---

### 6.9. Summary: Answers to Assessment Test (Part 3/4)

BigQuery for ETL workloads. C: Vision AI is the easiest approach due to pre-trained models available. A, B, C: TPU cannot be used for this case; Option A is invalid, and Option C is the best option. B: There's no need to port into BigQuery for batch processing; only online prediction requires it. D: Access logging provides stderr and stdout from containers in Hadoop/Spark jobs. B: Option B recognizes the right level of MLOps maturity with automating training pipelines. C: Option C addresses technical nature of problem, but not redundant like Option A. D: Moving to custom models from AutoML doesn't help much here; use Vertex AI Feature Store instead. A: Vertex AI Feature Store deals only with structured data and extracts features from images incorrectly. C: Building a model in BigQuery ML

---

---

### 6.9. Summary: Answers to Assessment Test (Part 4/4)

and predicting in BigQuery is the most efficient method. B, C: Use portability feature between BigQuery and Vertex AI model registry for optimal deployment. D: Packaging models into containers isn't necessary for BigQuery models.

---

---

<!-- _class: lead -->

# 7. Chapter 1Framing ML Problems

---

---

## 7.1. Translating Business Use Cases

---

---

### 7.1. Summary: Translating Business Use Cases

Here is a two-sentence summary:

To successfully implement machine learning (ML) for a specific use case, it's essential to first identify the impact, success criteria, and data available, and then match this with a suitable ML approach and metric; stakeholders' expectations, including business impact, budget, timeline, and data privacy, must be considered to ensure smooth approvals. Next, determine if the problem is solvable using ML and choose an approach that fits the use case, taking into account advancements in relevant technologies such as natural language processing.

---

---

## 7.2. Machine Learning Approaches

---

---

### 7.2. Summary: Machine Learning Approaches

Machine learning has many algorithms and approaches, but some are not perfect for every use case, requiring a deep understanding of the landscape to choose the right method. There are various categories or approaches that solve specific classes of problems, distinguished by data type and prediction type, allowing exam candidates to select the most suitable approach.

---

---

### 7.2.1. Supervised, Unsupervised, and Semi‐supervised Learning

---

---

### 7.2.1. Summary: Supervised, Unsupervised, and Semi‐supervised Learning

Machine learning approaches can be classified into supervised, unsupervised, or a combination of both, depending on the availability of labeled data. Supervised learning involves training models with labeled datasets to make predictions, whereas unsupervised learning algorithms, such as clustering and autoencoders, group and classify data without prior labels, resulting in uncertain outcomes.

---

---

### 7.2.2. Classification, Regression, Forecasting, and Clustering

---

---

### 7.2.2. Summary: Classification, Regression, Forecasting, and Clustering

Classification involves predicting labels or categories from input data, with binary classification having two labels and multiclass classification having more. Regression predicts continuous values, such as house prices or rainfall amounts, using different algorithms than classification models that use structured data like Table 1.2 or time-series data with IID values.

---

---

## 7.3. ML Success Metrics

---

---

### 7.3. Summary: ML Success Metrics

Here is a two-sentence summary:

To evaluate the performance of a machine learning model, various metrics can be used depending on the specific problem type, such as precision, recall, or F1 score. These metrics calculate different aspects of the model's accuracy, with precision measuring correct positive predictions, recall measuring correct positive predictions out of all actual positives, and F1 score being a harmonic mean of both precision and recall to balance false positives and false negatives.

---

---

### 7.3.1. Area Under the Curve Receiver Operating Characteristic (AUC ROC)

---

---

### 7.3.1. Summary: Area Under the Curve Receiver Operating Characteristic (AUC ROC)

The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at different classification thresholds, summarizing the performance of a binary classification model. The ideal point on the ROC curve is the top-left corner with 100% true positive and 0% false positive rates, and AUC is used to compare two models by calculating the area under the curve.

---

---

### 7.3.2. The Area Under the Precision‐Recall (AUC PR) Curve

---

---

### 7.3.2. Summary: The Area Under the Precision‐Recall (AUC PR) Curve

The area under the precision-recall (AUC PR) curve measures the relationship between recall and precision, with a horizontal line at the top indicating optimal performance. The ideal point on the curve has 100% precision and recall, which is rarely achieved in practice due to dataset imbalance, making AUC PR valuable for imbalanced datasets.

---

---

### 7.3.3. Regression

---

---

### 7.3.3. Summary: Regression

MAE and RMSE measure the average absolute difference and squared difference between actual and predicted values, respectively, while RMSLE penalizes under-prediction by using logarithmic scaling. MAPE measures proportional differences and R2 assesses the goodness of fit by comparing labels to predicted values, with higher values indicating better model performance.

---

---

## 7.4. Responsible AI Practices

---

---

### 7.4. Summary: Responsible AI Practices

AI and machine learning solutions must consider fairness, interpretability, privacy, and security to ensure responsible development. By incorporating best practices from software engineering and unique ML considerations, developers can prioritize user experience, detect biases in data and models, and implement techniques to minimize leakage and prevent cyber threats.

---

---

## 7.5. Summary

---

---

### 7.5. Summary: Summary

In this chapter, you learned how to take a business use case and understand the different dimensions to an ask and to frame a machine learning problem statement as a first step.

---

---

## 7.6. Exam Essentials

---

---

### 7.6. Summary: Exam Essentials

Translate business challenges into machine learning problems by understanding specific use cases, problem types, data availability, and desired outcomes. Familiarize yourself with relevant metrics (e.g., precision, recall, RMSE) and algorithms (e.g., regression, classification, forecasting) for different problem types, while adhering to Responsible AI principles and best practices for fairness, interpretability, privacy, and security.

---

---

## 7.7. Review Questions

---

---

### 7.7. Summary: Review Questions (Part 1/3)

Here are concise summaries of each section: 1. When analyzing a potential use case, look for impact, success criteria, algorithm, budget and time frames. 2. Predicting rainfall is a forecasting problem. 3. To detect valid support tickets, use binary classification. 4. Tracking the ball in sports camera products involves image detection and video object tracking. 5. No clustering solution works without a clear policy on document classification. 6. For a house price prediction model, choose RMSE as the metric to avoid extreme errors. 7. Building a plant classification model with equal numbers of varieties 1 and 2 should use accuracy as the primary metric. 8. When detecting hidden cracks in engines using X-ray images, use precision and recall as the metric for your classification model. 9.

---

---

### 7.7. Summary: Review Questions (Part 2/3)

When given unlabeled training data, use unsupervised learning or semi-supervised learning to create a machine learning model. 10. To recommend videos based on user feedback, use collaborative filtering with explicit feedback from users. 11. Creating a pipeline without evaluating the model is a bad idea. 12. Supervised and supervised learning are valid types of machine learning approaches. 13. When building a credit-worthiness prediction model, deploy it after testing for biases in data. 14. Model interpretability/explanations can satisfy the requirement to know why each prediction. 15. The accuracy issue with an Android app's moustache detection model may be due to user variability or insufficient training data. 16. Even if secured behind a firewall and taking cybersecurity precautions,

---

---

### 7.7. Summary: Review Questions (Part 3/3)

sensitive medical data for cancer predictions still poses privacy concerns. 17. When creating a recommendation model based on customer browsing history and shopping cart history, use accuracy as the metric and consider using additional product data in training.

---

---

<!-- _class: lead -->

# 8. Chapter 2Exploring Data and Building Data Pipelines

---

---

## 8.1. Visualization

---

---

### 8.1. Summary: Visualization

Data visualization is a technique used to explore trends and outliers in data, facilitating data cleaning and feature engineering through the use of various chart types such as box plots, distribution plots, line plots, bar plots, and scatterplots. These visualizations enable independent analysis (univariate) or comparison between features (bivariate), helping identify imbalances, correlations, and relationships within the data.

---

---

### 8.1.1. Box Plot

---

---

### 8.1.1. Summary: Box Plot

A box plot displays the distribution of data, representing the 25th, 50th (median), and 75th percentiles, with the body indicating the interquartile range where most observations are present. Outliers are points outside the whiskers, extending from the minimum to maximum values represented by straight lines.

---

---

### 8.1.2. Line Plot

---

---

### 8.1.2. Summary: Line Plot

A line plot plots the relationship between two variables and is used to analyze the trends for data changes over time. Figure 2.2 shows a line plot. FIGURE 2.2 Line plot

---

---

### 8.1.3. Bar Plot

---

---

### 8.1.3. Summary: Bar Plot

A bar plot is used for analyzing trends in data and comparing categorical data such as sales figures every week, the number of visitors to a website, or revenue from a product every month. Figure 2.3 shows a bar plot. FIGURE 2.3 Bar plot

---

---

### 8.1.4. Scatterplot

---

---

### 8.1.4. Summary: Scatterplot

A scatterplot is the most common plot used in data science and is mostly used to visualize clusters in a dataset and show the relationship between two variables.

---

---

## 8.2. Statistics Fundamentals

---

---

### 8.2. Summary: Statistics Fundamentals

In statistics, we have three measures of central tendency: mean, median, and mode. They help us describe the data and can be used to clean data statistically.

---

---

### 8.2.1. Mean

---

---

### 8.2.1. Summary: Mean

Mean is the accurate measure to describe the data when we do not have any outliers present.

---

---

### 8.2.2. Median

---

---

### 8.2.2. Summary: Median

The median is calculated by arranging data values in order and identifying the middle value(s), taking the average if necessary to handle even numbers. For example, a dataset with an outlier may use the median as the measure of central tendency instead of the mean.

---

---

### 8.2.3. Mode

---

---

### 8.2.3. Summary: Mode

Mode is used if there is an outlier and the majority of the data is the same. Mode is the value or values in the dataset that occur most. For example, for the dataset 1, 1, 2, 5, 5, 5, 9, the mode is 5.

---

---

### 8.2.4. Outlier Detection

---

---

### 8.2.4. Summary: Outlier Detection

The mean is sensitive to outliers, which can significantly alter its value compared to the median or mode. Adding an outlier to a dataset can change the mean by as much as 16.44 units, but have little effect on the median and mode values.

---

---

### 8.2.5. Standard Deviation

---

---

### 8.2.5. Summary: Standard Deviation

Standard deviation is the square root of the variance. Standard deviation is an excellent way to identify outliers. Data points that lie more than one standard deviation from the mean can be considered unusual. Covariance is a measure of how much two random variables vary from each other.

---

---

### 8.2.6. Correlation

---

---

### 8.2.6. Summary: Correlation

Correlation is a measure of how closely two variables relate, with values ranging from -1 to +1 indicating positive, negative, or no correlation. High correlation between features and the target variable can cause model bias, such as learning hospital names instead of cancer presence in a prediction model.

---

---

## 8.3. Data Quality and Reliability

---

---

### 8.3. Summary: Data Quality and Reliability

Your model's quality is determined by the reliability and size of its training data, which refers to data with missing values, duplicates, or poor features that can lead to inaccurate predictions if used for training. Ensuring reliable data involves checking for label errors, noise in features, outliers, and skew, as well as establishing a clear definition of data quality while collecting it.

---

---

### 8.3.1. Data Skew

---

---

### 8.3.1. Summary: Data Skew

Data skew occurs when a normal distribution curve is asymmetric due to outliers or uneven data distribution, resulting in skewed statistics such as mean and median. Skewed data can be transformed into a normal distribution through techniques like log transformation and normalization, or addressed by methods like SMOTE for target variable skewness.

---

---

### 8.3.2. Data Cleaning

---

---

### 8.3.2. Summary: Data Cleaning

The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model. (See https://developers.google.com/machine-learning/data-prep/transform/normalization .)

---

---

### 8.3.3. Scaling

---

---

### 8.3.3. Summary: Scaling

Scaling converts floating-point feature values into a standard range (e.g., 0 to 1) to improve gradient descent convergence in deep neural networks and prevent "NaN traps". Scaling is useful when data is uniformly distributed, has no skew, or few outliers, such as age features with evenly represented ranges.

---

---

### 8.3.4. Log Scaling

---

---

### 8.3.4. Summary: Log Scaling

Log scaling is used to scale extremely large or small data samples to a standard range, typically 0-1, by taking the logarithm of each value. This transforms values like 10,000 and 100 into comparable values like 4 and 2, respectively, allowing for more uniform data distribution.

---

---

### 8.3.5. Z‐score

---

---

### 8.3.5. Summary: Z‐score

The scaling method uses z-scores calculated as (value - mean) / stddev, where a value within -3 to +3 standard deviations from the mean is considered normal and outliers are those outside this range. For example, a value of 130 with a mean of 100 and std dev of 20 results in a scaled value of 1.5, indicating it's 1.5 standard deviations above the mean.

---

---

### 8.3.6. Clipping

---

---

### 8.3.6. Summary: Clipping

In the case of extreme outliers, you can cap all feature values above or below to a certain fixed value. You can perform feature clipping before or after other normalization techniques.

---

---

### 8.3.7. Handling Outliers

---

---

### 8.3.7. Summary: Handling Outliers

An outlier in a dataset is an observation that deviates significantly from other data points due to errors or skew, requiring visualization techniques such as box plots and Z-scores to detect. Once identified, outliers can be removed or imputed using various methods, including replacing with mean, median, mode, or boundary values.

---

---

## 8.4. Establishing Data Constraints

---

---

### 8.4. Summary: Establishing Data Constraints

A schema is defined through data analysis to describe key properties of the data, including data type, allowed range, and distribution of values. Having a schema enables metadata-driven preprocessing, feature engineering, and validation, allowing for efficient transformation, detection of anomalies, and catchment of data quality issues during training and prediction.

---

---

### 8.4.1. Exploration and Validation at Big‐Data Scale

---

---

### 8.4.1. Summary: Exploration and Validation at Big‐Data Scale

TensorFlow Data Validation (TFDV) is a tool used for detecting anomalies in ML data, including schema and data anomalies, to ensure consistency across large datasets. TFDV is part of the TensorFlow Extended platform and can be integrated into an ML pipeline to validate and monitor data quality.

---

---

## 8.5. Running TFDV on Google Cloud Platform

---

---

### 8.5. Summary: Running TFDV on Google Cloud Platform

TFDV uses the Apache Beam SDK to build batch and streaming pipelines, which are then executed on Google Cloud's managed Dataflow service that runs at scale and integrates with services like BigQuery and Google Cloud Storage. Dataflow also supports Vertex AI Pipelines for machine learning, allowing users to process data in a scalable and serverless environment.

---

---

## 8.6. Organizing and Optimizing Training Datasets

---

---

### 8.6. Summary: Organizing and Optimizing Training Datasets

The dataset is typically divided into three parts: training (used to learn from), validation (used for hyperparameter tuning and model evaluation), and testing (used to evaluate performance after training). The training and validation sets are separate and do not overlap, while the test set should remain distinct and not be used during training or validation.

---

---

### 8.6.1. Imbalanced Data

---

---

### 8.6.1. Summary: Imbalanced Data

Downsampling a majority class, such as no-fraud credit card transactions, improves dataset balance by reducing its size, while upweighting the resulting samples increases their importance during training, effectively increasing their representation in the model, leading to faster convergence of the model's convergence. Downsampling reduces bias by removing more samples from the majority class and upweighting adds a new form of artificial emphasis on minority examples.

---

---

### 8.6.2. Data Splitting

---

---

### 8.6.2. Summary: Data Splitting

Data can be split randomly, but this causes skew if similar stories are assigned to the same training or testing group due to shared publication dates, resulting in biased results. To address this, data can be split based on temporal factors, such as month or week of publication, to avoid similar story overlap and promote more balanced groups.

---

---

### 8.6.3. Data Splitting Strategy for Online Systems

---

---

### 8.6.3. Summary: Data Splitting Strategy for Online Systems

Splitting data by time is recommended for online systems as it ensures the validation set reflects the lag between training and prediction data, allowing models to generalize effectively. A common approach involves collecting 30 days of data, creating a training set (days 1-29) and a validation set (day 30), which works best with large datasets and clusters where future predictions will not be trained on unseen information.

---

---

## 8.7. Handling Missing Data

---

---

### 8.7. Summary: Handling Missing Data

Handling missing data in a dataset involves methods such as deleting rows or columns, replacing values with mean or median, using imputation or LOCF methods, and utilizing machine learning algorithms like k-NN or Random Forest to predict missing values by analyzing correlations between variables. These methods aim to minimize loss of information while improving model performance on datasets with significant amounts of missing data.

---

---

## 8.8. Data Leakage

---

---

### 8.8. Summary: Data Leakage

Data leakage occurs when test data is included in the training set during model training, causing the model to overfit the actual data instead of generalizing well to new, unseen data. This can be prevented by selecting non-correlated features, splitting data into test, train, and validation sets, pre-processing training and test data separately, and using techniques like cross-validation or normalization.

---

---

## 8.9. Summary

---

---

### 8.9. Summary: Summary

We need to visualize data using techniques like box plots, line plots, and scatterplots to understand statistical fundamentals such as mean, median, mode, and standard deviation, which are crucial in finding outliers and validating data schema. Additionally, we discussed data cleaning and normalization methods, data splitting strategies, handling missing data, and data leakage to improve the quality and accuracy of machine learning pipelines.

---

---

## 8.10. Exam Essentials

---

---

### 8.10. Summary: Exam Essentials

This course covers the fundamentals of data visualization, statistical terms, data quality and reliability, data cleaning and normalizing techniques, data constraints, training data organization, handling missing data, avoiding data leaks, and ensures understanding of TFDV for large-scale validation. It also teaches how to apply various data splitting techniques, sampling strategies, and methods for removing or imputing missing data to prepare high-quality data for machine learning models.

---

---

## 8.11. Review Questions

---

---

### 8.11. Summary: Review Questions (Part 1/2)

Here are the concise summaries: 1. To improve model performance on new patient data, address strong correlation between feature hospital name and predicted result by using techniques like dimensionality reduction or feature selection. 2. Address input data distribution change after six months of deployment by retraining your model after monitoring for skew with alerts or performing hyperparameter tuning to detect data skew. 3. The most likely cause of reduced model accuracy in production is poor quality data, which can be addressed by retraining the model and ensuring proper data split ratios. 4. To resolve class imbalance issues when 1 percent of sensor reading data are positive examples, use techniques like oversampling or downsampling to create more balanced datasets. 5. Improve model

---

---

### 8.11. Summary: Review Questions (Part 2/2)

accuracy on production data by splitting training and test data based on time, normalizing data separately, and adding more data for fair distribution. 6. To overcome difficulty with gradient optimization due to dataset's range and missing values, improve data cleaning, feature construction, normalization, or hyperparameter tuning steps. 7. The reasons for model overfitting in production environment are: applying normalizing features like removing outliers to the entire dataset, high correlation between target variable and feature, or adding target variable as a feature.

---

---

<!-- _class: lead -->

# 9. Chapter 3Feature Engineering

---

---

## 9.1. Consistent Data Preprocessing

---

---

### 9.1. Summary: Consistent Data Preprocessing

Applying transformations to data can be done before model training (Pretraining Data Transformation) or within the model itself (Inside Model Data Transformation). Pretraining approach allows for efficient computation but requires updates at prediction time, while Inside Model approach decouples data and transformation but may increase model latency if complex transforms are used.

---

---

## 9.2. Encoding Structured Data Types

---

---

### 9.2. Summary: Encoding Structured Data Types

A good feature should be related to business objectives, be numeric with magnitude, and have enough examples. Feature engineering categorizes data into two types: categorical data (e.g. yes/no, male/female) and numeric data (e.g. observations, recordings, measurements).

---

---

### 9.2.1. Why Transform Categorical Data?

---

---

### 9.2.1. Summary: Why Transform Categorical Data?

Most machine learning (ML) algorithms operate on numeric data only and require conversion of categorical data to numeric for processing. Categorical data can sometimes be used directly in decision trees, but not in other ML algorithms.

---

---

### 9.2.2. Mapping Numeric Values

---

---

### 9.2.2. Summary: Mapping Numeric Values

Integer and floating‐point data don't need special encoding because they can be multiplied by a numeric weight. You may need to apply two kinds of transformations to numeric data: normalizing and bucketing.

---

---

### 9.2.3. Mapping Categorical Values

---

---

### 9.2.3. Summary: Mapping Categorical Values

There are ways to convert or transform your categorical data into numerical data so that your model can understand it.

---

---

### 9.2.4. Feature Selection

---

---

### 9.2.4. Summary: Feature Selection

Feature selection reduces the number of input variables to improve model performance by reducing noise and overfitting, resulting in increased training time efficiency and improved overall algorithmic performance. Techniques include either selecting the most important features (e.g. backward selection, Random Forest) or finding a combination of new features (e.g. PCA, t-SNE).

---

---

## 9.3. Class Imbalance

---

---

### 9.3. Summary: Class Imbalance

Classification models predict outcomes such as true positives (correctly identify positive classes), true negatives (correctly identify negative classes), false positives (incorrectly identify positive classes), and false negatives (incorrectly identify negative classes). Minimizing false negatives is crucial, as it prevents identifying healthy individuals who are actually sick.

---

---

### 9.3.1. Classification Threshold with Precision and Recall

---

---

### 9.3.1. Summary: Classification Threshold with Precision and Recall

A classification threshold influences the number of false positives and false negatives in a logistic regression model, with higher thresholds reducing false positives but increasing false negatives. Precision and recall are trade-offs between true positives and total positive predictions, where precision is important for minimizing false positives and recall is important for minimizing false negatives.

---

---

### 9.3.2. Area under the Curve (AUC)

---

---

### 9.3.2. Summary: Area under the Curve (AUC)

AUC ROC measures balanced datasets in classification problems with equal examples for both classes, while AUC PR is used for imbalanced datasets like credit card transactions to identify false positives or negatives. AUC PR is particularly relevant when the minority class has significantly fewer examples than the majority class, as seen in fraud detection scenarios.

---

---

## 9.4. Feature Crosses

---

---

### 9.4. Summary: Feature Crosses

In machine learning, a feature cross is created by multiplying two or more features to represent nonlinearity in a linear model. By combining multiple features into one, the predictive ability of a single feature can be increased, especially when dealing with complex models and large datasets.

---

---

## 9.5. TensorFlow Transform

---

---

### 9.5. Summary: TensorFlow Transform

Increasing the performance of a TensorFlow model requires an efficient input pipeline. First we will discuss the TF Data API and then we'll talk about TensorFlow Transform.

---

---

### 9.5.1. TensorFlow Data API (tf.data)

---

---

### 9.5.1. Summary: TensorFlow Data API (tf.data)

A well-designed data input pipeline can improve model execution speed by reducing device idle time through techniques like prefetching, interleaving, caching, and vectorizing user-defined functions to overlap preprocessing and model execution, while also reducing memory usage. Implementing these practices using the tf.data API can significantly boost training efficiency and mitigate data extraction overhead.

---

---

### 9.5.2. TensorFlow Transform

---

---

### 9.5.2. Summary: TensorFlow Transform

The TensorFlow Transform library is part of TensorFlow Extended (TFX), allowing transformations prior to training, which avoids the training-serving skew in Google Cloud. It enables pipeline creation using Cloud Dataflow and BigQuery, including data extraction, transformation, model training, evaluation, validation, and serving for prediction, all within a unified framework.

---

---

## 9.6. GCP Data and ETL Tools

---

---

### 9.6. Summary: GCP Data and ETL Tools

Cloud Data Fusion is a managed service that enables users to build and manage ETL pipelines without requiring any coding or infrastructure management, utilizing Cloud Dataproc for MapReduce and Spark workloads. Dataprep by Trifacta is a serverless intelligent tool that allows users to visually explore, clean, and prepare data for analysis without needing to write code or deploy infrastructure.

---

---

## 9.7. Summary

---

---

### 9.7. Summary: Summary

Feature engineering is crucial for model training and serving, involving techniques such as numerical feature transformation (bucketing, normalization) and categorical feature encoding (linear encoding, one-hot encoding). Additionally, selecting relevant features, handling class imbalance, and leveraging feature crossing are essential considerations in data preprocessing.

---

---

## 9.8. Exam Essentials

---

---

### 9.8. Summary: Exam Essentials

To process and prepare data effectively, one must understand techniques such as feature selection, class imbalance correction, and feature cross to ensure accurate classification results. Additionally, utilizing tools like TensorFlow Transform, Cloud Data Fusion, and Cloud Dataprep can streamline the ETL (extract, transform, load) process for clean and organized data in Google Cloud environments.

---

---

## 9.9. Review Questions

---

---

### 9.9. Summary: Review Questions (Part 1/3)

Here are the concise summaries of each text: 1. To improve performance, prioritize feature engineering by combining strong features using one-hot encoding and oversampling categorical features, and normalize numerical features. 2. For neural network-based projects with missing values, use normalization techniques to help gradient optimization move weights towards optimized solutions. 3. When training an AutoML model for detecting fraudulent transactions, use an optimization objective that maximizes the area under the precision-recall curve (AUC PR) value to balance detection and false positives. 4. To address overfitting in a time-series classification model with high AUC ROC values, perform hyperparameter tuning using cross-validation techniques such as nested or k-fold cross-validation.

---

---

### 9.9. Summary: Review Questions (Part 2/3)

5. When training a ResNet model on Vertex AI using TPUs, use the `interleave` option to read data and set the `prefetch` option equal to the batch size to reduce bottlenecking. 6. To create a dataset for image processing with low latency, convert images into TFRecords, store them in Cloud Storage, and then use the tf.data API to read the images. 7. For predicting housing prices based on city-specific features, use feature crosses such as [binned latitude x binned roomsPerPerson] or [latitude x longitude x roomsPerPerson]. 8. To build a unified analytics environment with cloud-native data integration services, use Cloud Data Fusion for its codeless interface and streamlining of ETL processes. 9. For validating model performance on specific subsets of data before deployment, use k-fold

---

---

### 9.9. Summary: Review Questions (Part 3/3)

cross-validation as the most streamlined, scalable, and reliable approach. 10. To improve the accuracy of a trained model from 99% in training to 66% in production, perform data normalization or remove missing values during model training or using tf.Transform for creating production pipelines.

---

---

<!-- _class: lead -->

# 10. Chapter 4Choosing the Right ML Infrastructure

---

---

## 10.1. Pretrained vs. AutoML vs. Custom Models

---

---

### 10.1. Summary: Pretrained vs. AutoML vs. Custom Models

Pretrained models are widely used AI solutions that provide a quick and easy way to integrate machine learning into applications, with thousands of users and millions of requests processed by a team of engineers. These models can be used without requiring expertise in machine learning, but may not offer the level of customization needed for complex or unique use cases, prompting users to consider AutoML or custom models as alternatives.

---

---

## 10.2. Pretrained Models

---

---

### 10.2. Summary: Pretrained Models

Google Cloud offers a range of pre-trained machine learning models, including Vision AI, Video AI, Natural Language AI, Translation AI, Speech-to-Text, and Text-to-Speech, which can be easily integrated into web consoles or SDKs for immediate use. These models have been trained on large datasets and are frequently retrained by Google's engineering team to provide high-performance solutions for various machine learning problems.

---

---

### 10.2.1. Vision AI

---

---

### 10.2.1. Summary: Vision AI

Vision AI provides a convenient service that allows users to process images and detect objects, faces, and handwriting through optical character recognition, with four types of predictions: object detection, image labels, dominant colors, and "Safe Search" classification which categorizes images as Adult, Spoof, Medical, Violence, or Racy. Users can access the service from their browser at https://cloud.google.com/vision and upload images to analyze in production.

---

---

### 10.2.2. Video AI

---

---

### 10.2.2. Summary: Video AI

This API provides real-time object, place, and action recognition in videos with capabilities to recognize over 20,000 different entities. It enables various use cases such as video recommendation systems, video archives indexing, and improving advertisement relevance by analyzing time-frame-specific labels and video content.

---

---

### 10.2.3. Natural Language AI

---

---

### 10.2.3. Summary: Natural Language AI

The Natural Language AI provides entity extraction, sentiment analysis, syntax analysis, and categorization services to analyze unstructured text using pretrained machine learning models. It offers use cases such as measuring customer sentiment towards products and extracting medical insights from healthcare text to build powerful apps.

---

---

### 10.2.4. Translation AI

---

---

### 10.2.4. Summary: Translation AI

The service uses Google Neural Machine Translation technology to translate over 100 languages, with two levels: Basic and Advanced, offering varying features such as document translation and glossaries. Pricing and functionality differ between the two levels, allowing for text and audio translations in ASCII or UTF-8 format, including real-time streaming capabilities.

---

---

### 10.2.5. Speech‐to‐Text

---

---

### 10.2.5. Summary: Speech‐to‐Text

The Speech-to-Text service converts recorded or streaming audio into text, often used for creating subtitles for videos and translations for multiple languages. This service is offered by Google Cloud.

---

---

### 10.2.6. Text‐to‐Speech

---

---

### 10.2.6. Summary: Text‐to‐Speech

The Text‐to‐Speech service uses state‐of‐the‐art speech synthesis expertise from DeepMind, providing realistic humanlike intonation with 220+ voices across 40+ languages and variants. It enables brands to create unique voices for their touchpoints.

---

---

## 10.3. AutoML

---

---

### 10.3. Summary: AutoML

AutoML automates model training tasks for popular machine learning problems like image classification, text classification, and more by providing an easy-to-use web console or SDK integration, requiring only user-provided data and settings. AutoML is available for various data types and use cases, including structured data, images, natural language, recommendations, and AI/retail applications.

---

---

### 10.3.1. AutoML for Tables or Structured Data

---

---

### 10.3.1. Summary: AutoML for Tables or Structured Data

BigQuery ML and Vertex AI Tables are two methods for training machine learning models, with BigQuery using SQL-based queries and Vertex AI providing Python, Java, or Node.js triggers. Vertex AI AutoML Tables offer various algorithms for data types such as classification, regression, and time-series forecasting, and users can configure settings like budget and early stopping to optimize costs and performance.

---

---

### 10.3.2. AutoML for Images and Video

---

---

### 10.3.2. Summary: AutoML for Images and Video

Vertex AI AutoML provides tools for building machine learning models on image and video data, including algorithms such as single-label classification, multiclass classification, object detection, and segmentation, as well as video classification, action recognition, and object tracking. The AutoML Edge model allows deployment on edge devices with lower memory and latency requirements, supported by iPhones, Android phones, and Edge TPU devices, offering trade-offs between accuracy and latency.

---

---

### 10.3.3. AutoML for Text

---

---

### 10.3.3. Summary: AutoML for Text

Machine learning models for text can be built using Vertex AI AutoML text, solving common problems like text classification and sentiment analysis through auto-magic solutions. AutoML helps with tasks such as text classification, multi-label classification, entity extraction, and text-to-text translation.

---

---

### 10.3.4. Recommendations AI/Retail AI

---

---

### 10.3.4. Summary: Recommendations AI/Retail AI

Google Cloud's Retail AI solution uses AutoML to provide a Google-quality search, image search, and personalized recommendations. These services can be customized by retailers and trained on their product data, allowing customers to create models that fine-tune based on user behavior and updates, with charges applied per 1,000 requests served.

---

---

### 10.3.5. Document AI

---

---

### 10.3.5. Summary: Document AI

Document AI extracts details from various types of documents containing text, tables, and images, such as forms and government documents, by understanding document structure and tolerating handwritten text. The platform consists of processors that perform actions like text extraction, key/value pair identification, and entity normalization, and a Document AI Warehouse for storing, searching, organizing, and analyzing extracted data.

---

---

### 10.3.6. Dialogflow and Contact Center AI

---

---

### 10.3.6. Summary: Dialogflow and Contact Center AI

Dialogflow is a conversational AI offering from Google Cloud that provides chatbots and voicebots. This is integrated into a telephony service and other services to provide you with a Contact Center AI (CCAI) solution. Let us look at the services provided by this solution.

---

---

## 10.4. Custom Training

---

---

### 10.4. Summary: Custom Training

Custom training allows for flexible use of various hardware options, including graphics processing units (GPUs) that accelerate deep learning model training with compute-intensive operations like matrix multiplications. Training on GPUs can reduce completion time to an order of magnitude compared to running on a single CPU, potentially shaving days or weeks off the computation process.

---

---

### 10.4.1. How a CPU Works

---

---

### 10.4.1. Summary: How a CPU Works

A CPU processes data by loading it from memory, performing operations, and storing results back into memory for each operation, making it suitable for general-purpose software applications but computationally inefficient for large-scale computations. This serial processing architecture is particularly limiting when dealing with trillions of calculations required in machine learning model training on large datasets.

---

---

### 10.4.2. GPU

---

---

### 10.4.2. Summary: GPU

A GPU is a specialized chip designed to rapidly process data in memory, originally intended for video games and movies; it works with CPUs to accelerate tasks like matrix multiplications, and can improve speed by an order of magnitude. To use GPUs, specify the type and number of GPUs in WorkerPoolSpec and MachineSpec, considering restrictions such as availability, instance types, and virtual resources.

---

---

### 10.4.3. TPU

---

---

### 10.4.3. Summary: TPU

Google's Tensor Processing Units (TPUs) are specialized hardware accelerators designed for machine learning workloads, addressing the limitations of GPUs by providing multiple matrix multiply units that enable massive parallel processing and acceleration of matrix operations, such as multiplication and accumulation. Each TPU contains thousands of multiply-accumulators connected to form a large physical matrix, allowing TPUs to perform huge operations on huge matrices crucial for neural network training loops.

---

---

## 10.5. Provisioning for Predictions

---

---

### 10.5. Summary: Provisioning for Predictions

The prediction phase involves provisioning resources for continuous online prediction or batch prediction, with online prediction requiring near real-time response times and batch prediction focusing on cost-effectiveness. The key considerations for predicting workload are scaling behavior and selecting the ideal machine type to optimize performance.

---

---

### 10.5.1. Scaling Behavior

---

---

### 10.5.1. Summary: Scaling Behavior

Vertex AI autoscales by adding more prediction nodes when CPU usage is high. When using GPU nodes, monitor all three resource triggers: CPU, memory, and GPU, to ensure effective scaling.

---

---

### 10.5.2. Finding the Ideal Machine Type

---

---

### 10.5.2. Summary: Finding the Ideal Machine Type

Here is a two-sentence summary:

To determine the ideal machine type for a custom prediction container, benchmark its CPU utilization and consider factors such as resource usage, latency, throughput requirements, and price, taking into account potential single-threaded limitations in the web server process. When using GPUs, be aware of restrictions and limitations, including region availability, DeployedModel resource or BatchPredictionJob options, and model compatibility, to deploy models effectively for predictions in the cloud or at the edge devices.

---

---

### 10.5.3. Edge TPU

---

---

### 10.5.3. Summary: Edge TPU

The Google-designed Edge TPU coprocessor accelerates machine learning inference on IoT devices, enabling real-time data processing and decision-making with 4 trillion operations per second on just 2 watts of power. The Edge TPU is available in various form factors for prototyping and production, allowing for deployment of models at the edge of the network.

---

---

### 10.5.4. Deploy to Android or iOS Device

---

---

### 10.5.4. Summary: Deploy to Android or iOS Device

Google's ML Kit provides a powerful and easy-to-use package of machine learning expertise for mobile developers, enabling iOS and Android apps to be more engaging, personalized, and helpful. By training and deploying models locally on the device, ML Kit reduces response times and bandwidth usage, allowing for offline prediction capabilities.

---

---

## 10.6. Summary

---

---

### 10.6. Summary: Summary

Google Cloud offers various pretrained models and AutoML capabilities for different scenarios, while also providing multiple hardware options such as GPUs and TPUs for training and prediction workloads. Additionally, Google Cloud enables deployment to edge devices, expanding model accessibility beyond cloud computing.

---

---

## 10.7. Exam Essentials

---

---

### 10.7. Summary: Exam Essentials

To choose an effective machine learning (ML) approach, consider selecting between pretrained models, AutoML, or custom models based on solution readiness, flexibility, and approach needs. Additionally, provision the right hardware for training and predictions by considering GPU, TPU, cloud, edge, and serverless solutions to ensure scalability and performance.

---

---

## 10.8. Review Questions

---

---

### 10.8. Summary: Review Questions (Part 1/4)

Here are the summaries: 1. To add object detection to photos, use AutoML's Vision AI and combine it with a custom model for better results. 2. For domain-specific terms without labeled data, use Google AutoML Translation to create a new translation model, and then have human translators review each translation (HITL). 3. For video subtitles, use AutoML to create a subtitle job on your existing dataset, then deploy the model using GPUs or TPUs as needed. 4. To build an insect classification app quickly, use AutoML with ML Kit for Android apps, or deploy on Coral.ai devices with edge TPU. 5. When training deep learning models, consider using VMs on Google Cloud with better hardware (e.g., deep learning VM with 1 GPU) and experimenting with different architectures. 6. For recommendations on

---

---

### 10.8. Summary: Review Questions (Part 2/4)

a website, use Recommendations AI's "Recommended for you" model to increase revenue by showing personalized products during checkout. 7. To engage customers more based on their browsing history, use Recommendations AI's "Recommended for you" model without HITL. 8. If there is no HITL data, use Recommendations AI's "Others you may like" or "Frequently bought together" models to suggest related products. 9. When training a deep learning model that takes too long to converge on Google Cloud, try using TPUs instead of GPUs for better performance and speed up training time. 10. To build a custom neural network summary model in Keras, create multiple AutoML jobs with different architectures, compare results, and optimize the training loop. 11. For sentiment analysis, use pre-trained Natural

---

---

### 10.8. Summary: Review Questions (Part 3/4)

Language API to predict sentiment quickly, or try converting Speech-to-Text (STT) into text for more accurate results. 12. When building a custom deep learning model for object tracking in videos, consider using TPUs or GPUs with custom TensorFlow operations. 13. To train models that need 50 GB of memory, use GPU instances like n1-standard-64 with 8 NVIDIA_TESLA_P100. 14. For real-time voice translation on edge devices, deploy the model on Coral.ai devices with Edge TPU or push the pre-trained model to any Android device using ML Kit. 15. When training large deep learning models, consider using TPUs (e.g., TPU Pod) for better performance and speed up training time. 16. To estimate energy usage of houses based on photos, deploy your custom model on a bigger instance (32-core), or use a GPU

---

---

### 10.8. Summary: Review Questions (Part 4/4)

instance with more memory to reduce latency issues.

---

---

<!-- _class: lead -->

# 11. Chapter 5Architecting ML Solutions

---

---

## 11.1. Designing Reliable, Scalable, and Highly Available ML Solutions

---

---

### 11.1. Summary: Designing Reliable, Scalable, and Highly Available ML Solutions

A scalable and reliable ML pipeline requires automation of data collection, transformation, training, tuning, deployment, and monitoring using services like Google Cloud Storage, Dataflow, Vertex AI Training, and Prediction. This can be achieved with services like Vertex AI Pipelines for orchestration, hyperparameter tuning, and experiment tracking, allowing for faster model selection and smoother iteration in the development process.

---

---

## 11.2. Choosing an Appropriate ML Service

---

---

### 11.2. Summary: Choosing an Appropriate ML Service

Google Cloud ML services consist of three layers: top (AI solutions), middle (Vertex AI services, including AutoML and Workbench), and bottom (infrastructure). The services are designed to be easy or difficult to implement, with AI solutions as the most user-friendly and infrastructure requiring manual management.

---

---

## 11.3. Data Collection and Data Management

---

---

### 11.3. Summary: Data Collection and Data Management

Google Cloud provides several data stores to handle your combination of latency, load, throughput, and size requirements for features: Google Cloud Storage BigQuery Vertex AI's datasets to manage training and annotation sets Vertex AI Feature Store NoSQL data store

---

---

### 11.3.1. Google Cloud Storage (GCS)

---

---

### 11.3.1. Summary: Google Cloud Storage (GCS)

Google Cloud Storage allows storing various data types such as images, videos, audio, and unstructured data in a scalable manner. Large files can be split into 100-10,000 shards to improve read and write throughput, especially for frameworks like TensorFlow or Avro.

---

---

### 11.3.2. BigQuery

---

---

### 11.3.2. Summary: BigQuery

Store tabular data in BigQuery for faster speed, and utilize the Google Cloud console, bq command-line tool, or BigQuery REST API to access its functionality. Frameworks such as TensorFlow, Keras, TFX, and Dataflow can also be used with BigQuery through their respective clients and connectors.

---

---

### 11.3.3. Vertex AI Managed Datasets

---

---

### 11.3.3. Summary: Vertex AI Managed Datasets

Google Cloud recommends using Vertex AI managed datasets for custom model training, as they provide centralized management, integrated data labeling, lineage tracking, and performance comparison capabilities without requiring manual data ingestion from storage. Managed datasets offer advantages over custom code, but can be bypassed if more control over data splitting is needed or lineage isn't critical to the application.

---

---

### 11.3.4. Vertex AI Feature Store

---

---

### 11.3.4. Summary: Vertex AI Feature Store

Vertex AI Feature Store is a fully managed repository for organizing, storing, and serving ML features, allowing users to search, retrieve, and export feature values from various sources. By centralizing feature creation and management, the Feature Store simplifies model training, reduces data duplication, detects drifts, and mitigates data skew.

---

---

### 11.3.5. NoSQL Data Store

---

---

### 11.3.5. Summary: NoSQL Data Store

Static reference features are stored in NoSQL databases like Memorystore, Datastore, and Bigtable for low-latency singleton read operations. These databases provide submillisecond latency for retrieval of quickly changing data, making them suitable for use cases such as real-time bidding, fraud detection, and ad prediction that require fast and efficient data access.

---

---

## 11.4. Automation and Orchestration

---

---

### 11.4. Summary: Automation and Orchestration

Machine learning workflows typically include phases such as data collection, model training, evaluation, and deployment, which must be orchestrated and automated through an ML pipeline to integrate into production environments. Kubeflow Pipelines and Vertex AI Pipelines are two alternatives that provide serverless solutions for managing machine learning pipelines, allowing teams to focus on development rather than infrastructure maintenance.

---

---

### 11.4.1. Use Vertex AI Pipelines to Orchestrate the ML Workflow

---

---

### 11.4.1. Summary: Use Vertex AI Pipelines to Orchestrate the ML Workflow

Vertex AI Pipelines is a serverless managed service that automates, monitors, and governs machine learning (ML) systems by orchestrating workflows using Kubeflow Pipelines SDK or TensorFlow Extended, and storing artifacts in Vertex ML Metadata for lineage analysis. This allows users to launch, interact with, and manage pipelines without requiring dedicated infrastructure, enabling efficient experiment tracking and reduced node requirements.

---

---

### 11.4.2. Use Kubeflow Pipelines for Flexible Pipeline Construction

---

---

### 11.4.2. Summary: Use Kubeflow Pipelines for Flexible Pipeline Construction

Kubeflow Pipelines is an open source framework that enables users to compose, orchestrate, and automate machine learning systems through a flexible pipeline design. It supports local, on-premises, or cloud deployments, offering components such as Google Cloud Pipeline Components for integrating Vertex AI functionality like AutoML into pipelines.

---

---

### 11.4.3. Use TensorFlow Extended SDK to Leverage Pre‐built Components for Common Steps

---

---

### 11.4.3. Summary: Use TensorFlow Extended SDK to Leverage Pre‐built Components for Common Steps

TensorFlow provides prebuilt components for common steps, while TFX offers frameworks and libraries for defining, launching, and monitoring machine learning models in production. TensorFlow Extended SDK is recommended when using TensorFlow with large datasets or structured data.

---

---

### 11.4.4. When to Use Which Pipeline

---

---

### 11.4.4. Summary: When to Use Which Pipeline

Vertex AI Pipelines can run Kubeflow Pipelines v1.8.9 or higher or TensorFlow Extended v0.30.0 or higher, while TFX is recommended for large-scale ML workflows processing terabytes of data due to its built-in support and lineage tracking capabilities. Vertex AI Pipelines offers a simpler configuration and monitoring experience compared to other orchestrators like Cloud Composer, making it a better choice for building and managing ML pipelines.

---

---

## 11.5. Serving

---

---

### 11.5. Summary: Serving

After you train, evaluate, and tune a machine learning (ML) model, the model is deployed to production for predictions. An ML model can provide predictions in two ways: offline prediction and online prediction.

---

---

### 11.5.1. Offline or Batch Prediction

---

---

### 11.5.1. Summary: Offline or Batch Prediction

Offline or batch prediction is used for tasks such as recommendations, demand forecasting, and text classification, where data is processed in batches to make predictions using a trained model stored in BigQuery or Google Cloud Storage. Vertex AI's batch prediction allows users to run large-scale predictive jobs with minimal latency.

---

---

### 11.5.2. Online Prediction

---

---

### 11.5.2. Summary: Online Prediction

Here is a concise summary of the content in two sentences:

Online predictions allow for real-time ML model responses, enabling applications such as real-time bidding and sentiment analysis, achieved through synchronous (caller waits for response) or asynchronous (model pushes/polls notifications to end users) methods. To minimize latency, optimize model efficiency by building smaller models or using accelerators, and serve predictions quickly by precomputing results, caching predictions, and utilizing low-latency data stores.

---

---

## 11.6. Summary

---

---

### 11.6. Summary: Summary

This chapter covers best practices for designing reliable ML solutions on Google Cloud Platform, including data collection and management strategies, automation techniques, and serving models in both batch and real-time modes using services like Vertex AI and Kubeflow Pipelines. It also discusses architecture patterns and optimization techniques to improve model latency while serving predictions in real-time.

---

---

## 11.7. Exam Essentials

---

---

### 11.7. Summary: Exam Essentials

Design reliable, scalable, and highly available machine learning (ML) solutions using Google Cloud AI/ML services by choosing the right ML service, understanding data collection and management, and implementing automation and orchestration, while also considering model deployment best practices such as batch vs. real-time prediction. This involves selecting from various data stores, understanding the AI/ML stack, and knowing when to use different pipeline options like Vertex AI Pipelines or Kubeflow/Pipeline.

---

---

## 11.8. Review Questions

---

---

### 11.8. Summary: Review Questions (Part 1/3)

Here are the summaries: 1. For web banner prediction, embed client on website, deploy gateway on App Engine and model on Vertex AI platform for minimal latency. 2. To improve input/output execution performance, load data into Cloud Bigtable and store data in TFRecords. 3. For product recommendation system, use Pub/Sub → Preprocess → ML training/serving → Storage to track results. 4. For customer support email classification, use Kubeflow Pipelines or Vertex AI Platform for distributed training on GKE cluster. 5. To digitize documents with minimal infrastructure efforts, use Google Cloud Storage and Document AI solution. 6. For transportation delay estimation, use Kubeflow Pipelines or BigQuery ML with scheduled query feature to retrain model every month. 7. To design a deep neural network

---

---

### 11.8. Summary: Review Questions (Part 2/3)

in Keras, create multiple models using AutoML Tables, automate training runs using Cloud Composer, and compare evaluation metrics using Kubeflow Pipelines or Vertex AI Platform. 8. For continuous integration and deployment of an ML model, use Google Cloud Storage trigger to send message to Pub/Sub topic, start training job on GKE cluster using Cloud Scheduler. 9. To track experiments with minimal manual effort, use Kubeflow Pipelines or Vertex AI Workbench Notebooks with shared Google Sheets file. 10. For batch prediction of digitized scanned customer forms, use batch prediction functionality of Vertex AI platform and deploy model on Compute Engine for prediction. 11. To store TensorFlow data from block storage to BigQuery, use tf.data.dataset reader for BigQuery or BigQuery I/O Connector.

---

---

### 11.8. Summary: Review Questions (Part 3/3)

12. To access BigQuery data with modified code in TensorFlow and other frameworks, use BigQuery Python Client library or BigQuery I/O Connector. Let me know if you need any further assistance!

---

---

<!-- _class: lead -->

# 12. Chapter 6Building Secure ML Pipelines

---

---

## 12.1. Building Secure ML Systems

---

---

### 12.1. Summary: Building Secure ML Systems

Google Cloud provides built-in security measures, including encryption of stored data (at rest) and encrypted data transmitted between systems (in transit). This helps protect users' and employees' sensitive information from unauthorized access within the cloud environment.

---

---

### 12.1.1. Encryption at Rest

---

---

### 12.1.1. Summary: Encryption at Rest

Google encrypts data stored in Cloud Storage and BigQuery tables with customer-managed or default encryption keys, with optional use of Authenticated Encryption with Associated Data (AEAD) encryption functions in BigQuery. The cloud storage system also supports server-side encryption with two types of hashes (CRC32C and MD5) to check data integrity and client-side encryption where the user is responsible for managing encryption keys and cryptographic operations.

---

---

### 12.1.2. Encryption in Transit

---

---

### 12.1.2. Summary: Encryption in Transit

To protect your data as it travels over the Internet during read and write operations, Google Cloud uses Transport Layer Security (TLS).

---

---

### 12.1.3. Encryption in Use

---

---

### 12.1.3. Summary: Encryption in Use

Encryption in use protects data from compromise or exfiltration by encrypting it while being processed, such as with Confidential Computing that ensures data is encrypted in memory. This can be achieved using Confidential VMs and GKE Nodes, which provide secure data storage during processing.

---

---

## 12.2. Identity and Access Management

---

---

### 12.2. Summary: Identity and Access Management

Vertex AI uses Identity and Access Management (IAM) to manage access to its resources at both project and resource levels, allowing granular control over user permissions. Predefined roles such as Vertex AI Administrator and Vertex AI User provide basic permissions, while custom roles can be created with specific permissions for more advanced access control.

---

---

### 12.2.1. IAM Permissions for Vertex AI Workbench

---

---

### 12.2.1. Summary: IAM Permissions for Vertex AI Workbench

Vertex AI Workbench is a data science service offered by Google Cloud Platform that leverages JupyterLab to explore and access data, offering two types of notebook instances: user-managed and managed, each with varying levels of customization and security controls. Users can set up single-user or service account-based access modes for both types of notebooks using the Google Cloud CLI or environment variables.

---

---

### 12.2.2. Securing a Network with Vertex AI

---

---

### 12.2.2. Summary: Securing a Network with Vertex AI

Google's shared responsibility model places responsibility for securing data and workloads with cloud users, while the shared fate model aims to foster a partnership between cloud providers and customers to continuously improve security through shared best practices like secure infrastructure as code deployment and risk protection programs. Understanding these models is crucial for securing Vertex AI environments, including Workbench notebook settings, public vs. private endpoints, and training jobs.

---

---

## 12.3. Privacy Implications of Data Usage and Collection

---

---

### 12.3. Summary: Privacy Implications of Data Usage and Collection

Google Cloud recommends strategies for handling personally identifiable information (PII) and protected health information (PHI), including sensitive data protection measures, to ensure compliance with regulations such as HIPAA's Privacy Rule. PII includes individual identifying details like name, address, Social Security number, and financial information, while PHI refers to healthcare-related information that requires federal protections for patient care purposes.

---

---

### 12.3.1. Google Cloud Data Loss Prevention

---

---

### 12.3.1. Summary: Google Cloud Data Loss Prevention

The Google Cloud Data Loss Prevention (DLP) API removes identifying information from sensitive data, such as personally identifiable information (PII), by masking, deleting, or obscuring it using techniques like encryption, tokenization, and bucketing. The API can also analyze data to identify risk levels and provide templates for configuring DLP jobs to de-identify and store sensitive data in various formats.

---

---

### 12.3.2. Google Cloud Healthcare API for PHI Identification

---

---

### 12.3.2. Summary: Google Cloud Healthcare API for PHI Identification

The Google Cloud Healthcare API offers a de-identification operation to remove protected health information (PHI) from various data formats, including text, images, and FHIR data, according to HIPAA's 18 identifiers. This de-identified data is no longer considered PHI under the HIPAA Privacy Rule, allowing its use or disclosure without restrictions.

---

---

### 12.3.3. Best Practices for Removing Sensitive Data

---

---

### 12.3.3. Summary: Best Practices for Removing Sensitive Data

Here is a concise summary:

To handle sensitive data in datasets, strategies such as restricting access to specific columns, using Cloud DLP for unstructured content, and coarsening IP addresses, numeric quantities, and zip codes can be employed to protect individual privacy while maintaining model training benefits. Techniques like Principal Component Analysis (PCA) and coarsening can also be used to reduce granularity of data, making it more difficult to identify sensitive information without revealing the underlying formula.

---

---

## 12.4. Summary

---

---

### 12.4. Summary: Summary

This chapter covers security best practices for managing machine learning data in Google Cloud, including encryption, access management with IAM, and techniques like federated learning and differential privacy to protect sensitive information such as PII and PHI data. It also discusses the use of Cloud DLP and Healthcare APIs to manage PII and PHI data, including an architecture pattern for scaling large dataset processing.

---

---

## 12.5. Exam Essentials

---

---

### 12.5. Summary: Exam Essentials

To build secure machine learning systems on Google Cloud, understand encryption at rest and in transit, as well as IAM roles for managing Vertex AI Workbench and network security, while also being aware of concepts like differential privacy and federated learning to address data privacy concerns. Familiarize yourself with tools such as the DLP API and Healthcare API to identify and mask sensitive PII and PHI type data, and apply best practices for removing sensitive information from datasets.

---

---

## 12.6. Review Questions

---

---

### 12.6. Summary: Review Questions (Part 1/2)

Here are the concise summaries: 1. For secure fingerprint authentication, recommend Federated learning as it protects sensitive information without sharing it with the server. 2. To manage resources in Vertex AI Workbench, set up restrictive IAM permissions and separate projects for each data scientist to ensure scalability and security. 3. Use Format-preserving encryption or tokenization to replace sensitive PII with surrogate characters in your dataset. 4. For access to a Vertex AI Python library on Google Colab Jupyter Notebook, choose Create a service account key, Set the environment variable named GOOGLE_APPLICATION_CREDENTIALS, or Give your service account the Vertex AI user role. 5. To manage access to data containing PII, use Network firewall and Cloud DLP to control who has

---

---

### 12.6. Summary: Review Questions (Part 2/2)

access and scan for sensitive information. 6. For least-managed and cost-effective management of Vertex AI Workbench instances, use Vertex AI–managed notebooks. 7. Use Cloud DLP to remove PHI from your FHIR data before building a text classification model to detect patient notes. 8. To ensure PII is not accessible by unauthorized individuals while streaming files to GCP, periodically conduct bulk scans using the Cloud Data Loss Prevention (DLP) API while writing data to BigQuery or Google Cloud Storage buckets.

---

---

<!-- _class: lead -->

# 13. Chapter 7Model Building

---

---

## 13.1. Choice of Framework and Model Parallelism

---

---

### 13.1. Summary: Choice of Framework and Model Parallelism

As deep learning models grow in complexity, the number of parameters and required datasets also increases, necessitating distributed training methods such as multinode training. Data parallelism and model parallelism are common techniques used to speed up training by dividing tasks across multiple nodes or partitions of the model.

---

---

### 13.1.1. Data Parallelism

---

---

### 13.1.1. Summary: Data Parallelism

Data parallelism involves splitting a dataset into parts and processing them on multiple GPUs, with the same parameters applied to each node for forward propagation, and gradients computed and sent back to the main node in batches. To maintain a smooth training process with many nodes, the learning rate is typically reduced in data parallelism.

---

---

### 13.1.2. Model Parallelism

---

---

### 13.1.2. Summary: Model Parallelism

Model parallelism is a technique where a single large model is partitioned across multiple GPUs, allowing it to be trained without fitting into a single GPU's memory. This approach enables the training of complex deep learning models by dividing them across multiple GPUs, overcoming memory limitations associated with individual GPUs.

---

---

## 13.2. Modeling Techniques

---

---

### 13.2. Summary: Modeling Techniques

Let's go over some basic terminology in neural networks that you might see in exam questions.

---

---

### 13.2.1. Artificial Neural Network

---

---

### 13.2.1. Summary: Artificial Neural Network

Artificial neural networks (ANNs) with one hidden layer, like feedforward neural networks, are primarily used for supervised learning of numerical and structured data, typically in regression problems. They process input data sequentially without feedback loops, relying on the linear transformation of inputs to produce outputs.

---

---

### 13.2.2. Deep Neural Network (DNN)

---

---

### 13.2.2. Summary: Deep Neural Network (DNN)

Deep neural networks (DNNs) are artificial neural networks that add depth by increasing the number of hidden layers between the input and output layers, typically with at least two layers. A DNN is considered a subset of artificial neural networks with multiple hidden layers, also referred to as deep nets.

---

---

### 13.2.3. Convolutional Neural Network

---

---

### 13.2.3. Summary: Convolutional Neural Network

Convolutional neural networks (CNNs) are a type of DNN network designed for image input. CNNs are most well‐suited to image classification tasks, although they can be used on a wide array of tasks that take images as input.

---

---

### 13.2.4. Recurrent Neural Network

---

---

### 13.2.4. Summary: Recurrent Neural Network

Recurrent Neural Networks (RNNs) are designed for processing sequences of data and have shown effectiveness in natural language processing, time-series forecasting, and speech recognition. LSTMs, a popular type of RNN, use stochastic gradient descent with a chosen loss function to minimize prediction errors and update weights to achieve low average loss across examples.

---

---

### 13.2.5. What Loss Function to Use

---

---

### 13.2.5. Summary: What Loss Function to Use

The choice of loss function in a neural network is closely tied to the output layer's activation function, with common losses used for regression problems being Mean Squared Error (MSE) and for binary classification problems being Binary Cross-Entropy. For multiclass classification, Sparse Categorical Cross-Entropy is typically used when classes are mutually exclusive, while Categorical Cross-Entropy can be used with non-mutually exclusive labels.

---

---

### 13.2.6. Gradient Descent

---

---

### 13.2.6. Summary: Gradient Descent

The gradient descent algorithm calculates the slope of the loss curve at the starting point, which indicates the direction of the steepest increase, and then moves in the opposite direction (negative gradient) to minimize loss. This movement results in a step that reduces the magnitude of the gradient, thereby decreasing the loss function.

---

---

### 13.2.7. Learning Rate

---

---

### 13.2.7. Summary: Learning Rate

Gradient descent algorithms adjust the current point by multiplying the gradient vector by a scalar learning rate, resulting in a step size of 2.5 times smaller than the original change. This multiplication determines the magnitude of each subsequent point in the optimization process.

---

---

### 13.2.8. Batch

---

---

### 13.2.8. Summary: Batch

Gradient descent uses a "batch" of examples to calculate the gradient in one iteration, with too many examples potentially slowing down computation. Typically, only a subset of the entire dataset is used as a batch for efficient optimization.

---

---

### 13.2.9. Batch Size

---

---

### 13.2.9. Summary: Batch Size

Batch size is the number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini‐batch is usually between 10 and 1,000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.

---

---

### 13.2.10. Epoch

---

---

### 13.2.10. Summary: Epoch

An epoch means an iteration for training the neural network with all the training data. In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass. An epoch is made up of one or more batches.

---

---

### 13.2.11. Hyperparameters

---

---

### 13.2.11. Summary: Hyperparameters

Hyperparameters such as loss function, learning rate, and batch size significantly impact machine learning model training. Choosing an optimal learning rate and batch size balances computational efficiency with learning speed, allowing for efficient training of ML models.

---

---

## 13.3. Transfer Learning

---

---

### 13.3. Summary: Transfer Learning

Transfer learning in machine learning involves storing knowledge gained from solving one problem and applying it to a related problem. It achieves this by adapting a pre-trained neural network model's layers to solve a new problem, saving time or improving performance, especially when limited data is available.

---

---

## 13.4. Semi‐supervised Learning

---

---

### 13.4. Summary: Semi‐supervised Learning

Semi-supervised learning combines small amounts of labeled data with large amounts of unlabeled data for machine learning, falling between unsupervised and supervised learning approaches. This method uses a mix of labeled and unlabeled examples during training to improve model performance.

---

---

### 13.4.1. When You Need Semi‐supervised Learning

---

---

### 13.4.1. Summary: When You Need Semi‐supervised Learning

Semi-supervised learning involves using labeled data and unlabeled data to improve model accuracy when limited resources are available. By retraining a model with labeled data, semi-supervised techniques can increase the size of the training dataset, leading to more accurate outcomes in applications such as fraud detection and speech recognition.

---

---

### 13.4.2. Limitations of SSL

---

---

### 13.4.2. Summary: Limitations of SSL

With a minimal amount of labeled data and plenty of unlabeled data, semi‐supervised learning shows promising results in classification tasks. But it doesn't mean that semi‐supervised learning is applicable to all tasks. If the portion of labeled data isn't representative of the entire distribution, the approach may fall short.

---

---

## 13.5. Data Augmentation

---

---

### 13.5. Summary: Data Augmentation

Neural networks require large amounts of data examples to perform well, but this can be difficult to obtain, so data augmentation techniques like flipping, translating, or rotating images are used to synthetically create more data. Data augmentation can also help increase the amount of relevant data in a dataset, even with a large initial amount, and can be applied in two ways: offline augmentation before training and online augmentation during training.

---

---

### 13.5.1. Offline Augmentation

---

---

### 13.5.1. Summary: Offline Augmentation

Offline augmentation involves applying transformations to the existing data before training, resulting in a larger dataset with minimal additional data collection. This approach increases dataset size proportionally to the number of transformations applied, making it suitable for smaller datasets.

---

---

### 13.5.2. Online Augmentation

---

---

### 13.5.2. Summary: Online Augmentation

Online augmentation involves applying data transformation techniques to mini-batches before feeding them into a machine learning model. This method, also known as augmentation on the fly, is preferred for large datasets and can include techniques such as flipping, rotating, scaling, adding noise, and transfer learning.

---

---

## 13.6. Model Generalization and Strategies to Handle Overfitting and Underfitting

---

---

### 13.6. Summary: Model Generalization and Strategies to Handle Overfitting and Underfitting

Neural network models are evaluated using two key metrics: bias and variance. High bias means a model oversimplifies data, performing poorly on both training and testing data, while high variance indicates it pays too much attention to training data, resulting in good performance but poor generalization.

---

---

### 13.6.1. Bias Variance Trade‐Off

---

---

### 13.6.1. Summary: Bias Variance Trade‐Off

A model's balance between bias and variance is crucial, with too little complexity resulting in high bias and low variance, while too much complexity leads to high variance and low bias. Increasing the capacity of a network can address underfitting, but specialized techniques are needed to prevent overfitting.

---

---

### 13.6.2. Underfitting

---

---

### 13.6.2. Summary: Underfitting

An underfit model fails to learn the problem and performs poorly due to high bias and low variance, resulting in poor performance on test datasets. To reduce underfitting, increasing model complexity, removing noise from the data, and using feature engineering can help improve its ability to generalize to unseen examples.

---

---

### 13.6.3. Overfitting

---

---

### 13.6.3. Summary: Overfitting

An overfit model has low bias and high variance, leading to performance issues with new examples or added statistical noise. To address overfitting, techniques such as regularization, dropout, early stopping, data augmentation, cross-validation can be employed to reduce network complexity and prevent excessive learning of training data.

---

---

### 13.6.4. Regularization

---

---

### 13.6.4. Summary: Regularization

Regularization techniques prevent overfitting by shrinking learned estimates, such as L1 regularization reducing features and L2 regularization stabilizing models. L1 is robust to outliers, helps with feature selection, but doesn't perform it; while L2 improves generalization in linear models, isn't robust to outliers.

---

---

## 13.7. Summary

---

---

### 13.7. Summary: Summary

This chapter covers key aspects of training neural networks with TensorFlow, including loss functions, hyperparameters, transfer learning, semi-supervised learning, data augmentation, model underfitting, and regularization techniques to improve model performance and adapt to different dataset sizes. Topics include choosing optimal models, adjusting hyperparameters such as learning rate and batch size, and using various techniques like online and offline augmentation to enhance training results.

---

---

## 13.8. Exam Essentials

---

---

### 13.8. Summary: Exam Essentials

Use model parallelism or data parallelism for multinode training of large neural networks with distributed training strategies in TensorFlow, also optimize hyperparameters like learning rate, batch size, and epoch through tuning methods such as grid search or Bayesian optimization. Utilize transfer learning, semi-supervised learning, data augmentation, regularization techniques like L1 and L2 regularization to handle underfitting and overfitting issues, and balance bias-variance trade-off while generalizing model performance.

---

---

## 13.9. Review Questions

---

---

### 13.9. Summary: Review Questions (Part 1/2)

Here are the concise summaries: 1. Model performing poorly due to input data changes: Retrain model with fewer features or optimize regularization and dropout parameters. 2. Overfitting in a neural network: Optimize learning rate, decrease number of neurons, or apply L2 regularization. 3. Out of memory error during training: Reduce batch size, change optimizer, or reduce image shape. 4. Class imbalance problem: Use categorical cross-entropy loss function with a class weight adjustment. 5. Oscillation in the loss function: Increase the learning rate hyperparameter or decrease the size of the training batch. 6. Monitoring model performance over time: Compare loss performance on validation data and use Continuous Evaluation feature for ROC curve analysis. 7. Long training time without

---

---

### 13.9. Summary: Review Questions (Part 2/2)

significant accuracy drop: Modify epochs parameter, increase batch size, or adjust learning rate. 8. One-hot encoding for categorical features: Use sigmoid activation function to match output range. 9. Difficulty in optimizing model parameters: Struggle with hyperparameter tuning, regularization, and transformer limitations. 10. Using a single GPU machine: Optimize tf distribution strategy using MirroredStrategy or MultiWorkerMiraded Strategy.

---

---

<!-- _class: lead -->

# 14. Chapter 8Model Training and Hyperparameter Tuning

---

---

## 14.1. Ingestion of Various File Types into Training

---

---

### 14.1. Summary: Ingestion of Various File Types into Training

Data for training can be structured (tables, CSV files), semi-structured (PDFs, JSON files) or unstructured (chats, emails, images, videos), and come in batch or real-time formats, ranging from small to petabyte scales, before being cleaned and transformed for machine learning training. The Google Cloud analytics portfolio provides tools to collect, store, process, and analyze this data, offering a comprehensive solution for data management.

---

---

### 14.1.1. Collect

---

---

### 14.1.1. Summary: Collect

Google Cloud services offer Pub/Sub for real-time streaming and messaging, Pub/Sub Lite for cost-optimized workloads, and Datastream for serverless CDC and replication between databases and applications. The BigQuery Data Transfer Service can load data from various sources, including external cloud storage, SaaS apps, and data warehouses, into BigQuery on a regular basis.

---

---

### 14.1.2. Process

---

---

### 14.1.2. Summary: Process

Once you have collected the data from various sources, you need tools to process or transform the data before it is ready for ML training. The following sections cover some of the tools that can help.

---

---

### 14.1.3. Store and Analyze

---

---

### 14.1.3. Summary: Store and Analyze

Google Cloud Storage offers various options for machine learning data storage, including BigQuery ML, Vertex Data Labeling, and Google Cloud Storage, which can store tabular, unstructured, and image/video data, with recommended sharding sizes of at least 100 MB to improve read and write throughput. For TensorFlow workloads, data is stored as sharded TFRecord files or Avro files in Google Cloud Storage, or managed using TF I/O for Parquet format storage.

---

---

## 14.2. Developing Models in Vertex AI Workbench by Using Common Frameworks

---

---

### 14.2. Summary: Developing Models in Vertex AI Workbench by Using Common Frameworks

Here is a two-sentence summary:

Vertex AI Workbench is a Jupyter Notebook-based development environment that allows users to train, tune, and deploy models using Google Cloud services. Users can choose between managed notebooks, which offer features like automatic shutdown and integration with Cloud Storage, or user-managed notebooks, which provide more control but fewer features, including VPC service controls for networking and security.

---

---

### 14.2.1. Creating a Managed Notebook

---

---

### 14.2.1. Summary: Creating a Managed Notebook

Create a managed notebook by enabling all APIs in Vertex AI, clicking "New Notebook", and selecting the default settings. The notebook comes with a monthly billing estimate, can be upgraded manually to preserve data on the data disk while replacing the boot disk with a new image.

---

---

### 14.2.2. Exploring Managed JupyterLab Features

---

---

### 14.2.2. Summary: Exploring Managed JupyterLab Features

You are redirected to a JupyterLab screen displaying available frameworks, including Serverless Spark and PySpark, allowing you to run a Dataproc cluster within the notebook for data processing. The environment also comes with a terminal option to execute commands on the entire notebook, along with existing tutorials to help get started with building models.

---

---

### 14.2.3. Data Integration

---

---

### 14.2.3. Summary: Data Integration

Click the Browse GCS icon on the left navigation bar ( Figure 8.7 ) to browse and load data from cloud storage folders. FIGURE 8.7 Data integration with Google Cloud Storage within a managed notebook

---

---

### 14.2.4. BigQuery Integration

---

---

### 14.2.4. Summary: BigQuery Integration

Click the BigQuery icon on the left as shown in Figure 8.8 to get data from your BigQuery tables. The interface also has an Open SQL editor option to query these tables without leaving the JupyterLab interface. FIGURE 8.8 Data Integration with BigQuery within a managed notebook

---

---

### 14.2.5. Ability to Scale the Compute Up or Down

---

---

### 14.2.5. Summary: Ability to Scale the Compute Up or Down

Click n1‐standard‐4 (see Figure 8.9 ). You will get the option to modify the hardware of the Jupyter environment. You can also attach a GPU to this instance without leaving the environment.

---

---

### 14.2.6. Git Integration for Team Collaboration

---

---

### 14.2.6. Summary: Git Integration for Team Collaboration

To integrate an existing Git repository or start a new one, click the left navigation branch icon or run the command "git clone <your‐repository name>" in the terminal to access collaboration options. Alternatively, scale up hardware from a managed notebook by cloning a repository using the same method.

---

---

### 14.2.7. Schedule or Execute a Notebook Code

---

---

### 14.2.7. Summary: Schedule or Execute a Notebook Code

To execute code manually in Jupyter Notebook, click "Run" and to automate execution, use "Execute", which allows submitting notebooks to Executor for setting up Vertex AI training jobs or deploying Prediction endpoints without leaving the interface. This functionality also enables scheduling notebook runs by selecting Type options, allowing one-time or scheduled execution.

---

---

### 14.2.8. Creating a User‐Managed Notebook

---

---

### 14.2.8. Summary: Creating a User‐Managed Notebook

User-managed notebooks can be created by choosing a framework during creation, with TensorFlow being an option, allowing for advanced networking configurations and git integration with terminal access. These notebooks can trigger Vertex AI training without requiring large hardware or compute instances, using APIs or SDKs that create containers outside the JupyterLab environment.

---

---

## 14.3. Training a Model as a Job in Different Environments

---

---

### 14.3. Summary: Training a Model as a Job in Different Environments

Vertex AI supports two training types: AutoML, which automates model creation and training with minimal technical effort, and Custom Training, where users have full control over creating a tailored application for their specific objective. The latter allows customization of objectives, algorithms, loss functions, and more, enabling optimized training for targeted outcomes.

---

---

### 14.3.1. Training Workflow with Vertex AI

---

---

### 14.3.1. Summary: Training Workflow with Vertex AI

Vertex AI offers two options for creating training jobs: Training Pipelines, which orchestrate custom training and hyperparameter tuning, and Custom Jobs, which specify settings for running custom-trained code. Both are used exclusively for custom-trained models, while AutoML models rely on other methods to train their models.

---

---

### 14.3.2. Training Dataset Options in Vertex AI

---

---

### 14.3.2. Summary: Training Dataset Options in Vertex AI

In Vertex AI, you can use datasets stored in Google Cloud Storage or BigQuery for training, with the option to store them as a managed dataset for easier management and tracking of lineage and governance. This allows for features such as automatic data splitting, label creation, and task-based human labeling, providing a centralized location for managing machine learning model training.

---

---

### 14.3.3. Pre‐built Containers

---

---

### 14.3.3. Summary: Pre‐built Containers

Vertex AI supports pre-built training with scikit-learn, TensorFlow, PyTorch, and XGBoost containers, managed by Google. To set up a pre-built container job, upload your Python source distribution to Cloud Storage, then use the gcloud command to build a Docker image, push it to Container Registry, and create a custom job specifying region, machine type, replica count, executor image URI, working directory, and script path.

---

---

### 14.3.4. Custom Containers

---

---

### 14.3.4. Summary: Custom Containers

Here is a concise summary of the content in two sentences:

To use a custom container with Vertex AI, create a Dockerfile and push it to an Artifact Registry, allowing you to use your chosen ML framework and access unsupported libraries. By running a training job in a custom container, you can utilize ML frameworks, non-ML dependencies, and binaries not supported on Vertex AI, providing extended support for distributed training.

---

---

### 14.3.5. Distributed Training

---

---

### 14.3.5. Summary: Distributed Training

Here is a concise summary of the content in two sentences:

To run distributed training with Vertex, specify multiple machines (nodes) and define custom worker pools to allocate resources for replica groups. A typical worker pool configuration includes tasks such as primary, secondary, parameter servers, and evaluators, which can be used with frameworks like TensorFlow or PyTorch, and can also utilize Reduction Server to increase throughput and reduce latency during distributed training.

---

---

## 14.4. Hyperparameter Tuning

---

---

### 14.4. Summary: Hyperparameter Tuning

Hyperparameters are settings of the training algorithm that are not learned during training, such as learning rate in gradient descent. Choosing optimal hyperparameters involves trial and error, where the right value is determined through training, evaluation, and adjustment.

---

---

### 14.4.1. Why Hyperparameters Are Important

---

---

### 14.4.1. Summary: Why Hyperparameters Are Important

Here is a concise summary of the content:

Hyperparameter selection is crucial for neural network success and involves solving two problems: efficiently searching the hyperparameter space using algorithms like grid search, random search, or Bayesian optimization, and managing large numbers of experiments to find optimal values. Hyperparameter tuning uses algorithms like Gaussian Process Bandits and can leverage Google Cloud's compute infrastructure to test different configurations and maximize predictive accuracy.

---

---

### 14.4.2. Techniques to Speed Up Hyperparameter Optimization

---

---

### 14.4.2. Summary: Techniques to Speed Up Hyperparameter Optimization

Hyperparameter optimization can be sped up by using a simple validation set instead of cross-validation and parallelizing the problem across multiple machines, resulting in speedups of ~k and ~n for datasets and machines, respectively. Additionally, pre-computing results, decreasing the number of hyperparameter values to consider, and using algorithms like random search can also significantly improve performance.

---

---

### 14.4.3. How Vertex AI Hyperparameter Tuning Works

---

---

### 14.4.3. Summary: How Vertex AI Hyperparameter Tuning Works

Hyperparameter tuning optimizes target variables, called hyperparameter metrics, by running multiple trials of a training application with adjusted values within specified limits. To configure hyperparameter tuning jobs using gcloud CLI commands, create a YAML file defining the metric to optimize, hyperparameters to tune, and desired range for those parameters, then run the `gcloud ai hp-tuning-jobs create` command to start the job.

---

---

### 14.4.4. Vertex AI Vizier

---

---

### 14.4.4. Summary: Vertex AI Vizier

Vertex AI Vizier is a black-box optimization service that helps tune hyperparameters in complex ML models, allowing for objective function-free evaluation due to system complexity, which can also be used for model parameter tuning and other optimizations. The service is suitable for various use cases, including optimizing neural network parameters, user interface elements, computing resources, and even recipe ingredient proportions.

---

---

## 14.5. Tracking Metrics During Training

---

---

### 14.5. Summary: Tracking Metrics During Training

In the following sections, we will cover how you can track and debug machine learning model metrics by using tools such as an interactive shell, the TensorFlow Profiler, and the What‐If Tool.

---

---

### 14.5.1. Interactive Shell

---

---

### 14.5.1. Summary: Interactive Shell

Here is a 2-sentence summary:

You can debug and inspect your Vertex AI training code by using an interactive shell, which allows you to browse file systems, run debugging utilities, and check permissions; however, this access is only available while the job is in the RUNNING state, after which you will lose access to the shell. Additional tools such as py-spy, Perf, and nvidia-smi are also available to track metrics, profile GPU usage, and analyze performance of your training node.

---

---

### 14.5.2. TensorFlow Profiler

---

---

### 14.5.2. Summary: TensorFlow Profiler

Vertex AI TensorBoard Profiler helps monitor and optimize model training performance by visualizing resource consumption of training operations, identifying performance bottlenecks to train models faster and cheaper. To access the Profiler dashboard, navigate to the custom jobs page or experiments page in the Google Cloud console, where you can capture profiling sessions on running training jobs to visualize results.

---

---

### 14.5.3. What‐If Tool

---

---

### 14.5.3. Summary: What‐If Tool

You can use the What-If Tool (WIT) in AI Platform Prediction models to inspect and visualize their behavior through an interactive dashboard. To use WIT, install the witwidget library, configure a WitConfigBuilder with your model's details, and pass it to WitWidget for display, adjusting settings as needed.

---

---

## 14.6. Retraining/Redeployment Evaluation

---

---

### 14.6. Summary: Retraining/Redeployment Evaluation

Machine learning models tend to degrade over time due to changes in user behavior and training data, leading to decreased performance. The speed of degradation varies, but it is often caused by data drift and concept drift, which refer to the shifting distribution of input data or the change in underlying patterns or relationships within that data.

---

---

### 14.6.1. Data Drift

---

---

### 14.6.1. Summary: Data Drift

Data drift occurs when the statistical distribution of production data changes from the original training data, often due to shifts in data attributes such as unit conversions or measurement scales, requiring model retraining for accuracy. Data drift can be detected through feature distribution analysis, correlation checks, and monitoring system reviews.

---

---

### 14.6.2. Concept Drift

---

---

### 14.6.2. Summary: Concept Drift

Concept drift occurs when the statistical properties of a target variable change over time, requiring models to adapt to these changes. To detect such drift, deployed machine learning models like those using Vertex AI Model Monitoring need to be regularly monitored for shifts in performance or behavior.

---

---

### 14.6.3. When Should a Model Be Retrained?

---

---

### 14.6.3. Summary: When Should a Model Be Retrained?

Use a periodic training strategy with set intervals (e.g., weekly, monthly) based on data updates, or trigger retraining when model performance falls below a threshold, or when data drift occurs, to maintain optimal model performance. Alternatively, implement a performance-based approach using a sophisticated monitoring system, or manually retrain models as needed.

---

---

## 14.7. Unit Testing for Model Training and Serving

---

---

### 14.7. Summary: Unit Testing for Model Training and Serving

Testing machine learning systems involves testing the model code, data, and model itself to identify bugs early. Various tests can be run without trained parameters, including checking output shapes and ranges, validating gradients, and ensuring dataset integrity and labeling consistency.

---

---

### 14.7.1. Testing for Updates in API Calls

---

---

### 14.7.1. Summary: Testing for Updates in API Calls

You can test updates to an API call by retraining your model, but that would be resource intensive. Rather, you can write a unit test to generate random input data and run a single step of gradient descent to complete without runtime errors.

---

---

### 14.7.2. Testing for Algorithmic Correctness

---

---

### 14.7.2. Summary: Testing for Algorithmic Correctness

Train your model for multiple iterations, monitoring a decrease in loss, then train with regularization to prevent overfitting and test specific algorithm components. This ensures memorization of training data is not occurring, indicating algorithmic correctness.

---

---

## 14.8. Summary

---

---

### 14.8. Summary: Summary

Google Cloud Platform provides various file types for AI/ML workloads, including structured, unstructured, and semi-structured files, with services such as Pub/Sub, BigQuery Data Transfer Service, and Cloud Datastream handling ingestion and migration. For machine learning model training, Google Cloud offers services like Vertex AI, which supports frameworks like scikit-learn, TensorFlow, and PyTorch, allowing for prebuilt containerized or custom containers, hyperparameter tuning, and tracking model performance with interactive shell and metrics tools.

---

---

## 14.9. Exam Essentials

---

---

### 14.9. Summary: Exam Essentials

Here is a concise summary of the content in two sentences:

To ingest various file types into training on GCP, use services like Pub/Sub, BigQuery Data Transfer Service, Cloud Dataflow, and Vertex AI Workbench to collect, process, store, and analyze data; also understand frameworks for ETL and machine learning such as scikit-learn, TensorFlow, and PyTorch. Additionally, learn how to perform custom training using Vertex AI, hyperparameter tuning with search algorithms like grid search and Bayesian search, unit testing, and retraining/redeployment evaluation to ensure model performance and accuracy.

---

---

## 14.10. Review Questions

---

---

### 14.10. Summary: Review Questions (Part 1/3)

Here are the summaries of the provided content: 1. Use Vertex AI custom jobs for training, and minimize code refactoring by using Cloud Storage as a temporary location for model data. 2. Normalize data with Apache Spark using the Dataproc connector for BigQuery to make the process more efficient by minimizing computation time and manual intervention. 3. Run multiple training jobs on the Vertex AI platform with an interactive shell enabled or hyperparameter tuning to explore model performance, compare evaluation metrics, and store training data. 4. Decrease the number of parallel trials, change the search algorithm from grid search to random search, decrease the range of floating-point values, set the early stopping parameter to TRUE, or use Vertex AI AutoML to speed up the tuning job

---

---

### 14.10. Summary: Review Questions (Part 2/3)

without compromising its effectiveness. 5. Convert PySpark commands into Spark SQL queries to transform data and run pipeline on Dataproc to expedite pipeline runtime while meeting speed and processing requirements. 6. Use Vertex AI training custom containers to run training jobs using any framework, or use Kubeflow Pipelines to manage the team's submissions, and minimize overhead by using existing resources and managed services. 7. Load the data into Cloud Bigtable, read the data from Bigtable using a TF Bigtable connector, or store data in Cloud Storage with shards of TFRecords to improve input/output execution performance. 8. Use Pub/Sub with Cloud Dataflow streaming pipeline to ingest player interaction data for a mobile game, and perform ML on the streaming data to build a pipeline

---

---

### 14.10. Summary: Review Questions (Part 3/3)

with the least overhead. 9. Configure the pipeline using Pub/Sub-1> 2> 3> Storage- > Visualization in Data Studio by ingesting data into BigQuery and manipulating results with pandas DataFrame in Vertex AI platform notebook. 10. Use Looker Studio, Looker, or Jupyter Notebooks to track metrics of the model such as CPU utilization and network I/O and features used while training. 11. Use Cloud Dataproc to manage input and output for Parquet format data while training TensorFlow models on Google Cloud. 12. Run multiple training jobs on the Vertex AI platform with TensorBoard or Looker to show an interactive demo with visual graphs, explain classification metric, and demonstrate inference.

---

---

<!-- _class: lead -->

# 15. Chapter 9Model Explainability on Vertex AI

---

---

## 15.1. Model Explainability on Vertex AI

---

---

### 15.1. Summary: Model Explainability on Vertex AI

Developers of machine learning (ML) models are increasingly responsible for explaining model predictions as their impact on business outcomes grows, particularly in high-stakes applications such as credit loan decisions or medical diagnoses. Gaining visibility into the training process and developing human-explainable ML models is crucial to address user queries and justify model-driven decisions.

---

---

### 15.1.1. Explainable AI

---

---

### 15.1.1. Summary: Explainable AI

Explainability in machine learning refers to the ability to interpret the internal workings of a model, enabling transparency and trust in predictions. Techniques such as global and local explanations help model owners understand uncertainty and improve the model's accuracy, particularly with complex models like deep neural nets that are difficult to debug.

---

---

### 15.1.2. Interpretability and Explainability

---

---

### 15.1.2. Summary: Interpretability and Explainability

Interpretability measures a model's ability to identify causality, while explainability focuses on understanding how parameters within deep neural networks produce their outputs.

---

---

### 15.1.3. Feature Importance

---

---

### 15.1.3. Summary: Feature Importance

Feature importance is a technique that assigns scores to each feature, indicating their relative usefulness in predicting the target variable, helping model builders identify unimportant features to remove before deployment. This can save compute and infrastructure costs, as well as training time, by pruning unnecessary variables from the model.

---

---

### 15.1.4. Vertex Explainable AI

---

---

### 15.1.4. Summary: Vertex Explainable AI

Vertex Explainable AI integrates feature attributions into Vertex AI, providing insights into how each feature contributes to predicted results for classification and regression tasks. It supports AutoML models for images and tables, as well as custom-trained TensorFlow models, enabling model verification, bias recognition, and improvement ideas.

---

---

### 15.1.5. Data Bias and Fairness

---

---

### 15.1.5. Summary: Data Bias and Fairness

Data bias occurs when certain parts are not collected or misrepresented, leading to skewed outcomes that may unfairly treat individuals based on characteristics such as race, gender, or disability. Vertex AI's Explainable AI feature and What-If Tool can detect bias and fairness in data using techniques like feature overview functionality, allowing for detection before model training.

---

---

### 15.1.6. ML Solution Readiness

---

---

### 15.1.6. Summary: ML Solution Readiness

Google emphasizes Responsible AI principles through tools like Explainable AI, Model Cards, and TensorFlow for transparency. Companies can achieve model governance by implementing guidelines such as human review, responsibility assignment matrices, model versioning, and fairness indicators to detect implicit bias.

---

---

### 15.1.7. How to Set Up Explanations in the Vertex AI

---

---

### 15.1.7. Summary: How to Set Up Explanations in the Vertex AI

To use Vertex Explainable AI, you need to configure custom-trained models (TensorFlow, Scikit, or XGBoost) with explanations, while AutoML tabular classification and regression models do not require specific configuration. You can perform online, batch, and local kernel explanations using the Vertex AI API or SDK.

---

---

## 15.2. Summary

---

---

### 15.2. Summary: Summary

Explainable AI is a field that focuses on making machine learning models more transparent and accountable by providing insights into their decision-making processes. The chapter discussed several key concepts, including feature importance, data bias and fairness, ML solution readiness, and three primary techniques for model feature attribution (sampled Shapley, XRAI, and integrated gradients) used on the Vertex AI platform.

---

---

## 15.3. Exam Essentials

---

---

### 15.3. Summary: Exam Essentials

Explainability is crucial to understand how machine learning models make decisions, allowing us to identify biases and improve fairness through feature attribution methods such as Sampled Shapley algorithm and integrated gradients. Explainable AI in Vertex AI supports various solutions, including TensorFlow's Explainable AI SDK and AutoML features, providing insights into model performance and data quality.

---

---

## 15.4. Review Questions

---

---

### 15.4. Summary: Review Questions

Here are the concise summaries:

1. Remove non-informative features using Principal Component Analysis.
2. Explain a TensorFlow model using Integrated Gradients or XRAI.
3. Supported feature attribution techniques: XRAI, Sampled Shapley, Minimum likelihood Interpretability.
4. Use XRAI for image classification on Vertex AI.
5. Set up explanations in user-managed notebooks by configuring the autoML classification model or generating an explanation when creating a batch prediction job.
6. Use sample-based explanations (XRAI) to help explain poor-quality images in the dataset identifying planes.

---

---

<!-- _class: lead -->

# 16. Chapter 10Scaling Models in Production

---

---

## 16.1. Scaling Prediction Service

---

---

### 16.1. Summary: Scaling Prediction Service

A trained TensorFlow model can be deployed after training by saving it to a directory using `tf.saved_model.save()` which contains the complete program including parameters and computation, allowing for sharing or deployment with other services. The saved model is stored as a directory on disk with a protocol buffer file (`saved_model.pb`) describing the function `tf.Graph`.

---

---

### 16.1.1. TensorFlow Serving

---

---

### 16.1.1. Summary: TensorFlow Serving

TensorFlow Serving allows you to host a trained model as an API endpoint through a model server, providing two types of endpoints: REST and gRPC. It enables model serving, version management, and loading models from various sources, with the option to use a managed container on Vertex AI for easy management.

---

---

## 16.2. Serving (Online, Batch, and Caching)

---

---

### 16.2. Summary: Serving (Online, Batch, and Caching)

In Chapter 5 , “Architecting ML Solutions,” we covered two types of serving options in ML systems, batch prediction (or offline serving) and online prediction, and their recommended architectures. In this chapter, we will cover some best practices for your serving and caching strategy.

---

---

### 16.2.1. Real‐Time Static and Dynamic Reference Features

---

---

### 16.2.1. Summary: Real‐Time Static and Dynamic Reference Features

Static and dynamic features are input features fetched in real-time to invoke a model for prediction, differing in their computation and storage. Static features, computed on batches, are stored in NoSQL databases optimized for singleton lookup, while dynamic features, computed in real-time, are stored in low-latency read/write databases like Cloud Bigtable.

---

---

### 16.2.2. Pre‐computing and Caching Prediction

---

---

### 16.2.2. Summary: Pre‐computing and Caching Prediction

Here is a concise summary of the content in two sentences:

To improve online prediction latency, pre-computed predictions can be stored in a low-latency data store like Memorystore or Datastore, allowing clients to fetch predictions instead of calling the model directly. This approach enables caching and optimization for specific entity lookups, such as customer IDs, device IDs, or input feature combinations, reducing the complexity of large datasets with high cardinality.

---

---

## 16.3. Google Cloud Serving Options

---

---

### 16.3. Summary: Google Cloud Serving Options

In Google Cloud, you can deploy your models for either online predictions or batch predictions. You can perform batch and online predictions for both AutoML and custom models. In the following sections, we will cover how to set up online and batch jobs using Vertex AI.

---

---

### 16.3.1. Online Predictions

---

---

### 16.3.1. Summary: Online Predictions

To create a real-time prediction endpoint using Vertex AI, you can deploy models trained on Google Cloud (using AutoML or custom models) or import models from other environments (on-premise, another cloud, or local device), requiring matching filename conventions for artifacts and potentially creating a custom container image. Once deployed, you can make predictions by deploying the model resource to an endpoint, making a prediction, and undeploying if not in use.

---

---

### 16.3.2. Batch Predictions

---

---

### 16.3.2. Summary: Batch Predictions

To perform batch prediction, upload your input data to Cloud Storage and specify its format as required for AutoML or custom models using options such as JSON Lines, TFRecord, CSV files, file list, BigQuery, or a text file with Cloud Storage URIs. You can then create a batch prediction job in the Google Cloud console or through Vertex AI APIs, selecting output destinations like BigQuery or Cloud Storage and optionally enabling model monitoring.

---

---

## 16.4. Hosting Third‐Party Pipelines (MLflow) on Google Cloud

---

---

### 16.4. Summary: Hosting Third‐Party Pipelines (MLflow) on Google Cloud

Here is a concise summary of the content:

You can host third-party pipelines on Google Cloud that are specific to MLflow, leveraging its tracking, projects, models, and model registry components to manage the end-to-end machine learning life cycle. To use MLflow on Google Cloud, you create a PostgreSQL DB, Google Cloud Storage bucket, Compute Engine instance, or run it using a Google Cloud plug-in, providing scalability, high-performance compute, and collaboration features.

---

---

## 16.5. Testing for Target Performance

---

---

### 16.5. Summary: Testing for Target Performance

Here is a concise summary:

To test your model's performance in production, monitor its age, numerical stability, and output quality, including checking for NaN or null values in weights and layer outputs. Vertex AI services like Model Monitoring and Feature Store can help detect issues and track model performance over time.

---

---

## 16.6. Configuring Triggers and Pipeline Schedules

---

---

### 16.6. Summary: Configuring Triggers and Pipeline Schedules

Here is a two-sentence summary of the content:

You can trigger training or prediction jobs on Vertex AI using services such as Cloud Scheduler, Managed Notebooks, and Cloud Build, which allow for scheduling and automation of workflows. Additionally, options like Cloud Functions, Pub/Sub, Workflows, and Cloud Composer provide further orchestration choices, including serverless and stateful workflow management, to automate and govern ML systems.

---

---

## 16.7. Summary

---

---

### 16.7. Summary: Summary

TF Serving is used for scaling prediction services by utilizing the predict function and understanding SignatureDef of saved TF models to determine output. It also covers deployment options like online and batch mode with Vertex AI Prediction and Google Cloud serving, as well as tools to monitor and automate model pipelines using Cloud Run and others.

---

---

## 16.8. Exam Essentials

---

---

### 16.8. Summary: Exam Essentials

TensorFlow Serving is a platform that enables deployment of trained TensorFlow models for production-ready prediction services, offering scaling options (online, batch, caching) and Google Cloud serving options, including real-time endpoints and batch jobs through Google Cloud Vertex AI Prediction. With Vertex AI services like Model Monitoring, triggers, pipelines, and Workflows, users can automate model deployments, monitoring performance degradation and triggering re-deployments as needed to maintain target performance levels.

---

---

## 16.9. Review Questions

---

---

### 16.9. Summary: Review Questions (Part 1/2)

Here are the summaries: 1. Predict web banner with 300ms latency requirement: Deploy the model using TF Serving, embed client on website, deploy gateway on App Engine. 2. Batch predict text classification model on BigQuery data: Submit batch prediction job on Vertex AI and use SavedModel with Dataflow for data reading. 3. Serve forecasting model predictions to users: Create Cloud Function with Pub/Sub topic, send notifications when balance drops below threshold. 4. Write correct way to write predict request in TF Serving: json.dumps({'signature_name': 'serving_default', 'instances': ['input text']}). 5. Execute preprocessing at prediction time on Vertex AI platform: Use Pub/Sub and Cloud Functions for data transformation. 6. Serve model with aggregated scanned customer forms data: Deploy

---

---

### 16.9. Summary: Review Questions (Part 2/2)

model on Vertex AI, use batch prediction functionality or Compute Engine serving pipeline. 7. Schedule Jupyter Notebook batch jobs using Vertex AI managed notebooks: Use schedule function in Vertex AI managed notebooks. Please note that I kept the first request as it was because it had a clear and unique solution

---

---

<!-- _class: lead -->

# 17. Chapter 11Designing ML Training Pipelines

---

---

## 17.1. Orchestration Frameworks

---

---

### 17.1. Summary: Orchestration Frameworks

An orchestrator is used to manage multiple steps in an ML pipeline, running each step sequentially based on defined conditions. Orchestrating pipelines is useful for both development and production phases, automating tasks such as data cleaning and model training with Kubeflow Pipelines, Vertex AI Pipelines, Apache Airflow, or Cloud Composer.

---

---

### 17.1.1. Kubeflow Pipelines

---

---

### 17.1.1. Summary: Kubeflow Pipelines

Kubeflow Pipelines is a platform for building, deploying, and managing multistep ML workflows based on Docker containers, using Kubeflow as the ML toolkit for Kubernetes. It provides a user interface, engine, SDK, notebooks, and orchestration tools to manage and track experiments, jobs, and runs of ML workflows.

---

---

### 17.1.2. Vertex AI Pipelines

---

---

### 17.1.2. Summary: Vertex AI Pipelines

Vertex AI Pipelines allow users to run Kubeflow and TensorFlow Extended pipelines without server setup, automatically provisioning and managing infrastructure. Each pipeline run produces metadata and ML artifacts, enabling lineage tracking and automating workflow management through portable, scalable components.

---

---

### 17.1.3. Apache Airflow

---

---

### 17.1.3. Summary: Apache Airflow

Apache Airflow is an open-source platform for managing data engineering pipelines as a directed acyclic graph (DAG), allowing users to build and run workflows composed of individual tasks with dependencies. The platform provides a user interface, scheduling functionality, and execution tools, enabling the creation and automation of complex data workflows.

---

---

### 17.1.4. Cloud Composer

---

---

### 17.1.4. Summary: Cloud Composer

Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow, allowing users to create environments quickly without installation or management overhead. It enables the creation of data-driven workflows for batch workloads with low latency and supports integration with Google services like BigQuery and Dataflow.

---

---

### 17.1.5. Comparison of Tools

---

---

### 17.1.5. Summary: Comparison of Tools

Kubeflow Pipelines and Vertex AI Pipelines offer managed orchestration capabilities for ML workflows using supported frameworks like TensorFlow and PyTorch, while Cloud Composer provides a Python-based implementation with integrated management of ETL/ELT pipelines and automated infrastructure setup. Vertex AI Pipelines also inherits Kubeflow's failure management features, allowing for handling failures via built-in GCP metrics.

---

---

## 17.2. Identification of Components, Parameters, Triggers, and Compute Needs

---

---

### 17.2. Summary: Identification of Components, Parameters, Triggers, and Compute Needs

Here is a concise summary of the content:

You can trigger an MLOps pipeline to automate retraining with new data, on demand, schedule, or when model performance degrades, and there are two approaches: executing the continuous training (CT) pipeline to update the existing model or deploying a new pipeline for a new implementation. A CI/CD pipeline deploys a new pipeline, while a CT pipeline updates the existing prediction service or serves a newly trained model.

---

---

### 17.2.1. Schedule the Workflows with Kubeflow Pipelines

---

---

### 17.2.1. Summary: Schedule the Workflows with Kubeflow Pipelines

Kubeflow Pipelines provide a Python SDK to operate pipelines programmatically using kfp.Client, allowing invocation through various services including Cloud Scheduler and Pub/Sub. The built-in scheduler Argo is also available for recurring pipelines, with alternatives like Jenkins and Apache Airflow offering general-purpose workflow management.

---

---

### 17.2.2. Schedule Vertex AI Pipelines

---

---

### 17.2.2. Summary: Schedule Vertex AI Pipelines

Cloud Scheduler allows scheduling pipeline execution through an event-driven Cloud Function with HTTP or Pub/Sub triggers, enabling manual or automated runs of precompiled pipelines. Alternatively, using Cloud Functions with a Pub/Sub trigger enables triggering pipeline runs in response to messages published to a specific topic.

---

---

## 17.3. System Design with Kubeflow/TFX

---

---

### 17.3. Summary: System Design with Kubeflow/TFX

In the following sections, we will discuss system design with the Kubeflow DSL, and then we will cover system design with TFX.

---

---

### 17.3.1. System Design with Kubeflow DSL

---

---

### 17.3.1. Summary: System Design with Kubeflow DSL

Kubeflow Pipelines uses Argo Workflows by default and exposes a Python domain-specific language (DSL) for authoring pipelines, allowing users to define operations performed in the ML workflow. To compile a pipeline, users create a container, specify its configuration and dependencies, sequence the operations, and then generate a YAML file that can be executed by Kubeflow Pipelines.

---

---

### 17.3.2. System Design with TFX

---

---

### 17.3.2. Summary: System Design with TFX

TFX is a Google production-scale machine learning platform based on TensorFlow, providing a configuration framework and shared libraries for building and managing ML workflows in a production environment. A TFX pipeline typically includes components such as ExampleGen, Trainer, Tuner, and Model Server, which work together to orchestrate the data processing, training, and deployment of machine learning models in a scalable and high-performance manner.

---

---

## 17.4. Hybrid or Multicloud Strategies

---

---

### 17.4. Summary: Hybrid or Multicloud Strategies

Multicloud refers to an interconnection between two or more public cloud providers, allowing users to combine data and resources across different cloud ecosystems, such as Google Cloud Platform (GCP) with AWS or Azure. The GCP platform supports multicloud capabilities through features like BigQuery Omni and Anthos, enabling hybrid ML development, AI services, and on-premises deployment of cloud-native applications.

---

---

## 17.5. Summary

---

---

### 17.5. Summary: Summary

Here is a concise summary of the content in two sentences:

This chapter covered ML pipeline orchestration using tools like Kubeflow, Vertex AI Pipelines, Apache Airflow, and Cloud Composer, with a focus on their differences and use cases for automation. It also explored system design, workflow scheduling, and hybrid/multicloud environments using GCP services like BigQuery Omni and Anthos to set up diverse ML pipeline workflows.

---

---

## 17.6. Exam Essentials

---

---

### 17.6. Summary: Exam Essentials

Kubeflow Pipelines and Vertex AI Pipelines are two popular orchestration frameworks for automating machine learning (ML) workflows, with both allowing for scheduling and deployment of ML pipelines using Cloud Build and Cloud Function event triggers. Both Kubeflow and TFX can be used as runtime or orchestrators to define and orchestrate ML pipelines, with the option to run on GCP using Vertex AI Pipelines.

---

---

## 17.7. Review Questions

---

---

### 17.7. Summary: Review Questions (Part 1/2)

Here are concise summaries: 1. To validate a TensorFlow model, use TFX ModelValidator with AUC ROC as the main metric and k-fold cross-validation. 2. Design Kubeflow Pipelines workflow: Use Cloud Storage trigger, Pub/Sub, and Cloud Scheduler to automate pipeline refreshes on GCP with minimal effort. 3. For Kubeflow Pipelines training job, design similar to step 2. 4. To orchestrate a recommendation system in Kubeflow Pipelines, use either Google Cloud BigQuery component or create custom BigQuery client library component. 5. Orchestrate an ML workflow using Kubeflow with MLOps: Use TFX or Dataflow for model serving after data transformation completion. 6. Set up Kubeflow on GKE (Cloud Run) with the least effort, by choosing either of two options: Kubeflow Pipelines on GKE, or Vertex AI

---

---

### 17.7. Summary: Review Questions (Part 2/2)

Pipelines with Kubeflow. 7. Automate unit tests execution in Cloud Source Repositories using Cloud Build and Cloud Logging sink to Pub/Sub topic. 8. To scale an on-premises training pipeline securely, use Anthos to set up Cloud Run with GKE for Kubeflow Pipelines and/or Vertex AI Training job.

---

---

<!-- _class: lead -->

# 18. Chapter 12Model Monitoring, Tracking, and Auditing Metadata

---

---

## 18.1. Model Monitoring

---

---

### 18.1. Summary: Model Monitoring

A machine learning model's performance in real-time data can be uncertain due to changes in the environment, such as new human behavior or input patterns that aren't immediately apparent. As time passes, a deployed model's performance will degrade due to "drift", which refers to changes in concept (e.g., shifts in market trends) and/or data (e.g., updates in user behavior).

---

---

### 18.1.1. Concept Drift

---

---

### 18.1.1. Summary: Concept Drift

Machine learning models often struggle with concept drift, a phenomenon where the relationship between input variables and predicted variables changes over time due to shifting underlying assumptions or external factors. This can occur in applications like email spam detection, where spammers adapt their tactics to evade filters.

---

---

### 18.1.2. Data Drift

---

---

### 18.1.2. Summary: Data Drift

Data drift occurs when changes in the input data distribution or schema affect a machine learning model's performance, causing it to fail as new patterns emerge. To mitigate this, model monitoring involves continuously evaluating the model using the same metrics used during training and tracking changes in the input data to detect potential issues early on.

---

---

## 18.2. Model Monitoring on Vertex AI

---

---

### 18.2. Summary: Model Monitoring on Vertex AI

Vertex AI offers model monitoring features to detect skew and drift in training-serving data, specifically in categorical and numerical features, enabling the detection of differences in input feature distributions that can impact model performance over time. Skew is detected based on comparison with original training data, while drift is monitored by analyzing changes in statistical distribution without access to original training data.

---

---

### 18.2.1. Drift and Skew Calculation

---

---

### 18.2.1. Summary: Drift and Skew Calculation

Calculating baseline distributions involves determining statistical distribution methods for categorical and numerical data types, such as count or percentage for categories, and buckets for numbers. A comparison method uses these distributions to calculate distances using L-infinity distance for categorical features and Jensen-Shannon divergence for numerical features when anomalies (skew or drift) are identified.

---

---

### 18.2.2. Input Schemas

---

---

### 18.2.2. Summary: Input Schemas

Vertex AI can parse input values from prediction requests using a specified schema, which is automatically parsed for AutoML models but requires manual specification for custom-trained models.

---

---

## 18.3. Logging Strategy

---

---

### 18.3. Summary: Logging Strategy

In deploying a predictive model, logging requests is essential in certain domains for future audits, and it can also update training data. Vertex AI allows enabling prediction logs for various models, including AutoML tabular and image models, during deployment or endpoint creation.

---

---

### 18.3.1. Types of Prediction Logs

---

---

### 18.3.1. Summary: Types of Prediction Logs

You can enable three kinds of logs to get information from the prediction nodes. These three types of logs are independent of each other and so can be enabled or disabled independently.

---

---

### 18.3.2. Log Settings

---

---

### 18.3.2. Summary: Log Settings

Logging settings can be updated when deploying a model, and changes must be redeployed to take effect. High QPS will result in a significant number of logs, making logging costs a consideration for models expected to handle many requests.

---

---

### 18.3.3. Model Monitoring and Logging

---

---

### 18.3.3. Summary: Model Monitoring and Logging

Model monitoring and request-response logging share the same backend infrastructure in BigQuery table, resulting in restrictions: enabling model monitoring disables request-response logging, and vice versa, once one is activated it cannot be modified by the other.

---

---

## 18.4. Model and Dataset Lineage

---

---

### 18.4. Summary: Model and Dataset Lineage

Metadata in machine learning experiments helps track experiment parameters, observations, and artifacts (datasets, models) to monitor model performance after deployment, compare hyperparameter effectiveness, and track lineage for auditing purposes. This information enables the rerun of ML workflows with specific parameters and tracking downstream usage of artifacts.

---

---

### 18.4.1. Vertex ML Metadata

---

---

### 18.4.1. Summary: Vertex ML Metadata

Vertex ML Metadata is an open-source library based on Google's TensorFlow Extended, providing a graph-like data model for representing relationships between metadata resources such as artifacts, contexts, executions, and events, which can be used for analyzing, debugging, or auditing purposes. It uses the MLMD library to store and query metadata, with a metadataschema that specifies the schema for each type of data, represented in YAML format using OpenAPI objects.

---

---

## 18.5. Vertex AI Experiments

---

---

### 18.5. Summary: Vertex AI Experiments

Vertex AI Experiments helps you develop, analyze, and optimize machine learning models by tracking trials, input variables, output results, and choosing the best direction of exploration. You can access and visualize experiment runs using the Google Cloud console or the Python SDK, along with Vertex ML Metadata for tracking artifacts and lineage.

---

---

## 18.6. Vertex AI Debugging

---

---

### 18.6. Summary: Vertex AI Debugging

You can debug efficiency issues or permission restrictions in a model training container by connecting to it through Vertex AI's interactive Bash shell, where you can check permissions, visualize Python execution with profiling tools, and analyze performance metrics like CPU and GPU usage. To access the interactive shell, navigate to the provided URI during custom training job initiation, ensuring your service account has the necessary permissions set up.

---

---

## 18.7. Summary

---

---

### 18.7. Summary: Summary

This chapter covers steps beyond deploying a model, including monitoring performance and logging strategies with Vertex AI. It also discusses tracking model lineage for multiple models using Vertex ML Metadata and Vertex AI Experiments.

---

---

## 18.8. Exam Essentials

---

---

### 18.8. Summary: Exam Essentials

Model monitoring involves tracking changes in input data (data drift, concept drift) after deployment to ensure performance integrity. Logging strategies, using Vertex AI, and understanding ML metadata are crucial for tracking deployments, performance, and creating new training data.

---

---

## 18.9. Review Questions

---

---

### 18.9. Summary: Review Questions (Part 1/3)

Here are the concise summaries: 1. Model drift occurs when input feature distribution changes over time, leading to poor performance on deployed models. 2. Distribution drift is the change in statistical distribution of input features between training and production data, typically caused by shifts in the underlying data distribution. 3. Training-serving skew refers to the difference in baseline statistical distributions of input features between training and production data. 4. Baseline statistical distribution of input features in production data should be used to detect prediction drift. 5. Continuous statistical distribution of features in production data should be used to detect prediction drift. 6. The Jensen-Shannon divergence distance score is used for categorical features in

---

---

### 18.9. Summary: Review Questions (Part 2/3)

Vertex AI model monitoring. 7. Periodically switching off monitoring to save money can lead to alerts being sent too infrequently, while reducing the sampling rate or inputs to the model may not effectively address cost constraints. 8. Features of Vertex AI model monitoring include: - Sampling rate configuration - Monitoring frequency - Choosing different distance metrics 9. Custom models with automatic schema parsing without values in key/value pairs are an invalid combination for model building and schema parsing in Vertex AI model monitoring. 10. Category is not a valid data type in the model monitoring schema. 11. Container logging is not a valid logging type in Vertex AI. 12. You can get a log of sample prediction requests and responses by using Container logging, Input logging, or

---

---

### 18.9. Summary: Review Questions (Part 3/3)

Access logging. 13. The primary reason for using a metadata store is to track lineage. 14. An artifact in a metadata store refers to an entity that was created by or can be consumed by an ML workflow. 15. Workflow step context execution is not part of the data model in a Vertex ML metadata store.

---

---

<!-- _class: lead -->

# 19. Chapter 13Maintaining ML Solutions

---

---

## 19.1. MLOps Maturity

---

---

### 19.1. Summary: MLOps Maturity

Organizations adopt machine learning by first manually training models and later automating the process using pipelines to achieve full automation. The maturity of their machine learning practice is defined by the three levels of MLOps: Level 0 (manual), Level 1 (strategic automation), and Level 2 (CI/CD automation).

---

---

### 19.1.1. MLOps Level 0: Manual/Tactical Phase

---

---

### 19.1.1. Summary: MLOps Level 0: Manual/Tactical Phase

Organizations experimenting with machine learning (ML) are in the "Proof of Concept" phase, where they validate ideas on how ML can improve business processes through model building and testing. The output model is then handed off to the deployment team using a model registry, which deploys it to serve predictions.

---

---

### 19.1.2. MLOps Level 1: Strategic Automation Phase

---

---

### 19.1.2. Summary: MLOps Level 1: Strategic Automation Phase

Organizations at MLOps Level 1 prioritize ML to solve business objectives, utilizing pipelines for continuous training and delivery of model prediction services. This phase requires new services like automated validation, pipeline triggers, feature stores, and metadata management to support infrastructure sharing and clear separation between dev and prod environments.

---

---

### 19.1.3. MLOps Level 2: CI/CD Automation, Transformational Phase

---

---

### 19.1.3. Summary: MLOps Level 2: CI/CD Automation, Transformational Phase

An organization in the transformational phase utilizes AI to drive innovation and agility, with ML experts integrated across product teams and business units. Automated CI/CD pipelines are used for model updates and deployment, streamlining the process and reducing manual intervention.

---

---

## 19.2. Retraining and Versioning Models

---

---

### 19.2. Summary: Retraining and Versioning Models

Model performance can degrade over time, prompting the need to retrain it at some point. Vertex AI Model Monitoring enables collection of real data for model evaluation and new training datasets, allowing users to determine a suitable retraining frequency.

---

---

### 19.2.1. Triggers for Retraining

---

---

### 19.2.1. Summary: Triggers for Retraining

Model performance degrades over time, requiring retraining, which can be triggered by an absolute threshold or a sudden drop in accuracy. A retraining policy must balance factors such as training costs, training time, delayed training, and predicted costs to find an optimal approach that minimizes downtime and maintains model performance.

---

---

### 19.2.2. Versioning Models

---

---

### 19.2.2. Summary: Versioning Models

Here is a concise summary:

Model versioning allows deploying multiple versions of a model, enabling users to select a specific version through a version ID, ensuring backward compatibility with changing models that have the same inputs and outputs or new models with different behavior. This feature enables convenient access to each model using REST endpoints, allowing for monitoring and comparison of different deployed model versions.

---

---

## 19.3. Feature Store

---

---

### 19.3. Summary: Feature Store

Feature engineering is a crucial investment for organizations building machine learning (ML) models, but its time-consuming nature often leads to duplicated efforts, creating problems such as non-reusable features, governance challenges, cross-collaboration issues, training and serving differences, and limitations in productizing features. Ad hoc feature creation hinders the sharing of valuable features among teams, making it difficult for organizations to optimize their ML solutions efficiently.

---

---

### 19.3.1. Solution

---

---

### 19.3.1. Summary: Solution

A central feature store enables data engineers and ML engineers to access, version, and share features and their metadata, applying software engineering principles for efficient collaboration and scalability. Key features of modern feature stores include fast processing of large feature sets and low latency access, with options like Feast (open-source) and Google Cloud's Vertex AI Feature Store (managed service).

---

---

### 19.3.2. Data Model

---

---

### 19.3.2. Summary: Data Model

The Vertex AI Feature Store uses a time-series data model to store features as they change over time, organized into a hierarchical structure with featurestores containing one or more entity types representing similar features. Each entity type is defined by unique entities (e.g., player_id) that can be mapped to specific features such as batting_avg and age.

---

---

### 19.3.3. Ingestion and Serving

---

---

### 19.3.3. Summary: Ingestion and Serving

Vertex AI Feature Store supports both batch and streaming ingestion by storing features in BigQuery and then ingesting them into the store. The store offers two methods to retrieve features: batch for model training and online for real-time inference, returning data up to a specified time point.

---

---

## 19.4. Vertex AI Permissions Model

---

---

### 19.4. Summary: Vertex AI Permissions Model

Manage access to resources such as datasets, models, and perform operations like training and deploying with Identity and Access Management (IAM) by following best practices including least privilege, managing service accounts and keys, auditing, and policy management to ensure secure ML pipelines. Implementing these security measures ensures restricted users and applications can only perform necessary actions, while regularly monitoring and updating security assets.

---

---

### 19.4.1. Custom Service Account

---

---

### 19.4.1. Summary: Custom Service Account

Google Cloud's Vertex AI training job uses automatic service accounts created for your project, which may grant unnecessary permissions beyond their required scope. To improve security, it's recommended to create custom service accounts with precisely defined permissions instead.

---

---

### 19.4.2. Access Transparency in Vertex AI

---

---

### 19.4.2. Summary: Access Transparency in Vertex AI

Cloud Audit logs track user activity within your organization, while Access Transparency logs capture actions taken by Google personnel on your project. Most Google services provide access logs, but some features are not covered and can be found on the Cloud Google documentation website.

---

---

## 19.5. Common Training and Serving Errors

---

---

### 19.5. Summary: Common Training and Serving Errors

Common errors during training and serving can be identified to debug problems effectively by understanding their types. This guide focuses on TensorFlow, a widely-used framework for machine learning, to provide insight into frequently encountered errors.

---

---

### 19.5.1. Training Time Errors

---

---

### 19.5.1. Summary: Training Time Errors

During the training phase, the most relevant errors are seen when you run Model.fit() .Errors happen when the following scenarios occur: Input data is not transformed or not encoded. Tensor shape is mismatched. Out of memory errors occur because of instance size (CPU and GPU).

---

---

### 19.5.2. Serving Time Errors

---

---

### 19.5.2. Summary: Serving Time Errors

The serving time errors are seen only during deployment and the nature of the errors is also different. The typical errors are as follows: Input data is not transformed or not encoded. Signature mismatch has occurred. Refer to this URL for a full list of TensorFlow errors: www.tensorflow.org/api_docs/python/tf/errors

---

---

### 19.5.3. TensorFlow Data Validation

---

---

### 19.5.3. Summary: TensorFlow Data Validation

To prevent and reduce these errors, you can use TensorFlow Data Validation (TFDV). TFDV can analyze training and serving data as follows: To compute statistics To infer schema To detect anomalies Refer here for full documentation: https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell

---

---

### 19.5.4. Vertex AI Debugging Shell

---

---

### 19.5.4. Summary: Vertex AI Debugging Shell

Vertex AI provides an interactive shell to debug training, allowing users to inspect and analyze their containers, run tracing tools, and validate IAM permissions. This shell enables issue detection and resolution without requiring code modifications or restarts of the container.

---

---

## 19.6. Summary

---

---

### 19.6. Summary: Summary

This chapter focuses on long-term maintenance of ML applications through automation and integration with CI/CD principles, including retraining model policies that balance quality and cost. A key concept to improve efficiency is the implementation of a feature store, which enables sharing of features across departments and can be implemented using open source software or Vertex AI Feature Store.

---

---

## 19.7. Exam Essentials

---

---

### 19.7. Summary: Exam Essentials

MLOps maturity levels include experimental, strategic, and fully mature phases, each with varying automation and integration with CI/CD pipelines. Key concepts in MLOps include model versioning, retraining triggers, and feature stores to optimize model updates and reuse of expensive engineering efforts.

---

---

## 19.8. Review Questions

---

---

### 19.8. Summary: Review Questions (Part 1/3)

Here are the concise summaries: 1. MLOps workflow steps: Model training, testing, and validation is not a major step in the MLOps workflow. 2. MLOps level for small retail organization: Implement MLOps level 0 or 1 on an ad-hoc basis. 3. MLOps level for online fashion store: Implement MLOps level 1. 4. MLOps level for image processing team: Implement MLOps level 2 due to new algorithms and custom models. 5. Problems solved by MLOps level 0: Automates training, but does not automate model deployment. 6. MLOps level for large organization with multiple products: Implement MLOps level 1 or 2 to scale. 7. Handoff in MLOps level 1: The pipeline to train a model is handed off to deployment. 8. Handoff in MLOps level 0: The pipeline to train a model is not handed off to deployment. 9. Trigger

---

---

### 19.8. Summary: Review Questions (Part 2/3)

for retraining in MLOps level 2: Feature store, performance degradation, and metadata store triggers. 10. Considerations for retraining trigger: Algorithm, cost of retraining, time to access data. 11. Policies for triggering retraining from monitoring data: Model performance degradation below a threshold, sudden drop in performance of the model. 12. Deploying new version of model: Whenever the model has similar inputs and outputs and is used for the same purpose. 13. Good reasons to use feature store: Many features not shared between teams, features created by data teams are not available during serving time. 14. Hierarchy of Vertex AI Feature Store data model: Featurestore > Entity > Feature, Featurestore > Feature > FeatureValue, Featurestore > Entity > FeatureValue. 15. Highest level in

---

---

### 19.8. Summary: Review Questions (Part 3/3)

Vertex AI Feature Store hierarchy: Featurestore. Note that some questions do not have a concise two-sentence summary, but I provided the most relevant information to answer the question.

---

---

<!-- _class: lead -->

# 20. Chapter 14BigQuery ML

---

---

## 20.1. BigQuery – Data Access

---

---

### 20.1. Summary: BigQuery – Data Access

To access data in BigQuery, users can write SQL queries through the web console or run them in a Jupyter Notebook using either the magic command %%bigquery or a Python API. The Python API allows users to import the BigQuery library, create a client, and execute queries as strings, capturing results in Pandas DataFrames.

---

---

## 20.2. BigQuery ML Algorithms

---

---

### 20.2. Summary: BigQuery ML Algorithms

BigQuery ML enables users to create and deploy machine learning models directly from standard SQL queries without writing any Python code, making it a serverless solution for model training and prediction. This eliminates the need for external infrastructure or additional setup, allowing users to leverage machine learning capabilities within BigQuery.

---

---

### 20.2.1. Model Training

---

---

### 20.2.1. Summary: Model Training

A CREATE MODEL statement is used to create a model in BigQuery ML, specifying the model type (e.g. linear regression or classification) and input label columns, which identify the target column in the data for training. The model can be trained using a query job generated by the SQL command, with options such as DNN models available using TensorFlow estimators.

---

---

### 20.2.2. Model Evaluation

---

---

### 20.2.2. Summary: Model Evaluation

To evaluate a machine learning model using a separate dataset not seen by the model, use the keyword ML.EVALUATE with a query that selects from the test dataset. The result is displayed in the web interface, providing an instantaneous assessment of the model's performance.

---

---

### 20.2.3. Prediction

---

---

### 20.2.3. Summary: Prediction

The ML.PREDICT function in BigQuery ML allows predicting output from a trained model by passing a table with input columns, resulting in a table with the same number of rows and two new columns containing predicted values and probabilities for each label column. By specifying a model and selecting specific columns or tables to pass to the function, users can retrieve desired predictions and probabilities from their machine learning models.

---

---

## 20.3. Explainability in BigQuery ML

---

---

### 20.3. Summary: Explainability in BigQuery ML

Explainability in BigQuery allows global feature importance values at the model level or explanations for each prediction using SQL functions, and can be accessed by setting enable_global_explain=TRUE during training. The feature attributions returned as a table show the impact of each input feature on making predictions, with higher attribution indicating greater relevance to the model.

---

---

## 20.4. BigQuery ML vs. Vertex AI Tables

---

---

### 20.4. Summary: BigQuery ML vs. Vertex AI Tables

BigQuery ML and Vertex AI cater to different types of users: data analysts and SQL experts with BigQuery, and machine learning engineers familiar with Kubeflow, Java, Python, and Jupyter Notebooks with Vertex AI. The interface and approach to training and prediction differ significantly between the two products, making them better suited for specific user audiences.

---

---

## 20.5. Interoperability with Vertex AI

---

---

### 20.5. Summary: Interoperability with Vertex AI

Although Vertex AI and BigQuery ML are very distinct products, they have been designed to interoperate at every point in the machine learning pipeline. There are at least six integration points that make it easy to use both products together seamlessly.

---

---

### 20.5.1. Access BigQuery Public Dataset

---

---

### 20.5.1. Summary: Access BigQuery Public Dataset

BigQuery provides over 200 publicly available datasets that can be accessed through Google Cloud Platform projects for a pay-per-query model, allowing users to utilize these datasets without upfront costs. These datasets can also be integrated with Vertex AI for machine learning model training and data augmentation, enhancing predictive models such as traffic condition forecasting.

---

---

### 20.5.2. Import BigQuery Data into Vertex AI

---

---

### 20.5.2. Summary: Import BigQuery Data into Vertex AI

You can create a Vertex AI dataset directly from a BigQuery URL, eliminating the need to export and import data, allowing seamless connection to data in BigQuery. This is achieved through the `create` method of `TabularDataset` class in Python or the console interface.

---

---

### 20.5.3. Access BigQuery Data from Vertex AI Workbench Notebooks

---

---

### 20.5.3. Summary: Access BigQuery Data from Vertex AI Workbench Notebooks

Using a Jupyter Notebook from a managed notebook instance in Vertex AI Workbench allows direct access to BigQuery datasets, enabling features like SQL query execution and data download into Pandas DataFrames for data exploration and analysis. This integration is particularly useful for data scientists using Jupyter Notebooks for tasks such as visualization, machine learning model experimentation, and feature engineering.

---

---

### 20.5.4. Analyze Test Prediction Data in BigQuery

---

---

### 20.5.4. Summary: Analyze Test Prediction Data in BigQuery

Model training allows exporting predictions to BigQuery, enabling post-processing analysis using SQL methods on the test dataset results. The feature enables users to further analyze and refine their machine learning models by leveraging BigQuery's data processing capabilities.

---

---

### 20.5.5. Export Vertex AI Batch Prediction Results

---

---

### 20.5.5. Summary: Export Vertex AI Batch Prediction Results

You can use BigQuery tables as direct input for batch predictions in Vertex AI, allowing for seamless integration of your machine learning workflow with your existing data storage. Predictions are also sent back to BigQuery to be stored as a table, streamlining MLOps and data management.

---

---

### 20.5.6. Export BigQuery Models into Vertex AI

---

---

### 20.5.6. Summary: Export BigQuery Models into Vertex AI

BigQuery ML models can be exported to Google Cloud Storage (GCS) and imported into Vertex AI for flexible training and deployment. Starting soon, models can be registered directly in the Vertex AI Model Registry, eliminating the need for GCS export, with support for both BigQuery built-in and TensorFlow models.

---

---

## 20.6. BigQuery Design Patterns

---

---

### 20.6. Summary: BigQuery Design Patterns

BigQuery ML introduces novel solutions to frequent data science and machine learning challenges, replacing traditional approaches with more elegant methods. These design patterns offer clever solutions to recurring issues in machine learning, enhancing the field's capabilities.

---

---

### 20.6.1. Hashed Feature

---

---

### 20.6.1. Summary: Hashed Feature

The proposed solution addresses three problems with categorical variables: incomplete vocabulary, high cardinality, and the cold start problem by transforming the variable into a low-cardinality domain using a hashing function like FarmHash. This transformation can be performed easily in BigQuery using a formula like ABS(MOD(FARM_FINGERPRINT(zipcode), numbuckets)).

---

---

### 20.6.2. Transforms

---

---

### 20.6.2. Summary: Transforms

BigQuery ML's TRANSFORM clause allows users to apply transformations to inputs before feeding them into a model, ensuring consistency between training and production environments. This clause automatically applies specified transformations during prediction, enabling features like feature cross and quantile bucketization, which can enhance model performance in BigQuery ML.

---

---

## 20.7. Summary

---

---

### 20.7. Summary: Summary

BigQuery ML enables the use of machine learning in the SQL community by democratizing access and reducing model creation time through direct SQL transformations. The service is interoperable with Vertex AI, allowing seamless integration and utilization of its features.

---

---

## 20.8. Exam Essentials

---

---

### 20.8. Summary: Exam Essentials

BigQuery ML is a SQL-based service for analysts and data analysts familiar with SQL, offering features like training, prediction, and model explanation through standard SQL queries. Vertex AI is designed for ML engineers and offers additional capabilities such as automated workflows and advanced machine learning algorithms, making it a complementary tool to BigQuery ML.

---

---

## 20.9. Review Questions

---

---

### 20.9. Summary: Review Questions (Part 1/3)

Here are concise summaries of each scenario: 1. You will use Vertex AI AutoML Tables to forecast sales per country with new data about the spread of the illness and a plan for recovery. 2. Use TensorFlow model on Vertex AI tables to predict time and distance based on current location and userid. 3. Use BigQuery classification model_type or Vertex AI custom model to build collaborative filtering recommendations using explicit feedback. 4. Use BigQuery ML and choose TensorFlow as the model type to run predictions with saved models. 5. Use Kubeflow Pipelines to create a Vertex AI AutoML Table with explanations for initial machine learning model. 6. Create a Vertex AI pipeline component to download BigQuery dataset to GCS bucket and then run Vertex AI AutoML Tables. 7. Export test prediction

---

---

### 20.9. Summary: Review Questions (Part 2/3)

data from GCS, create an automation job to transfer it to BigQuery for analysis. 8. Run predictions in BigQuery and export the prediction data directly to BigQuery. 9. Use BigQuery ML models with TRANSFORM functionality or Vertex AI custom models to use highly accurate models built by another team. 10. Use hashing function (ABS(MOD(FARM_FINGERPRINT(zipcode),buckets))) in BigQuery to bucketize a categorical feature for improvement. 11. Use BigQuery TRANSFORM clause during CREATE_MODEL for simple feature engineering, or sequence of queries, Data Fusion, or Vertex AI AutoML Tables for more complex tasks. 12. All statements about using BigQuery ML with Vertex AI are correct except bringing TensorFlow models into BigQuery ML (it can be done but is not recommended). 13. Vertex AI Pipelines will

---

---

### 20.9. Summary: Review Questions (Part 3/3)

work with BigQuery and all other statements are correct. Note: I've kept only the essential information from each scenario, omitting redundant or unnecessary details.

---

---

<!-- _class: lead -->

# 21. AppendixAnswers to Review Questions

---

---

## 21.1. Chapter 1: Framing ML Problems

---

---

### 21.1. Summary: Chapter 1: Framing ML Problems

Here are concise two-sentence summaries:

A model for predicting future sales should be designed using unsupervised learning techniques such as clustering and topic modeling to identify patterns in historical data, and then validated using metrics like recall to minimize false negatives.
Supervised learning is used when dealing with labeled customer data, while unsupervised methods like collaborative filtering are suitable for recommendations based on user behavior and product interactions.

---

---

## 21.2. Chapter 2: Exploring Data and Building Data Pipelines

---

---

### 21.2. Summary: Chapter 2: Exploring Data and Building Data Pipelines

The issue lies in label leakage due to oversampling, where training on hospital names affects new patient data performance. Transforming data before splitting into testing and training sets can help avoid data leakage, but removing features with missing values is not a relevant solution to this problem.

---

---

## 21.3. Chapter 3: Feature Engineering

---

---

### 21.3. Summary: Chapter 3: Feature Engineering

Here are concise summaries of each point:

A. One-hot encoding converts categorical features to numeric features to improve algorithm performance.
B. Normalizing data converts range to normalized format, helping model convergence.

C. AUC PR minimizes false positives in imbalanced datasets compared to AUC ROC.

D. Cross-validation prevents data leakage by using separate test sets for training.

E. Prefetching and interleaving improve processing time with TensorFlow data pipelines.

F. Cloud Data Fusion is a UI-based tool for ETL, while TensorFlow Transform is more scalable.

G. Separating transformations for training and testing prevents training-serving skew in models.

---

---

## 21.4. Chapter 4: Choosing the Right ML Infrastructure

---

---

### 21.4. Summary: Chapter 4: Choosing the Right ML Infrastructure (Part 1/3)

Here are the concise summaries: A. Use AutoML Edge model type for classification problems and deploy on edge devices with Android app deployment to meet market requirements. B. To optimize CPU usage, use option D which is better than options A and B in terms of instance sizing, as it goes with 1 TPU instead of 8 GPUs. C. The Google Translate Glossary feature can replace specific words/phrases with translations from a predefined list, while AutoML Edge and custom models are not recommended for first steps. D. The correct recommendations are: "Recommended for you" for home pages, "Similar items" based on product information only, "Others you may like" based on browsing history, and "Frequently bought together" at checkout to engage customers. E. To increase engagement, use "Others you may

---

---

### 21.4. Summary: Chapter 4: Choosing the Right ML Infrastructure (Part 2/3)

like", "Recommended for you", or "Frequently bought together", as these aim to spend more time in the website/app browsing through products. F. For projects without user data, use "Similar items" based on project catalog information only, and consider custom models with pre-trained APIs. G. The click-through rate objective focuses on user clicks, while revenue per order captures effectiveness at checkout, requiring different optimization strategies. H. Use a Vertex AI custom job instead of AutoML for this task due to the lack of pretrained API availability on GCP. I. The Natural Language API does not accept voice, and using custom models with pre-trained APIs takes time. J. TPUs are suitable for sparse matrices and high-precision predictions, but may not be efficient, and GPUs support more

---

---

### 21.4. Summary: Chapter 4: Choosing the Right ML Infrastructure (Part 3/3)

devices. K. To identify root cause of latency in deployed model, check the code for single-threaded issues before increasing instance size.

---

---

## 21.5. Chapter 5: Architecting ML Solutions

---

---

### 21.5. Summary: Chapter 5: Architecting ML Solutions

The simplest solution to handle this task is using App Engine with Vertex AI Prediction, eliminating the need for Memorystore and Bigtable. Using Cloud BigQuery as a NoSQL store can also help reduce latency.

---

---

## 21.6. Chapter 6: Building Secure ML Pipelines

---

---

### 21.6. Summary: Chapter 6: Building Secure ML Pipelines

Here are the concise summaries:

Federated learning is used to deploy ML models on local devices where data is stored, while setting up separate projects helps manage resources. To access APIs, a service account key and authentication with GOOGLE_APPLICATION_CREDENTIALS are required, along with Vertex AI IAM access.

---

---

## 21.7. Chapter 7: Model Building

---

---

### 21.7. Summary: Chapter 7: Model Building (Part 1/2)

Here are the concise summaries: * Model underperforming on new data: Retrain when necessary and consider prediction drift or training-serving skew. * Model memorizing training data: Use dropout (20%) instead of increasing neurons, and regularization won't help. * Out-of-memory error: Change batch size to resolve issue. * Multiclass classification: Use sparse categorical cross-entropy for accuracy. * Oscillating loss curves: Reduce learning rate or adjust model settings. * Image classification: Minimize validation loss for optimal performance. * Training time optimization: Increase batch size to reduce training time. * One-hot encoding: Use categorical cross-entropy for correct loss function. * Overfitting: Regularize with L2 to balance bias and variance. * Hyperparameter tuning: Example

---

---

### 21.7. Summary: Chapter 7: Model Building (Part 2/2)

of tuning model settings. * MirroredStrategy: Used on single machine.

---

---

## 21.8. Chapter 8: Model Training and Hyperparameter Tuning

---

---

### 21.8. Summary: Chapter 8: Model Training and Hyperparameter Tuning

To train TensorFlow code with minimal manual intervention and low computation time, use BigQuery SQL for data processing and enable an interactive shell to evaluate model metrics during training. Additionally, utilize Vertex AI's hyperparameter tuning feature and consider using custom containers or Bigtable for large datasets.

---

---

## 21.9. Chapter 9: Model Explainability on Vertex AI

---

---

### 21.9. Summary: Chapter 9: Model Explainability on Vertex AI

Shapley values, integrated gradients, and sampled Shapley are methods for feature attribution in machine learning models, particularly useful for nondifferentiable models like TensorFlow graphs that perform decoding tasks; these techniques provide informative features in a model. Integrated gradients and XRAI are also supported attribution techniques, while local kernel explanations can be used to detect misclassifications in predictions using the Explainable AI SDK.

---

---

## 21.10. Chapter 10: Scaling Models in Production

---

---

### 21.10. Summary: Chapter 10: Scaling Models in Production

To meet low-latency requirements, Bigtable should be used for dynamic feature lookup and caching predictions in a Cloud Bigtable datastore will provide the least latency. Embedding the client on the website, deploying the gateway on App Engine, and deploying the model using Vertex AI Prediction requires minimal effort to setup.

---

---

## 21.11. Chapter 11: Designing ML Training Pipelines

---

---

### 21.11. Summary: Chapter 11: Designing ML Training Pipelines

You can use TFX or Kubeflow to create performance benchmarks and orchestrate TensorFlow pipelines, utilizing serverless setup for minimal effort. Cloud Build and event-based triggers can be used to automate testing and scheduling of Kubeflow Pipelines and Vertex AI pipelines.

---

---

## 21.12. Chapter 12: Model Monitoring, Tracking, and Auditing Metadata

---

---

### 21.12. Summary: Chapter 12: Model Monitoring, Tracking, and Auditing Metadata (Part 1/2)

Here are the concise summaries: 1. Data drift occurs when input distribution changes, requiring retraining or new data collection. Concept drift involves changing relationship between inputs and predicted values. 2. Training-serving skew refers to comparing training data's statistical distribution with production's, while prediction drift compares production data's distribution over time. 3. Chebyshev distance measures greatest distance between two vectors; sampling rate reduces consumed data. 4. Options A (AutoML), B (custom models), and D (schema) are valid for custom models' data processing. 5. String and number types are supported in custom models; schema not required. 6. Request-response logging provides a sample in BigQuery table; container, access, and request-response logging

---

---

### 21.12. Summary: Chapter 12: Model Monitoring, Tracking, and Auditing Metadata (Part 2/2)

valid. 7. Artifact refers to metadata store elements, not all information is an artifact. 8. Artifact, context, and execution are data model elements.

---

---

## 21.13. Chapter 13: Maintaining ML Solutions

---

---

### 21.13. Summary: Chapter 13: Maintaining ML Solutions

Here is a concise summary of the content in two sentences:

MLOps Level 1, also known as Strategic, is suitable for organizations experimenting with machine learning and dealing with only one model, whereas Level 2 is better suited for those handling many models and algorithms. The correct approach to triggering retraining involves monitoring performance, sudden drops or threshold degradations, but not triggers based on data access time or predictions.

---

---

## 21.14. Chapter 14: BigQuery ML

---

---

### 21.14. Summary: Chapter 14: BigQuery ML

A: Option A is correct but missing ARIMA_PLUS algorithm, which is intended for this use in BigQuery ML matrix factorization model.
 
B: Option B is wrong because there's no feature on Google Maps today that uses Matrix Factorization.

C: Option C is the correct answer as it leverages BigQuery ML's matrix factorization model type and is a quick solution suitable for data analysts.

D: Options A, B, and D are incorrect in this context as they involve unnecessary data transfers or retraining.

---

---

<!-- _class: lead -->

# 22. Index

---

---

## 22.1. A

---

---

### 22.1. Summary: A

The content covers various topics in the field of Artificial Intelligence and Machine Learning (AI/ML), including optimization algorithms such as AdaGrad and Adam, security measures like AEAD, and AI best practices for fairness, interpretability, and privacy. Additionally, it discusses different AI/ML stacks like Apache Airflow and Beam, AutoML tools like CCAI and Vertex AI, and their applications in various domains such as customer service and retail.

---

---

## 22.2. B

---

---

### 22.2. Summary: B

BigQuery and its related tools provide a comprehensive platform for collecting, processing, and analyzing large datasets, including batch prediction, machine learning, and data visualization capabilities, with features such as data transfer, integration, and Omni. It supports various programming languages like Python and integrates with other services like Dataproc, Jupyter Notebooks, and Pub/Sub for efficient data analysis and deployment of predictive models.

---

---

## 22.3. C

---

---

### 22.3. Summary: C

Caching architecture and data encoding methods include feature hashing, one-hot encoding, label encoding, integer encoding, mapping embedding, and hybrid models, used in CI/CD pipelines for balanced classes. Common machine learning algorithms like CNNs, binary classification, and multiclass classification use techniques such as clipping, clustering, and correlation analysis to optimize performance.

---

---

## 22.4. D

---

---

### 22.4. Summary: D

This content appears to be a technical document on data processing and machine learning with Google Cloud products. Key concepts include data augmentation, model training using BigQuery, Cloud Dataflow, and TensorFlow Transform library, as well as differential privacy, data quality, and visualization tools.

---

---

## 22.5. E

---

---

### 22.5. Summary: E

BigQuery provides tools for Exploratory Data Analysis (EDA), including visualization, bivariate and univariate analysis, and edge inference, with features such as encryption at rest, in transit, and server-side, utilizing technologies like Edge TPU and embedding, to facilitate explainability through Vertex Explainable AI. BigQuery ML offers global and local models for machine learning tasks, including Format-Preserving Encryption (FPE) for data protection.

---

---

## 22.6. F

---

---

### 22.6. Summary: F

This project involves developing a machine learning model with TensorFlow for Android and iOS deployment using TFX and Edge TPU, while handling data quality and imbalanced classes, and incorporating features such as feature engineering and federated learning. The model is deployed on various edge devices with different machine types, including Cloud Data Fusion and Dataprep by Trifacta, to improve accuracy and scalability.

---

---

## 22.7. G

---

---

### 22.7. Summary: G

GANs are used in Google Cloud Platform for AI and machine learning tasks, while GCP also offers a range of APIs and services including Cloud Storage and Healthcare API. Notable integrations include Github, GNMT, DICOM, FHIR, GPUs, ALUs, model parallelism, virtual CPUs, and gradient descent.

---

---

## 22.8. H

---

---

### 22.8. Summary: H

Google Cloud hosts third-party pipelines like MLFlow for hybrid cloud strategies and hyperparameter tuning algorithms including Bayesian search, grid search, importance, optimization speed, parameter comparison, random search. Additionally, Vertex AI is used with Vizier to analyze review question answers.

---

---

## 22.9. I

---

---

### 22.9. Summary: I

IAM (identity and access management), 104 FPE (Format‐Preserving Encryption), 113 project‐level roles, 105 resource‐level roles, 105 Vertex AI and, 106 federated learning, 112 Vertex AI Workbench permissions, 106 –108 infrastructure, 86 review question answers, 302 –304 inside model data transformation, 41 integer encoding, 43 interactive shells, 175 –176

---

---

## 22.10. J

---

---

### 22.10. Summary: J

JupyterLab features, 154 –155

---

---

## 22.11. K

---

---

### 22.11. Summary: K

k‐NN (k‐nearest neighbors) algorithm, missing data, 32 Kubeflow DSL, system design, 232 –233 pipeline components, 233 Kubeflow Pipelines, 92 –93, 224 –225, 229 workflow scheduling, 230 –232 Kubernetes Engine, 87

---

---

## 22.12. L

---

---

### 22.12. Summary: L

This text appears to be a list of technical terms related to machine learning and data analysis. Key concepts include label encoding, online prediction, and model monitoring, with specific techniques such as LOCF, log scaling, and logging being used to implement these methods.

---

---

## 22.13. M

---

---

### 22.13. Summary: M

Google Cloud AI Platform has 12 managed datasets, with a maximum of 75 QPS (queries per second) and 75 restrictions. It supports MAE (mean absolute error), JupyterLab features, and Vertex AI Workbench BigQuery integration for data analysis and machine learning tasks.

---

---

## 22.14. N

---

---

### 22.14. Summary: N

Naive Bayes and various machine learning models such as ANNs, CNNs, DNNs, and RNNs are affected by missing data, with 32 NaN errors and 42 NaN values found. Normalization techniques like normalization and mapping bucketing are used to address issues like numeric data, NoSQL data stores, and correlation analysis of -62 to -63 negative correlations.

---

---

## 22.15. O

---

---

### 22.15. Summary: O

Offline data augmentation and online prediction methods are used to prepare and make predictions on datasets. Models can be deployed through various frameworks such as Apache Airflow, Cloud Composer, Kubeflow Pipelines, or Vertex AI Pipelines for efficient optimization of hyperparameters.

---

---

## 22.16. P

---

---

### 22.16. Summary: P

Google Cloud Healthcare API uses Data Loss Prevention (DLP) to protect PHI. The API provides a CI/CD pipeline with Kubeflow for workflow scheduling, and TensorFlow Extended SDK for model deployment.

---

---

## 22.17. Q

---

---

### 22.17. Summary: Q

quality of data, 24 –27

---

---

## 22.18. R

---

---

### 22.18. Summary: R

Random Forest models can detect concept drift and adapt to changing data, triggering retraining through performance-based or periodic triggers, with metrics such as RMSE and RMSLE evaluating model accuracy. Concept drift is also relevant in Retail AI, where data changes trigger retraining of models using techniques like RNNs and request-response logging to evaluate reliability and performance.

---

---

## 22.19. S

---

---

### 22.19. Summary: S

This content appears to be a list of technical terms related to machine learning and data science. Key concepts include SaaS, semi-supervised learning, sensitivity to data removal, Seq2seq+, server-side encryption, skewed data, SMOTE, streaming data, structured and unstructured data, standard deviation, statistics correlation, and system design frameworks like Kubeflow DSL and TFX.

---

---

## 22.20. T

---

---

### 22.20. Summary: T

t-SNE and Temporal Fusion Transformer are used in multiclass classification tasks with TensorFlow Serving model. APIs, such as TFX and tf.data API, facilitate exploratory data analysis and system design phases before production pipeline implementation.

---

---

## 22.21. U

---

---

### 22.21. Summary: U

univariate analysis, data visualization, 20 unstructured data, model training and, 145 unsupervised learning, 6 topic modeling, 6 –7 user‐managed notebook, Vertex AI Workbench, 151 –153, 159 –161

---

---

## 22.22. V

---

---

### 22.22. Summary: V

Here is a concise summary of the content:

Vertex AI offers various tools and features, including AutoML, Model Monitoring, and Pipelines, to support machine learning solution readiness, model retraining, and data management. The platform also provides Explainable AI capabilities through its Explainability term, feature attributions, and global/local explainability options for differentiable and nondifferentiable models.

---

---

## 22.23. W

---

---

### 22.23. Summary: W

WIT (What‐If Tool), 177 –178 Workflow, 216 workpool tasks, distributed training, 168 –169

---

---

## 22.24. Z

---

---

### 22.24. Summary: Z

z‐score, 26 zero correlation, 24

---

---

<!-- _class: lead -->

# 23. Online Test Bank

---

---

## 23.1. Register and Access the Online Test Bank

---

---

### 23.1. Summary: Register and Access the Online Test Bank

To access the online test bank for your book, go to www.wiley.com/go/sybextestprep and follow the registration instructions, including verifying your book ownership with a security code, which will be emailed to you. Once registered, click "Register or Login" and enter the pin code to access the test bank site and view your new test bank at the top of the page.

---

---

<!-- _class: lead -->

# 24. WILEY END USER LICENSE AGREEMENT

---