TÍTULO: Chapter 2Exploring Data and Building Data Pipelines
============================================================

Chapter 2Exploring Data and Building Data Pipelines GOOGLE CLOUD PROFESSIONAL MACHINE LEARNING ENGINEER EXAM OBJECTIVES COVERED IN THIS CHAPTER: 5.1 Developing end‐to‐end ML pipelines. Considerations include: Data and model validation Data collecting and cleaning is a major step in machine learning as your model is only as good as your data. Most of the time in machine learning is spent cleaning data and feature engineering. In this chapter, we will focus on data cleaning and exploratory data analysis (EDA). We will talk about data visualization and statistical techniques to check for bad data (omitted values, outliers, duplicate values). Then we will cover how to normalize the data and handle bad data (such as having a data schema), how to handle missing data, and how to check for data leakage. We will also cover how you can use TensorFlow Data Validation (TFDV) to validate data for large‐scale systems. Visualization Data visualization is a data exploratory technique to find trends and outliers in the data. Data visualization helps in the data cleaning process because you can find out whether your data is imbalanced by visualizing the data on a chart. It also helps in the feature engineering process because you can select features and discard features and see how a feature will influence your model by visualizing it. There are two ways to visualize data: Univariate Analysis In this analysis, each of the features is analyzed independently, such as the range of the feature and whether outliers exist in the data. The most common visuals used for this are box plots and distribution plots. Bivariate Analysis In this analysis, we compare the data between two features. This analysis can be helpful in finding correlation between features. Some of the ways you can perform this analysis are by using line plots, bar plots, and scatterplots. Box Plot A box plot helps visualize the division of observations into defined intervals known as quartiles and how that compares to the entire observation. It represents the data as 25th, 50th, and 75th quartiles. It consists of the body, or interquartile range, where maximum observations are present. Whiskers or straight lines represent the maximum and minimum. Points that lie outside the whiskers will be considered outliers. Figure 2.1 shows a box plot. FIGURE 2.1 Box plot showing quartiles Line Plot A line plot plots the relationship between two variables and is used to analyze the trends for data changes over time. Figure 2.2 shows a line plot. FIGURE 2.2 Line plot Bar Plot A bar plot is used for analyzing trends in data and comparing categorical data such as sales figures every week, the number of visitors to a website, or revenue from a product every month. Figure 2.3 shows a bar plot. FIGURE 2.3 Bar plot Scatterplot A scatterplot is the most common plot used in data science and is mostly used to visualize clusters in a dataset and show the relationship between two variables. Statistics Fundamentals In statistics, we have three measures of central tendency: mean, median, and mode. They help us describe the data and can be used to clean data statistically. Mean Mean is the accurate measure to describe the data when we do not have any outliers present. Median Median is used if there is an outlier in the dataset. You can find the median by arranging data values from the lowest to the highest value. If there are even numbers, the median is the average of two numbers in the middle, and if there are odd numbers, the median is the middle value. For example, in the dataset 1, 1, 2, 4, 6, 6, 9, the median is 4. For the dataset 1, 1, 4, 6, 6, 9, the median is 5. Take the mean of 4 and 6, or (4+6) / 2 = 5. Mode Mode is used if there is an outlier and the majority of the data is the same. Mode is the value or values in the dataset that occur most. For example, for the dataset 1, 1, 2, 5, 5, 5, 9, the mode is 5. Outlier Detection Mean is the measure of central tendency that is affected by the outliers, which in turn impacts standard deviation. For example, consider the following small dataset: [15, 18, 7, 13, 16, 11, 21, 5, 15, 10, 9, 210] By looking at it, one can quickly say 210 is an outlier that is much larger than the other values. As you can see from Table 2.1, there has been a significant change in mean by adding an outlier compared to median and mode. Variance is the average of the squared differences from the mean. TABLE 2.1 Mean, median, and mode for outlier detection With Outlier Without Outlier Mean: 12.72 Mean: 29.16 Median: 13 Median: 14 Mode: 15 Mode: 15 Standard Deviation Standard deviation is the square root of the variance. Standard deviation is an excellent way to identify outliers. Data points that lie more than one standard deviation from the mean can be considered unusual. Covariance is a measure of how much two random variables vary from each other. Correlation Correlation is simply a normalized form of covariance. The value of the correlation coefficient ranges from –1 to +1. The correlation coefficient is also known as Pearson's correlation coefficient. Positive Correlation When we increase the value of one variable, the value of another variable increases respectively; this is called positive correlation. Negative Correlation When we increase the value of one variable, the value of another variable decreases respectively; this is called negative correlation. Zero Correlation When the change in the value of one variable does not impact the other substantially, then it is called zero correlation. Correlation is helpful in detecting label leakage. For highly correlated labels, for example, if you are training a cancer prediction model, you are using hospital name as a feature, which is highly correlated with the target variable, whether a person has cancer. This correlation can cause your model to learn on hospital names. Refer to this video for more details: https://developers.google.com/machine-learning/crash-course/cancer-prediction. Data Quality and Reliability The quality of your model is going to depend on the quality and reliability (or feasibility) of the data. Your model quality will also depend on the size of the training data. Reliability is the degree to which you can trust your data. If your data is unreliable, that means it has missing values, duplicate values, and bad features; you can consider it as unclean data. If you train a model on unclean data, you are less likely to get useful predictions. To ensure your data is reliable, you can do the following: Check for label errors as sometimes humans do labeling and we do make mistakes. Check for noise in features, such as, for example, GPS measurements. Check for outliers and data skew. It's important to have a concrete definition of quality while collecting the data. We will discuss several parameters of data quality in the following sections. Data Skew Data skew means when the normal distribution curve is not symmetric, the data is skewed. It means that there are outliers in the data or the data distribution is not even. The skewness for a normal distribution is 0. The data can be right skewed or left skewed (see Figure 2.4). You can analyze skew by knowing the statistical measure such as mean and median and standard deviation from the dataset. FIGURE 2.4 Data skew For right‐skewed data, a real‐world example can be income data because most people will have an average income and only 0.01 percent will have income higher than rest of the population (billionaires such as Jeff Bezos), leading to outliers, or right skew. Skewed data does not work well with models because having extreme outliers affects the model's capability to predict well. With several transformations such as log transformation and normalization, you can transform skewed distribution to normal distribution by removing outliers. If the skewness is in the target variable, you can use the Synthetic Minority Oversampling Technique (SMOTE), undersampling, or oversampling. Data Cleaning The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model. (See https://developers.google.com/machine-learning/data-prep/transform/normalization.) Scaling Scaling means converting floating‐point feature values from their natural range into a standard range—for example, from 1,000–5,000 to 0 to 1 or –1 to +1. Scaling is useful when a feature set consists of multiple features. It has the following benefits: In deep neural network training, scaled features help gradient descent converge better than non‐scaled features. Scaling removes the possibility of “NaN traps” as every number value is scaled to a range of numbers. Without scaling, the model will give too much importance to features having a wider range. You would use scaling when your data is uniformly distributed or has no skew with few or no outliers; for example, age can be scaled because every range will have a uniform number of people representing age. Log Scaling Log scaling is used when some of the data samples are in the power of law, or very large. For example, you would use log scaling when some of the sample is 10,000 while some is in the range 0–100. So, taking a log will bring them to same range. For example, log of (100,000) = 100 and log of (100) = 10. Therefore, your data will be scaled to the 0 to 100 range with log scaling. Z‐score This is another scaling method where the value is calculated as standard deviations away from the mean. You would calculate the z‐score as follows when you have a few outliers: Scaled value = (value − mean) / stddev For example, given Mean = 100 Standard deviation = 20 Original value = 130 the scaled value is 1.5. The z‐score lies between –3 to +3, so anything outside of that range will be an outlier. Clipping In the case of extreme outliers, you can cap all feature values above or below to a certain fixed value. You can perform feature clipping before or after other normalization techniques. Handling Outliers An outlier is a value that is the odd one out or an observation that lies far from the rest of the data points because it is too large or too small. They may exist in data due to human error or skew. You need to use the following visualization techniques and statistical techniques (some of which were discussed in previous sections) to detect outliers: Box plots Z‐score Clipping Interquartile range (IQR) Once an outlier is detected, you can either remove it from the dataset so that it does not affect model training or impute or replace outlier data to either mean, median, mode, or boundary values. Establishing Data Constraints The data analysis and exploration process leads to key insights and outcomes such as data quality issues (missing values, outliers, and type conversions). To have a consistent and reproducible check, you need to set up the data constraint by defining a schema for your ML pipeline. A schema with defined metadata describes the property of your data, such as data type (numerical vs. categorical), allowed range, format, and distribution of values of the data. A schema is an output of the data analysis process. The following are the advantages of having a schema: For feature engineering and data transformation, your categorical data and numerical data needs to be transformed. Having a schema enables metadata‐driven preprocessing. You can validate new data using the data schema and catch anomalies such as skews and outliers during training and prediction. Exploration and Validation at Big‐Data Scale The volume of data is growing at a fast pace. To train deep neural networks, a large amount of data is needed. The challenge to validate these large datasets is that the data needs to be validated in memory. You will need multiple machines to scale data validation for large datasets. TensorFlow Data Validation (TFDV) can be used to understand, validate, and monitor ML data at scale (see Figure 2.5). TFDV is used for detecting data anomalies and schema anomalies in the data. It is a part of the TensorFlow Extended (TFX) platform and provides libraries for data validation and schema validation for large datasets in an ML pipeline. The key TFX libraries are TensorFlow Data Validation, TensorFlow Transform, used for data preprocessing and feature engineering, TensorFlow Model Analysis for ML model evaluation and analysis, and TensorFlow Serving for serving ML models. FIGURE 2.5 TensorFlow Data Validation You can use TFDV in these ways: Exploratory Data Analysis Phase You can use TFDV to produce a data schema to understand the data for your ML pipeline. This schema can act as a defined contract between your ML pipeline and data. Whenever your schema is violated, you need to fix either your data or the pipeline. Production Pipeline Phase After your model is deployed, this schema can be used to define a baseline to detect any new data causing skew or drift in the model during training and serving. Running TFDV on Google Cloud Platform TFDV core application programming interfaces (APIs) are built on the Apache Beam open source software development kit (SDK) for building batch and streaming pipelines. Dataflow is a managed service that runs Apache Beam data processing pipelines at scale. Dataflow integrates natively with the data warehousing serverless service BigQuery and data lakes (Google Cloud Storage) as well as Vertex AI Pipelines machine learning. Organizing and Optimizing Training Datasets You learned in the previous section how to check for data quality. In this section, we will talk about data sampling, imbalanced data, and how to split your dataset. We generally divide the data into training, test, and validation dataset (see Figure 2.6). FIGURE 2.6 Dataset representation Training Dataset This is the actual dataset that we use to train the model, and our model learns from this data. Validation Dataset This is a subset of data that is used for the evaluation of the model. This data is used for hyperparameter tuning. The model does not use this data to learn from it, unlike training data. It is used for improving the model behavior after training. Test Dataset This is the sample of data used to test or evaluate the performance of your model. The test set is different from the training and validation sets as it's used after model training and model validation. The test set should not contain the data samples in the validation or training set because it might cause data leakage. Also, you should never train your data using the test set. Imbalanced Data When two classes in a dataset are not equal, the result is imbalanced data. In the example shown in Figure 2.7, there is less chance of credit card fraud in the dataset compared to No fraud. In the case of credit card transactions, suppose, out of all transactions, 1,000 are not fraud examples and only five are fraud transactions. This is a classic representation of imbalanced data. In this scenario, we do not have enough transactions to train the model to classify whether a credit card transaction is fraud. The training model will spend more time on no‐fraud scenarios. In a random sampling, you can perform either oversampling, which means duplicating samples from the minority class, or undersampling, which means deleting samples from the majority class. Both of these approaches include bias because they introduce either more samples or fewer samples to remove imbalance. FIGURE 2.7 Credit card data representation An effective way to handle imbalanced data is to downsample and upweight the majority class. Let's now consider the previous example and downsample the majority class, which is no‐fraud examples. Downsampling by 10 will improve the balance (see Figure 2.8). FIGURE 2.8 Downsampling credit card data Now let's upweight the downsampled class. Since we downsampled by a factor of 10, the example weight should be 10. Example weight means counting an individual example as more important during training. An example weight of 10 means the model treats the example as 10 times as important (when computing loss) as it would for an example of weight 1. Refer to https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data to learn more. One of the advantages of downsampling and upweighting the downsampled class is faster model convergence because we have more of the minority class examples compared to the data before downsampling and upweighting. Data Splitting Mostly with general data cleaning, you would start with random splitting of the data. For example, consider datasets having naturally clustered examples. Say you want your model to classify the topics in the text of a book. The topics can be horror, love story, and drama. A random split would be a problem in that case. Why would having a random split cause a skew? It can cause a skew because stories with the same type of topic are written on the same timeline. If the data is split randomly, the test set and training set might contain the same stories. To fix this, try splitting the data based on the time the story was published. For example, you can put stories written in June in the training set and stories published in July in the test set. Another simple approach to fixing this problem would be to split the data based on when the story was published. For example, you could train on stories for the month of April and then use the second week of May as the test set to prevent overlap. Data Splitting Strategy for Online Systems For online systems, it's recommended to split the data by time as the training data is older than the serving data. By splitting data by time, you ensure your validation set mirrors the lag of time between training and prediction. Here are the steps: Perform data collection for 30 days. Create a training set for data from day 1 to day 29. Create a validation set for data for day 30. Generally, time‐based splits work best with very large datasets, such as, for example, data with millions of examples. When it is clustered data—for example, a book or an online system—you have to make sure that you split the data in a way that your model will not get trained using information not available at prediction time. That's why you should use a time‐based approach rather than a random split. For data splitting, you should always use domain knowledge to understand when a random split will work versus when to do a time‐based split. Handling Missing Data Why do we have missing data? In real‐world scenarios, you would have missing data as a result of failure to record data, or it could be due to data corruption. Some examples of missing data in structured data can be NaN values or a null value for a given feature. So, to train your model effectively, you would need to think of ways to handle missing data. The following list includes some of the ways to handle missing data: Delete the rows or columns with missing values such as null or NaN. If a column has more than half of the rows as null, then the entire column can be dropped. The rows that have one or more column values as null can also be dropped. The disadvantage of this approach is that information is lost and the model will perform poorly if a large percentage of data is missing in a complete dataset. Replace missing values for numeric continuous columns or features such as age or interest rate in your dataset having missing values. You can replace missing values with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to removing or deleting the columns or rows. This method works well with small datasets. Also, this method does not factor in the covariance between features and can cause data leakage, which we will discuss in the next section. For missing values in categorical columns (string or numerical), the missing values can be replaced with the most frequent category. This method is called the imputation method. If the number of missing values is very large, then missing values can be replaced with a new category. The disadvantage of this method is encoding the extra added features while one‐hot encoding. You can use the last valid observation to fill in the missing value. This is known as the last observation carried forward (LOCF) method. This method can reduce bias. For the time‐series dataset, you can use the interpolation of the variable before and after a time stamp for a missing value. ML algorithms that can ignore missing values in data can also be used. For example, the k‐nearest neighbors (k‐NN) algorithm can ignore a column from a distance measure when a value is missing. Naive Bayes can also support missing values when making a prediction. Also, the Random Forest algorithm works well on nonlinear and categorical data. It adapts to the data structure, taking into consideration the high variance or the bias, producing better results on large datasets. Use machine learning to predict missing values. The correlation between the variable containing the missing value and other variables can be used to predict missing values. The regression or classification model can be used for the prediction of missing values based on non‐missing variables. Data Leakage Data leakage happens when you expose your machine learning model to the test data during training. As a result, your model performs great during training and testing, but when you expose the model to unseen data, it underperforms. Data leakage leads to overfitting in the model as the model has already learned from the test and training data. The following are some of the reasons for data leakage: The target variable is the output that your model is trying to predict, and features are the data that is fed into the model to make a prediction or predict the target variable. The cause of data leakage is that by mistake you have added your target variable as your feature. While splitting the test data and the training data for model training, you have included the test data with the training data. The presence of features that expose the information about the target variable will not be available after the model is deployed. This is also called label leakage and can be detected by checking the correlation between the target variable and the feature. Applying preprocessing techniques (normalizing features, removing outliers, etc.) to the entire dataset will cause the model to learn not only the training set but also the test set, which leads to data leakage. A classic example of data leakage is time‐series data. For example, when dealing with time‐series data, if we use data from the future when doing computations for current features or predictions, we would highly likely end up with a leaked model. It generally happens when the data is randomly split into train and test subsets. These are the situations where you might have data leakage: If the model's predicted output is as good as actual output, it might be because of a data leakage. This means the model might be somehow memorizing the data or might have been exposed to the actual data. While doing the exploratory data analysis, having features that are very highly correlated with the target variable might be a data leakage. Data leakage happens primarily because of the way we split our data and when we split our data. Now, let's understand how to prevent data leakage: Select features that are not correlated with a given target variable or that don't contain information about the target variable. Split the data into test, train, and validation sets. The purpose of the validation set is to mimic the real‐life scenario and it will help identify any possible case of overfitting. Preprocess the training and test data separately. You would perform normalization on training data rather than on the complete dataset to avoid any leakage. In case of time‐series data, have a cutoff value on time as it prevents you from getting any information after the time of prediction. Cross‐validation is another approach to avoid data leakage when you have limited data. However, if data leakage still happens, then scale or normalize the data and compute the parameters on each fold of cross‐validation separately. Furthermore, the difference in production data versus training data must be reflected in the difference between the validation data split and the training data split and between the testing data split and the validation data split. For example, if you are planning on making predictions about user lifetime value (LTV) over the next 30 days, then make sure that the data in your validation data split is from 30 days after the data in your training data split and that the data in your testing data split is from 30 days before your validation data split. Summary In this chapter, we discussed why we need to visualize data and the various ways to visualize data, such as using box plots, line plots, and scatterplots. Then we covered statistical fundamentals such as mean, median, mode, and standard deviation and why they are relevant when finding outliers in data. Also, you learned how to check data correlation using a line plot. You learned about various data cleaning and normalizing techniques such as log scaling, scaling, clipping, and using a z‐score to improve the quality of data. We also discussed establishing data constraints and why it's important to define a data schema in an ML pipeline and the need to validate data. We covered using TFDV for validating data at scale and why you need TFDV to validate data schema for large‐scale deep learning systems. Then we discussed the strategy used for splitting the data and spoke about the data splitting strategy for an imbalanced dataset. We covered splitting based on time for online systems and clustered data. Last, we covered strategies for how to deal with missing data and data leakage. Exam Essentials Be able to visualize data. Understand why we need to visualize data and various ways to do so, such as using box plots, line plots, and scatterplots. Understand the fundamentals of statistical terms. Be able to describe mean, median, mode, and standard deviation and how they are relevant in finding outliers in data. Also know how to check data correlation using a line plot. Determine data quality and reliability or feasibility. Understand why you want data without outliers and what data skew is, and learn about various data cleaning and normalizing techniques such as log scaling, scaling, clipping, and z‐score. Establish data constraints. Understand why it's important to define a data schema in an ML pipeline and the need to validate data. Also, you need to understand TFDV for validating data at scale. Organize and optimize training data. You need to understand how to split your dataset into training data, test data, and validation data and how to apply the data splitting technique when you have clustered and online data. Also understand the sampling strategy when you have imbalanced data. Handle missing data. Know the various ways to handle missing data, such as removing missing values; replacing missing values with mean, median, or mode; or using ML to create missing values. Avoid data leaks. Know the various ways data leakage and label leakage can happen in the data and how to avoid it. Review Questions You are the data scientist for your company. You have a dataset that includes credit card transactions, and 1 percent of those credit card transactions are fraudulent. Which data transformation strategy would likely improve the performance of your classification model? Write your data in TFRecords. Z‐normalize all the numeric features. Use one‐hot encoding on all categorical features. Oversample the fraudulent transactions. You are a research scientist building a cancer prediction model from medical records. Features of the model are patient name, hospital name, age, vitals, and test results. This model performed really well on held‐out test data but performed poorly on new patient data. What is the reason for this? Strong correlation between feature hospital name and predicted result. Random splitting of data between all the features available. Missing values in the feature hospital name and age. Negative correlation between the feature hospital name and age. Your team trained and tested a deep neural network model with 99 percent accuracy. Six months after model deployment, the model is performing poorly due to change in input data distribution. How should you address input data distribution? Create alerts to monitor for skew and retrain your model. Perform feature selection and retrain the model. Retrain the model after hyperparameter tuning. Retrain your model monthly to detect data skew. You are an ML engineer who builds and manages a production system to predict sales. Model accuracy is important as the production model has to keep up with market changes. After a month in production, the model did not change but the model accuracy was reduced. What is the most likely cause of the reduction in model accuracy? Accuracy dropped due to poor quality data. Lack of model retraining. Incorrect data split ratio in validation, test, and training data. Missing data for training. You are a data scientist in a manufacturing firm. You have been asked to investigate failure of a production line based on sensor readings. You realize that 1 percent of the data samples are positive examples of a faulty sensor reading. How will you resolve the class imbalance problem? Generate 10 percent positive examples using class distribution. Downsample the majority data with upweighting to create 10 percent samples. Delete negative examples until positive and negative examples are equal. Use a convolutional neural network with the softmax activation function. You are the data scientist of a meteorological department asked to build a model to predict daily temperatures. You split the data randomly and then transform the training and test datasets. Temperature data for model training is uploaded hourly. During testing, your model performed with 99 percent accuracy; however, in production, accuracy dropped to 70 percent. How can you improve the accuracy of your model in production? Split the training and test data based on time rather than a random split to avoid leakage. Normalize the data for the training and test datasets as two separate steps. Add more data to your dataset so that you have fair distribution. Transform data before splitting, and cross‐validate to make sure the transformations are applied to both the training and test sets. You are working on a neural‐network‐based project. The dataset provided to you has columns with different ranges and a lot of missing values. While preparing the data for model training, you discover that gradient optimization is having difficulty moving weights. What should you do? Use feature construction to combine the strongest features. Use the normalization technique to transform data. Improve the data cleaning step by removing features with missing values. Change the hyperparameter tuning steps to reduce the dimension of the test set and have a larger training set. You are an ML engineer working to set a model in production. Your model performs well with training data. However, the model performance degrades in production environment and your model is overfitting. What can be the reason for this? (Choose three.) Applying normalizing features such as removing outliers to the entire dataset High correlation between the target variable and the feature Removing features with missing values Adding your target variable as your feature