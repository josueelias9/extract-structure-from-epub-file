{
  "Table of Contents": {
    "content": "1. Cover\n  2. Table of Contents\n  3. Title Page\n  4. Copyright\n  5. Dedication\n  6. Acknowledgments\n  7. About the Author\n  8. About the Technical Editors\n     1. About the Technical Proofreader\n     2. Google Technical Reviewer\n  9. Introduction\n     1. Google Cloud Professional Machine Learning Engineer Certification\n     2. Who Should Buy This Book\n     3. How This Book Is Organized\n     4. Bonus Digital Contents\n     5. Conventions Used in This Book\n     6. Google Cloud Professional ML Engineer Objective Map\n     7. How to Contact the Publisher\n     8. Assessment Test\n     9. Answers to Assessment Test\n  10. Chapter 1: Framing ML Problems\n     1. Translating Business Use Cases\n     2. Machine Learning Approaches\n     3. ML Success Metrics\n     4. Responsible AI Practices\n     5. Summary\n     6. Exam Essentials\n     7. Review Questions\n  11. Chapter 2: Exploring Data and Building Data Pipelines\n     1. Visualization\n     2. Statistics Fundamentals\n     3. Data Quality and Reliability\n     4. Establishing Data Constraints\n     5. Running TFDV on Google Cloud Platform\n     6. Organizing and Optimizing Training Datasets\n     7. Handling Missing Data\n     8. Data Leakage\n     9. Summary\n     10. Exam Essentials\n     11. Review Questions\n  12. Chapter 3: Feature Engineering\n     1. Consistent Data Preprocessing\n     2. Encoding Structured Data Types\n     3. Class Imbalance\n     4. Feature Crosses\n     5. TensorFlow Transform\n     6. GCP Data and ETL Tools\n     7. Summary\n     8. Exam Essentials\n     9. Review Questions\n  13. Chapter 4: Choosing the Right ML Infrastructure\n     1. Pretrained vs. AutoML vs. Custom Models\n     2. Pretrained Models\n     3. AutoML\n     4. Custom Training\n     5. Provisioning for Predictions\n     6. Summary\n     7. Exam Essentials\n     8. Review Questions\n  14. Chapter 5: Architecting ML Solutions\n     1. Designing Reliable, Scalable, and Highly Available ML Solutions\n     2. Choosing an Appropriate ML Service\n     3. Data Collection and Data Management\n     4. Automation and Orchestration\n     5. Serving\n     6. Summary\n     7. Exam Essentials\n     8. Review Questions\n  15. Chapter 6: Building Secure ML Pipelines\n     1. Building Secure ML Systems\n     2. Identity and Access Management\n     3. Privacy Implications of Data Usage and Collection\n     4. Summary\n     5. Exam Essentials\n     6. Review Questions\n  16. Chapter 7: Model Building\n     1. Choice of Framework and Model Parallelism\n     2. Modeling Techniques\n     3. Transfer Learning\n     4. Semi‐supervised Learning\n     5. Data Augmentation\n     6. Model Generalization and Strategies to Handle Overfitting and Underfitting\n     7. Summary\n     8. Exam Essentials\n     9. Review Questions\n  17. Chapter 8: Model Training and Hyperparameter Tuning\n     1. Ingestion of Various File Types into Training\n     2. Developing Models in Vertex AI Workbench by Using Common Frameworks\n     3. Training a Model as a Job in Different Environments\n     4. Hyperparameter Tuning\n     5. Tracking Metrics During Training\n     6. Retraining/Redeployment Evaluation\n     7. Unit Testing for Model Training and Serving\n     8. Summary\n     9. Exam Essentials\n     10. Review Questions\n  18. Chapter 9: Model Explainability on Vertex AI\n     1. Model Explainability on Vertex AI\n     2. Summary\n     3. Exam Essentials\n     4. Review Questions\n  19. Chapter 10: Scaling Models in Production\n     1. Scaling Prediction Service\n     2. Serving (Online, Batch, and Caching)\n     3. Google Cloud Serving Options\n     4. Hosting Third‐Party Pipelines (`MLflow`) on Google Cloud\n     5. Testing for Target Performance\n     6. Configuring Triggers and Pipeline Schedules\n     7. Summary\n     8. Exam Essentials\n     9. Review Questions\n  20. Chapter 11: Designing ML Training Pipelines\n     1. Orchestration Frameworks\n     2. Identification of Components, Parameters, Triggers, and Compute Needs\n     3. System Design with Kubeflow/TFX\n     4. Hybrid or Multicloud Strategies\n     5. Summary\n     6. Exam Essentials\n     7. Review Questions\n  21. Chapter 12: Model Monitoring, Tracking, and Auditing Metadata\n     1. Model Monitoring\n     2. Model Monitoring on Vertex AI\n     3. Logging Strategy\n     4. Model and Dataset Lineage\n     5. Vertex AI Experiments\n     6. Vertex AI Debugging\n     7. Summary\n     8. Exam Essentials\n     9. Review Questions\n  22. Chapter 13: Maintaining ML Solutions\n     1. MLOps Maturity\n     2. Retraining and Versioning Models\n     3. Feature Store\n     4. Vertex AI Permissions Model\n     5. Common Training and Serving Errors\n     6. Summary\n     7. Exam Essentials\n     8. Review Questions\n  23. Chapter 14: BigQuery ML\n     1. BigQuery – Data Access\n     2. BigQuery ML Algorithms\n     3. Explainability in BigQuery ML\n     4. BigQuery ML vs. Vertex AI Tables\n     5. Interoperability with Vertex AI\n     6. BigQuery Design Patterns\n     7. Summary\n     8. Exam Essentials\n     9. Review Questions\n  24. Appendix: Answers to Review Questions\n     1. Chapter 1: Framing ML Problems\n     2. Chapter 2: Exploring Data and Building Data Pipelines\n     3. Chapter 3: Feature Engineering\n     4. Chapter 4: Choosing the Right ML Infrastructure\n     5. Chapter 5: Architecting ML Solutions\n     6. Chapter 6: Building Secure ML Pipelines\n     7. Chapter 7: Model Building\n     8. Chapter 8: Model Training and Hyperparameter Tuning\n     9. Chapter 9: Model Explainability on Vertex AI\n     10. Chapter 10: Scaling Models in Production\n     11. Chapter 11: Designing ML Training Pipelines\n     12. Chapter 12: Model Monitoring, Tracking, and Auditing Metadata\n     13. Chapter 13: Maintaining ML Solutions\n     14. Chapter 14: BigQuery ML\n  25. Index\n  26. End User License Agreement",
    "subsections": {
      "List of Tables": {
        "content": "1. Chapter 1\n     1. TABLE 1.1 ML problem types\n     2. TABLE 1.2 Structured data\n     3. TABLE 1.3 Time‐Series Data\n     4. TABLE 1.4 Confusion matrix for a binary classification example\n     5. TABLE 1.5 Summary of metrics\n  2. Chapter 2\n     1. TABLE 2.1 Mean, median, and mode for outlier detection\n  3. Chapter 3\n     1. TABLE 3.1 One‐hot encoding example\n     2. TABLE 3.2 Run a TFX pipeline on GCP\n  4. Chapter 4\n     1. TABLE 4.1 Vertex AI AutoML Tables algorithms\n     2. TABLE 4.2 AutoML algorithms\n     3. TABLE 4.3 Problems solved using AutoML\n     4. TABLE 4.4 Summary of the recommendation types available in Retail AI\n  5. Chapter 5\n     1. TABLE 5.1 ML workflow to GCP services mapping\n     2. TABLE 5.2 When to use BigQuery ML vs. AutoML vs. a custom model\n     3. TABLE 5.3 Google Cloud tools to read BigQuery data\n     4. TABLE 5.4 NoSQL data store options\n  6. Chapter 6\n     1. TABLE 6.1 Difference between server‐side and client‐side encryption\n     2. TABLE 6.2 Strategies for handling sensitive data\n     3. TABLE 6.3 Techniques to handle sensitive fields in data\n  7. Chapter 7\n     1. TABLE 7.1 Distributed training strategies using TensorFlow\n     2. TABLE 7.2 Summary of loss functions based on ML problems\n     3. TABLE 7.3 Differences between L1 and L2 regularization\n  8. Chapter 8\n     1. TABLE 8.1 Dataproc connectors\n     2. TABLE 8.2 Data storage guidance on GCP for machine learning\n     3. TABLE 8.3 Differences between managed and user‐managed notebooks\n     4. TABLE 8.4 Worker pool tasks in distributed training\n     5. TABLE 8.5 Search algorithm options for hyperparameter tuning on GCP\n     6. TABLE 8.6 Tools to track metric or profile training metrics\n     7. TABLE 8.7 Retraining strategies\n  9. Chapter 9\n     1. TABLE 9.1 Explainable techniques used by Vertex AI\n  10. Chapter 10\n     1. TABLE 10.1 Static vs. dynamic features\n     2. TABLE 10.2 Input data options for batch training in Vertex AI\n     3. TABLE 10.3 ML orchestration options\n  11. Chapter 11\n     1. TABLE 11.1 Kubeflow Pipelines vs. Vertex AI Pipelines vs. Cloud Composer\n  12. Chapter 13\n     1. TABLE 13.1 Table of baseball batters\n  13. Chapter 14\n     1. TABLE 14.1 Models available on BigQuery ML\n     2. TABLE 14.2 Model types",
        "subsections": {},
        "summary": "* **Summary**: The document covers multiple chapters with various machine learning topics, including data types, metrics, outlier detection, AutoML algorithms, and model deployment.\n    * *_Chapters:_*\n        * 1-8: Covering data preprocessing, machine learning workflows, and deployment on Google Cloud Platform (GCP)\n        * 9-12: Focusing on explainable AI, model interpretation, and pipeline options\n        * *_Note:_* Chapters 13 is an outlier with a table of baseball batters."
      },
      "List of Illustrations": {
        "content": "1. Chapter 1\n     1. FIGURE 1.1 Business case to ML problem\n     2. FIGURE 1.2 AUC\n     3. FIGURE 1.3 AUC PR\n  2. Chapter 2\n     1. FIGURE 2.1 Box plot showing quartiles\n     2. FIGURE 2.2 Line plot\n     3. FIGURE 2.3 Bar plot\n     4. FIGURE 2.4 Data skew\n     5. FIGURE 2.5 TensorFlow Data Validation\n     6. FIGURE 2.6 Dataset representation\n     7. FIGURE 2.7 Credit card data representation\n     8. FIGURE 2.8 Downsampling credit card data\n  3. Chapter 3\n     1. FIGURE 3.1 Difficult to separate by line or a linear method\n     2. FIGURE 3.2 Difficult to separate classes by line\n     3. FIGURE 3.3 Summary of feature columnsGoogle Cloud via Coursera, www.coursera...\n     4. FIGURE 3.4 TensorFlow Transform\n  4. Chapter 4\n     1. FIGURE 4.1 Pretrained, AutoML, and custom models\n     2. FIGURE 4.2 Analyzing a photo using Vision AI\n     3. FIGURE 4.3 Vertex AI AutoML, providing a “budget”\n     4. FIGURE 4.4 Choosing the size of model in Vertex AI\n     5. FIGURE 4.5 TPU system architecture\n  5. Chapter 5\n     1. FIGURE 5.1 Google AI/ML stack\n     2. FIGURE 5.2 Kubeflow Pipelines and Google Cloud managed services\n     3. FIGURE 5.3 Google Cloud architecture for performing offline batch prediction...\n     4. FIGURE 5.4 Google Cloud architecture for online prediction\n     5. FIGURE 5.5 Push notification architecture for online prediction\n  6. Chapter 6\n     1. FIGURE 6.1 Creating a user‐managed Vertex AI Workbench notebook\n     2. FIGURE 6.2 Managed Vertex AI Workbench notebook\n     3. FIGURE 6.3 Permissions for a managed Vertex AI Workbench notebook\n     4. FIGURE 6.4 Creating a private endpoint in the Vertex AI console\n     5. FIGURE 6.5 Architecture for de‐identification of PII on large datasets using...\n  7. Chapter 7\n     1. FIGURE 7.1 Asynchronous data parallelism\n     2. FIGURE 7.2 Model parallelism\n     3. FIGURE 7.3 Training strategy with TensorFlow\n     4. FIGURE 7.4 Artificial or feedforward neural network\n     5. FIGURE 7.5 Deep neural network\n  8. Chapter 8\n     1. FIGURE 8.1 Google Cloud data and analytics overview\n     2. FIGURE 8.2 Cloud Dataflow source and sink\n     3. FIGURE 8.3 Summary of processing tools on GCP\n     4. FIGURE 8.4 Creating a managed notebook\n     5. FIGURE 8.5 Opening the managed notebook\n     6. FIGURE 8.6 Exploring frameworks available in a managed notebook\n     7. FIGURE 8.7 Data integration with Google Cloud Storage within a managed noteb...\n     8. FIGURE 8.8 Data Integration with BigQuery within a managed notebook\n     9. FIGURE 8.9 Scaling up the hardware from a managed notebook\n     10. FIGURE 8.10 Git integration within a managed notebook\n     11. FIGURE 8.11 Scheduling or executing code in the notebook\n     12. FIGURE 8.12 Submitting the notebook for execution\n     13. FIGURE 8.13 Scheduling the notebook for execution\n     14. FIGURE 8.14 Choosing TensorFlow framework to create a user‐managed notebook...\n     15. FIGURE 8.15 Create a user‐managed TensorFlow notebook\n     16. FIGURE 8.16 Exploring the network\n     17. FIGURE 8.17 Training in the Vertex AI console\n     18. FIGURE 8.18 Vertex AI training architecture for a prebuilt container\n     19. FIGURE 8.19 Vertex AI training console for pre‐built containersSource: Googl...\n     20. FIGURE 8.20 Vertex AI training architecture for custom containers\n     21. FIGURE 8.21 ML model parameter and hyperparameter\n     22. FIGURE 8.22 Configure hyperparameter tuning by training the pipeline UISourc...\n     23. FIGURE 8.23 Enabling an interactive shell in the Vertex AI consoleSource: Go...\n     24. FIGURE 8.24 Web terminal to access an interactive shellSource: Google LLC.\n  9. Chapter 9\n     1. FIGURE 9.1 SHAP model explainability\n     2. FIGURE 9.2 Feature attribution using integrated gradients for cat image\n  10. Chapter 10\n     1. FIGURE 10.1 TF model serving options\n     2. FIGURE 10.2 Static reference architecture\n     3. FIGURE 10.3 Dynamic reference architecture\n     4. FIGURE 10.4 Caching architecture\n     5. FIGURE 10.5 Deploying to an endpoint\n     6. FIGURE 10.6 Sample prediction request\n     7. FIGURE 10.7 Batch prediction job in Console\n  11. Chapter 11\n     1. FIGURE 11.1 Relation between model data and ML code for MLOps\n     2. FIGURE 11.2 End‐to‐end ML development workflow\n     3. FIGURE 11.3 Kubeflow architecture\n     4. FIGURE 11.4 Kubeflow components and pods\n     5. FIGURE 11.5 Vertex AI Pipelines\n     6. FIGURE 11.6 Vertex AI Pipelines condition for deployment\n     7. FIGURE 11.7 Lineage tracking with Vertex AI Pipelines\n     8. FIGURE 11.8 Lineage tracking in Vertex AI Metadata store\n     9. FIGURE 11.9 Continuous training and CI/CD\n     10. FIGURE 11.10 CI/CD with Kubeflow Pipelines\n     11. FIGURE 11.11 Kubeflow Pipelines on GCP\n     12. FIGURE 11.12 TFX pipelines, libraries, and components\n  12. Chapter 12\n     1. FIGURE 12.1 Categorical features\n     2. FIGURE 12.2 Numerical values\n     3. FIGURE 12.3 Vertex Metadata data model\n     4. FIGURE 12.4 Vertex AI Pipelines showing lineage\n  13. Chapter 13\n     1. FIGURE 13.1 Steps in MLOps level 0\n     2. FIGURE 13.2 MLOps Level 1 or strategic phase\n     3. FIGURE 13.3 MLOps level 2, the transformational phase\n  14. Chapter 14\n     1. FIGURE 14.1 Running a SQL query in the web console\n     2. FIGURE 14.2 Running the same SQL query through a Jupyter Notebook on Vertex ...\n     3. FIGURE 14.3 SQL options for `DNN_CLASSIFIER` and `DNN_REGRESSOR`\n     4. FIGURE 14.4 Query showing results of model evaluation\n     5. FIGURE 14.5 Query results showing only the predictions\n     6. FIGURE 14.6 Global feature importance returned for our model\n     7. FIGURE 14.7 Prediction result\n     8. FIGURE 14.8 Top feature attributions for the prediction",
        "subsections": {},
        "summary": "* **Overview**: The provided content appears to be a comprehensive guide to machine learning (ML) and Google Cloud AI/ML, covering various topics such as data validation, model training, serving, and explainability.\n\n* **Key Concepts**:\n    * *_Asynchronous Data Parallelism_*: enabling concurrent processing of multiple data sources.\n    * *_Model Parallelism_*: distributing a model across multiple hardware units to speed up inference.\n    * *_Hyperparameter Tuning_*: adjusting model parameters to optimize performance."
      },
      "Guide": {
        "content": "1. Cover\n  2. Table of Contents\n  3. Title Page\n  4. Copyright\n  5. Dedication\n  6. Acknowledgments\n  7. About the Author\n  8. About the Technical Editors\n  9. Introduction\n  10. Begin Reading\n  11. Appendix: Answers to Review Questions\n  12. Index\n  13. End User License Agreement",
        "subsections": {},
        "summary": "* *Table of Contents and Preface are missing, only Front Matter is present*\n    * _Table of Contents_\n    * _Introduction_\n    * _End User License Agreement_"
      },
      "Pages": {
        "content": "1. i\n  2. ii\n  3. iii\n  4. v\n  5. vii\n  6. ix\n  7. xxi\n  8. xxii\n  9. xxiii\n  10. xxiv\n  11. xxv\n  12. xxvi\n  13. xxvii\n  14. xxviii\n  15. xxix\n  16. xxx\n  17. xxxi\n  18. xxxii\n  19. xxxiii\n  20. xxxiv\n  21. xxxv\n  22. xxxvi\n  23. xxxvii\n  24. xxxviii\n  25. xxxix\n  26. xl\n  27. 1\n  28. 2\n  29. 3\n  30. 4\n  31. 5\n  32. 6\n  33. 7\n  34. 8\n  35. 9\n  36. 10\n  37. 11\n  38. 12\n  39. 13\n  40. 14\n  41. 15\n  42. 16\n  43. 17\n  44. 18\n  45. 19\n  46. 20\n  47. 21\n  48. 22\n  49. 23\n  50. 24\n  51. 25\n  52. 26\n  53. 27\n  54. 28\n  55. 29\n  56. 30\n  57. 31\n  58. 32\n  59. 33\n  60. 34\n  61. 35\n  62. 36\n  63. 37\n  64. 39\n  65. 40\n  66. 41\n  67. 42\n  68. 43\n  69. 44\n  70. 45\n  71. 46\n  72. 47\n  73. 48\n  74. 49\n  75. 50\n  76. 51\n  77. 52\n  78. 53\n  79. 54\n  80. 55\n  81. 57\n  82. 58\n  83. 59\n  84. 60\n  85. 61\n  86. 62\n  87. 63\n  88. 64\n  89. 65\n  90. 66\n  91. 67\n  92. 68\n  93. 69\n  94. 70\n  95. 71\n  96. 72\n  97. 73\n  98. 74\n  99. 75\n  100. 76\n  101. 77\n  102. 78\n  103. 79\n  104. 80\n  105. 81\n  106. 82\n  107. 83\n  108. 84\n  109. 85\n  110. 86\n  111. 87\n  112. 88\n  113. 89\n  114. 90\n  115. 91\n  116. 92\n  117. 93\n  118. 94\n  119. 95\n  120. 96\n  121. 97\n  122. 98\n  123. 99\n  124. 100\n  125. 101\n  126. 103\n  127. 104\n  128. 105\n  129. 106\n  130. 107\n  131. 108\n  132. 109\n  133. 110\n  134. 111\n  135. 112\n  136. 113\n  137. 114\n  138. 115\n  139. 116\n  140. 117\n  141. 118\n  142. 119\n  143. 120\n  144. 121\n  145. 122\n  146. 123\n  147. 124\n  148. 125\n  149. 126\n  150. 127\n  151. 128\n  152. 129\n  153. 130\n  154. 131\n  155. 132\n  156. 133\n  157. 134\n  158. 135\n  159. 136\n  160. 137\n  161. 138\n  162. 139\n  163. 140\n  164. 141\n  165. 143\n  166. 144\n  167. 145\n  168. 146\n  169. 147\n  170. 148\n  171. 149\n  172. 150\n  173. 151\n  174. 152\n  175. 153\n  176. 154\n  177. 155\n  178. 156\n  179. 157\n  180. 158\n  181. 159\n  182. 160\n  183. 161\n  184. 162\n  185. 163\n  186. 164\n  187. 165\n  188. 166\n  189. 167\n  190. 168\n  191. 169\n  192. 170\n  193. 171\n  194. 172\n  195. 173\n  196. 174\n  197. 175\n  198. 176\n  199. 177\n  200. 178\n  201. 179\n  202. 180\n  203. 181\n  204. 182\n  205. 183\n  206. 184\n  207. 185\n  208. 186\n  209. 187\n  210. 188\n  211. 189\n  212. 190\n  213. 191\n  214. 192\n  215. 193\n  216. 194\n  217. 195\n  218. 196\n  219. 197\n  220. 198\n  221. 199\n  222. 200\n  223. 201\n  224. 202\n  225. 203\n  226. 204\n  227. 205\n  228. 206\n  229. 207\n  230. 208\n  231. 209\n  232. 210\n  233. 211\n  234. 212\n  235. 213\n  236. 214\n  237. 215\n  238. 216\n  239. 217\n  240. 218\n  241. 219\n  242. 220\n  243. 221\n  244. 222\n  245. 223\n  246. 224\n  247. 225\n  248. 226\n  249. 227\n  250. 228\n  251. 229\n  252. 230\n  253. 231\n  254. 232\n  255. 233\n  256. 234\n  257. 235\n  258. 236\n  259. 237\n  260. 238\n  261. 239\n  262. 240\n  263. 241\n  264. 242\n  265. 243\n  266. 244\n  267. 245\n  268. 246\n  269. 247\n  270. 248\n  271. 249\n  272. 250\n  273. 251\n  274. 252\n  275. 253\n  276. 254\n  277. 255\n  278. 256\n  279. 257\n  280. 258\n  281. 259\n  282. 260\n  283. 261\n  284. 262\n  285. 263\n  286. 264\n  287. 265\n  288. 266\n  289. 267\n  290. 268\n  291. 269\n  292. 270\n  293. 271\n  294. 272\n  295. 273\n  296. 274\n  297. 275\n  298. 276\n  299. 277\n  300. 279\n  301. 280\n  302. 281\n  303. 282\n  304. 283\n  305. 284\n  306. 285\n  307. 286\n  308. 287\n  309. 288\n  310. 289\n  311. 290\n  312. 291\n  313. 292\n  314. 293\n  315. 294\n  316. 295\n  317. 296\n  318. 297\n  319. 298\n  320. 299\n  321. 300\n  322. 301\n  323. 302\n  324. 303\n  325. 304\n  326. 305\n  327. 306\n  328. 307\n  329. 308\n  330. 309\n  331. 310\n  332. 311\n  333. 312\n  334. 313\n  335. 314\n  336. 315\n  337. 316\n  338. 317\n  339. 318\n  340. 319\n  341. 320\n  342. 321\n  343. 322\n  344. 323\n  345. 324\n  346. 325\n  347. 326\n  348. 327\n  349. 328\n  350. 329",
        "subsections": {},
        "summary": "* There is no meaningful content to summarize from the provided numbers."
      }
    },
    "summary": "* **Overview of the Book**\n    * Google Cloud Professional Machine Learning Engineer Certification study guide\n    * Comprehensive coverage of machine learning and its applications on Google Cloud Platform (GCP)\n\n    * _Preparation for the GCP certification exam_\n    * _Practical guidance for building and deploying machine learning models on GCP_\n\n* **Book Structure**\n    * 14 chapters covering various aspects of machine learning and GCP\n    * 1 appendix with answers to review questions\n\n    * _Chapter 1: Framing ML Problems_ - setting up machine learning projects\n    * _Chapter 2: Exploring Data and Building Data Pipelines_ - data preparation and pipelines\n    * _Chapter 3: Feature Engineering_ - feature engineering techniques\n    * _Chapter 4: Choosing the Right ML Infrastructure_ - choosing between pre-trained, autoML, and custom models\n    * _Chapter 5: Architecting ML Solutions_ - designing reliable and scalable machine learning solutions\n    * _Chapter 6: Building Secure ML Pipelines_ - security best practices for machine learning pipelines\n    * _Chapter 7: Model Building_ - building and training machine learning models\n    * _Chapter 8: Model Training and Hyperparameter Tuning_ - tuning hyperparameters and tracking metrics\n    * _Chapter 9: Model Explainability on Vertex AI_ - understanding model predictions with explainability tools\n    * _Chapter 10: Scaling Models in Production_ - deploying machine learning models to production\n    * _Chapter 11: Designing ML Training Pipelines_ - designing training pipelines for scalability and reliability\n    * _Chapter 12: Model Monitoring, Tracking, and Auditing Metadata_ - monitoring and auditing model performance\n    * _Chapter 13: Maintaining ML Solutions_ - maintaining and updating machine learning models\n    * _Chapter 14: BigQuery ML_ - using BigQuery for machine learning tasks\n\n* **Target Audience**\n    * Machine learning professionals looking to prepare for the GCP certification exam\n    * Developers seeking practical guidance on building and deploying machine learning models on GCP"
  },
  "Official Google Cloud CertifiedProfessional Machine Learning EngineerStudy Guide": {
    "content": "",
    "subsections": {},
    "summary": ""
  },
  "Acknowledgments": {
    "content": "",
    "subsections": {},
    "summary": ""
  },
  "About the Author": {
    "content": "",
    "subsections": {},
    "summary": ""
  },
  "About the Technical Editors": {
    "content": "",
    "subsections": {
      "About the Technical Proofreader": {
        "content": "**Adam Vincent** is an experienced educator with a passion for spreading knowledge and helping people expand their skill sets. He is multi‐certified in Google Cloud, is a Google Cloud Authorized Trainer, and has created multiple courses about machine learning. Adam also loves playing with data and automating everything. When he is not behind a screen, he enjoys playing tabletop games with friends and family, reading sci‐fi and fantasy novels, and hiking.",
        "subsections": {},
        "summary": "* *Adam Vincent*: Experienced educator with a passion for sharing knowledge in Google Cloud.\n    * Multi-certified in Google Cloud\n    * Google Cloud Authorized Trainer\n    * Creates machine learning courses\n* Hobbies: \n    * Tabletop gaming\n    * Reading sci-fi and fantasy novels\n    * Hiking"
      },
      "Google Technical Reviewer": {
        "content": "Wiley and the authors wish to thank the Google Technical Reviewer Emma Freeman for her thorough review of the proofs for this book.",
        "subsections": {},
        "summary": "* **Acknowledgement**: Wiley thanks *Google Technical Reviewer Emma Freeman* for reviewing the book's proofs."
      }
    },
    "summary": ""
  },
  "Introduction": {
    "content": "",
    "subsections": {
      "Google Cloud Professional Machine Learning Engineer Certification": {
        "content": "A Professional Machine Learning Engineer designs, builds, and productionizes ML models to solve business challenges using Google Cloud technologies and knowledge of proven ML models and techniques. The ML engineer considers responsible AI throughout the ML development process and collaborates closely with other job roles to ensure the long‐term success of models. The ML engineer should be proficient in all aspects of model architecture, data pipeline interaction, and metrics interpretation. The ML engineer needs familiarity with foundational concepts of application development, infrastructure management, data engineering, and data governance. Through an understanding of training, retraining, deploying, scheduling, monitoring, and improving models, the ML engineer designs and creates scalable solutions for optimal performance.",
        "subsections": {
          "Why Become Professional ML Engineer (PMLE) Certified?": {
            "content": "There are several good reasons to get your PMLE certification.\n\n  * **Provides proof of professional achievement** Certifications are quickly becoming status symbols in the computer service industry. Organizations, including members of the computer service industry, are recognizing the benefits of certification.\n  * **Increases your marketability** According to Forbes (`www.forbes.com/sites/louiscolumbus/2020/02/10/15-top-paying-it-certifications-in-2020/?sh=12f63aa8358e`), jobs that require GCP certifications are the highest‐paying jobs for the second year in a row, paying an average salary of $175,761/year. So, there is a demand from many engineers to get certified. Of the many certifications that GCP offers, the AI/ML certified engineer is a new certification and is still evolving.\n  * **Provides an opportunity for advancement** IDC's research (`www.idc.com/getdoc.jsp?containerId=IDC_P40729`) indicates that while AI/ML adoption is on the rise, the cost, lack of expertise, and lack of life cycle management tools are among the top three inhibitors to realizing AI and ML at scale.\n  * This book is the first in the market to talk about Google Cloud AI/ML tools and the technology covering the latest Professional ML Engineer certification guidelines released on February 22, 2022.\n  * **Recognizes Google as a leader in open source and AI** Google is the main contributor to many of the path‐breaking open source softwares that dramatically changed the landscape of AI/ML, including TensorFlow, Kubeflow, Word2vec, BERT, and T5. Although these algorithms are in the open source domain, Google has the distinct ability of bringing these open source projects to the market through the Google Cloud Platform (GCP). In this regard, the other cloud providers are frequently seen as trailing Google's offering.\n  * **Raises customer confidence** As the IT community, users, small business owners, and the like become more familiar with the PMLE certified professional, more of them will realize that the PMLE professional is more qualified to architect secure, cost‐effective, and scalable ML solutions on the Google Cloud environment than a noncertified individual.",
            "subsections": {},
            "summary": "* **Earn recognition as a Professional ML Engineer**: gets proof of professional achievement\n  * Increases marketability with high-paying job opportunities\n  * Advances career prospects in AI/ML adoption\n* **Stay ahead in the industry**: recognizes Google's leadership in open source and AI\n* **Boost customer confidence**: demonstrates expertise in secure, cost-effective, and scalable ML solutions on GCP"
          },
          "How to Become Certified": {
            "content": "You do not have to work for a particular company. It's not a secret society. There is no prerequisite to take this exam. However, there is a recommendation to have 3+ years of industry experience, including one or more years designing and managing solutions using Google Cloud.\n\nThis exam is 2 hours and has 50–60 multiple‐choice questions.\n\nYou can register two ways for this exam:\n\n  * Take the online‐proctored exam from anywhere or sitting at home. You can review the online testing requirements at `www.webassessor.com/wa.do?page=certInfo&branding=GOOGLECLOUD&tabs=13`.\n  * Take the on‐site, proctored exam at a testing center.\n\nWe usually prefer to go with the on‐site option as we like the focus time in a proctored environment. We have taken all our certifications in a test center. You can find and locate a test center near you at `www.kryterion.com/Locate-Test-Center`.",
            "subsections": {},
            "summary": "* **Exam Details**: 2-hour, 50-60 multiple-choice questions\n* **Registration Options**:\n    * Online-proctored exam from anywhere (review requirements: https://wa.do?page=certInfo&branding=GOOGLECLOUD&tabs=13)\n    * On-site, proctored exam at a testing center (find locations: https://kryterion.com/Locate-Test-Center)"
          }
        },
        "summary": "* A Professional Machine Learning Engineer uses Google Cloud technologies to design, build, and deploy scalable ML models that solve business challenges.\n    * They collaborate with cross-functional teams and consider responsible AI throughout the development process.\n        * Proficient in model architecture, data pipeline interaction, metrics interpretation, and a range of application development, infrastructure management, and data engineering concepts."
      },
      "Who Should Buy This Book": {
        "content": "This book is intended to help students, developers, data scientists, IT professionals, and ML engineers gain expertise in the ML technology on the Google Cloud Platform and take the `Professional Machine Learning Engineer` exam. This book intends to take readers through the machine learning process starting from data and moving on through feature engineering, model training, and deployment on the Google Cloud. It also walks readers through best practices for when to pick custom models versus AutoML or pretrained models. Google Cloud AI/ML technologies are presented through real‐world scenarios to illustrate how IT professionals can design, build, and operate secure ML cloud environments to modernize and automate applications.\n\nAnybody who wants to pass the Professional ML Engineer exam may benefit from this book. If you're new to Google Cloud, this book covers the updated machine learning exam course material, including the Google Cloud Vertex AI platform, MLOps, and BigQuery ML. This is the only book on the market to cover the complete Vertex AI platform, from bringing your data to training, tuning, and deploying your models.\n\nSince it's a professional‐level study guide, this book is written with the assumption that you know the basics of the Google Cloud Platform, such as compute, storage, networking, databases, and identity and access management (IAM) or have taken the Google Cloud Associate‐level certification exam. Moreover, this book assumes you understand the basics of machine learning and data science in general. In case you do not understand a term or concept, we have included a glossary for your reference.",
        "subsections": {},
        "summary": "* _This book is a professional-level study guide to help readers gain expertise in ML on Google Cloud Platform (GCP) and pass the Professional Machine Learning Engineer exam._\n \n    * _It covers the entire GCP AI/ML suite, including Vertex AI, MLOps, and BigQuery ML, with real-world scenarios and best practices for designing secure ML cloud environments._\n\n        * _Assumes prior knowledge of Google Cloud basics and machine learning fundamentals; a glossary is provided for reference._"
      },
      "How This Book Is Organized": {
        "content": "This book consists of 14 chapters plus supplementary information: a glossary, this introduction, and the assessment test after the introduction. The chapters are organized as follows:\n\n  * **Chapter** 1**: Framing ML Problems** This chapter covers how you can translate business challenges into ML use cases.\n  * **Chapter** 2**: Exploring Data and Building Data Pipelines** This chapter covers visualization, statistical fundamentals at scale, evaluation of data quality and feasibility, establishing data constraints (e.g., TFDV), organizing and optimizing training datasets, data validation, handling missing data, handling outliers, and data leakage.\n  * **Chapter** 3**: Feature Engineering** This chapter covers topics such as encoding structured data types, feature selection, class imbalance, feature crosses, and transformations (TensorFlow Transform).\n  * **Chapter** 4**: Choosing the Right ML Infrastructure** This chapter covers topics such as evaluation of compute and accelerator options (e.g., CPU, GPU, TPU, edge devices) and choosing appropriate Google Cloud hardware components. It also covers choosing the best solution (ML vs. non‐ML, custom vs. pre‐packaged [e.g., AutoML, Vision API]) based on the business requirements. It talks about how defining the model output should be used to solve the business problem. It also covers deciding how incorrect results should be handled and identifying data sources (available vs. ideal). It talks about AI solutions such as CCAI, DocAI, and Recommendations AI.\n  * **Chapter** 5**: Architecting ML Solutions** This chapter explains how to design reliable, scalable, and highly available ML solutions. Other topics include how you can choose appropriate ML services for a use case (e.g., Cloud Build, Kubeflow), component types (e.g., data collection, data management), automation, orchestration, and serving in machine learning.\n  * **Chapter** 6**: Building Secure ML Pipelines** This chapter describes how to build secure ML systems (e.g., protecting against unintentional exploitation of data/model, hacking). It also covers the privacy implications of data usage and/or collection (e.g., handling sensitive data such as personally identifiable information [PII] and protected health information [PHI]).\n  * **Chapter** 7**: Model Building** This chapter describes the choice of framework and model parallelism. It also covers modeling techniques given interpretability requirements, transfer learning, data augmentation, semi‐supervised learning, model generalization, and strategies to handle overfitting and underfitting.\n  * **Chapter** 8**: Model Training and Hyperparameter Tuning** This chapter focuses on the ingestion of various file types into training (e.g., CSV, JSON, IMG, parquet or databases, Hadoop/Spark). It covers training a model as a job in different environments. It also talks about unit tests for model training and serving and hyperparameter tuning. Moreover, it discusses ways to track metrics during training and retraining/redeployment evaluation.\n  * **Chapter** 9**: Model Explainability on Vertex AI** This chapter covers approaches to model explainability on Vertex AI.\n  * **Chapter** 10**: Scaling Models in Production** This chapter covers scaling prediction service (e.g., Vertex AI Prediction, containerized serving), serving (online, batch, caching), Google Cloud serving options, testing for target performance, and configuring trigger and pipeline schedules.\n  * **Chapter** 11**: Designing ML Training Pipelines** This chapter covers identification of components, parameters, triggers, and compute needs (e.g., Cloud Build, Cloud Run). It also talks about orchestration framework (e.g., Kubeflow Pipelines/Vertex AI Pipelines, Cloud Composer/Apache Airflow), hybrid or multicloud strategies, and system design with TFX components/Kubeflow DSL.\n  * **Chapter** 12**: Model Monitoring, Tracking, and Auditing Metadata** This chapter covers the performance and business quality of ML model predictions, logging strategies, organizing and tracking experiments, and pipeline runs. It also talks about dataset versioning and model/dataset lineage.\n  * **Chapter** 13**: Maintaining ML Solutions** This chapter covers establishing continuous evaluation metrics (e.g., evaluation of drift or bias), understanding the Google Cloud permission model, and identification of appropriate retraining policies. It also covers common training and serving errors (TensorFlow), ML model failure, and resulting biases. Finally, it talks about how you can tune the performance of ML solutions for training and serving in production.\n  * **Chapter** 14**: BigQuery ML** This chapter covers BigQueryML algorithms, when to use BigQueryML versus Vertex AI, and the interoperability with Vertex AI.",
        "subsections": {
          "Chapter Features": {
            "content": "Each chapter begins with a list of the objectives that are covered in the chapter. The book doesn't cover the objectives in order. Thus, you shouldn't be alarmed at some of the odd ordering of the objectives within the book.\n\nAt the end of each chapter, you'll find several elements you can use to prepare for the exam.\n\n  * **Exam Essentials** This section summarizes important information that was covered in the chapter. You should be able to perform each of the tasks or convey the information requested.\n  * **Review Questions** Each chapter concludes with 8+ review questions. You should answer these questions and check your answers against the ones provided after the questions. If you can't answer at least 80 percent of these questions correctly, go back and review the chapter, or at least those sections that seem to be giving you difficulty.\n\n* * *\n\nThe review questions, assessment test, and other testing elements included in this book are _not_ derived from the PMLE exam questions, so don't memorize the answers to these questions and assume that doing so will enable you to pass the exam. You should learn the underlying topic, as described in the text of the book. This will let you answer the questions provided with this book _and_ pass the exam. Learning the underlying topic is also the approach that will serve you best in the workplace.\n\n* * *\n\nTo get the most out of this book, you should read each chapter from start to finish and then check your memory and understanding with the chapter‐end elements. Even if you're already familiar with a topic, you should skim the chapter; machine learning is complex enough that there are often multiple ways to accomplish a task, so you may learn something even if you're already competent in an area.\n\n* * *\n\nLike all exams, the Google Cloud certification from Google is updated periodically and may eventually be retired or replaced. At some point after Google is no longer offering this exam, the old editions of our books and online tools will be retired. If you have purchased this book after the exam was retired, or are attempting to register in the Sybex online learning environment after the exam was retired, please know that we make no guarantees that this exam’s online Sybex tools will be available once the exam is no longer available.\n\n* * *",
            "subsections": {},
            "summary": "* **Exam Structure**\n  * Chapter objectives not necessarily in order\n  * *_Exam Essentials_*: summary of chapter content for review\n  * *_Review Questions_*: 8+ questions to check understanding and identify areas for further study\n\n*   **Important Notes**\n    *   Review questions are not derived from PMLE exam questions; memorization is not a substitute for learning the underlying topic.\n    *   To pass the exam, learn the underlying topic described in the book.\n\n*   **Getting the Most Out of This Book**\n  * Read each chapter from start to finish and review with *_Exam Essentials_* and *_Review Questions_*\n  * Skim chapters even if familiar with a topic, as machine learning has multiple approaches."
          }
        },
        "summary": "* _Machine Learning (ML) Handbook_: A comprehensive guide to ML, consisting of 14 chapters covering topics from framing ML problems to maintaining ML solutions.\n* **Key Topics**:\n  * Building secure ML pipelines\n  * Architecting reliable ML solutions\n  * Choosing the right ML infrastructure and tools\n  * Scaling models in production\n  * Model monitoring, tracking, and auditing metadata\n* **Target Audience**: Developers and professionals looking to learn about machine learning best practices and techniques for building scalable and secure ML solutions."
      },
      "Bonus Digital Contents": {
        "content": "This book is accompanied by an online learning environment that provides several additional elements. The following items are available among these companion files:\n\n  * **Practice tests** All of the questions in this book appear in our proprietary digital test engine—including the 30‐question assessment test at the end of this introduction and the 100+ questions that make up the review question sections at the end of each chapter. In addition, there are two 50‐question bonus exams.\n  * **Electronic “flash cards”** The digital companion files include 50+ questions in flash card format (a question followed by a single correct answer). You can use these to review your knowledge of the exam objectives.\n  * **Glossary** The key terms from this book, and their definitions, are available as a fully searchable PDF.",
        "subsections": {
          "Interactive Online Learning Environment and Test Bank": {
            "content": "You can access all these resources at `www.wiley.com/go/sybextestprep`.\n\n* * *",
            "subsections": {},
            "summary": "You can access all these resources at `www.wiley.com/go/sybextestprep`.\n\n* * *"
          }
        },
        "summary": "* _Companion files include:_\n  * **Practice tests**: digital test engine with 30-question assessment test and 100+ review questions\n  * Electronic \"flash cards\": 50+ question format for reviewing exam objectives\n  * _Glossary_: fully searchable PDF of key terms and definitions_"
      },
      "Conventions Used in This Book": {
        "content": "This book uses certain typographic styles in order to help you quickly identify important information and to avoid confusion over the meaning of words such as on‐screen prompts. In particular, look for the following styles:\n\n  * _Italicized text_ indicates key terms that are described at length for the first time in a chapter. These words probably appear in the searchable online glossary. (Italics are also used for emphasis.)\n  * `A monospaced font` indicates the contents of configuration files, messages displayed as a text‐mode Google Cloud shell prompt, filenames, text‐mode command names, and Internet URLs.\n\nIn addition to these text conventions, which can apply to individual words or entire paragraphs, a few conventions highlight segments of text:\n\n* * *\n\nA note indicates information that's useful or interesting but that's somewhat peripheral to the main text. A note might be relevant to a small number of networks, for instance, or it may refer to an outdated feature.\n\n* * *\n\n* * *\n\nA tip provides information that can save you time or frustration and that may not be entirely obvious. A tip might describe how to get around a limitation or how to use a feature to perform an unusual task.\n\n* * *",
        "subsections": {},
        "summary": "* _Typographic Styles:_\n  * *_Italicized text_* : Key terms described for the first time\n  * `Monospaced font` : Configuration files, command names, and Internet URLs\n\n* **Note**: Peripheral information useful but not crucial to main text\n* **Tip**: Time-saving or frustration-reducing information not immediately obvious"
      },
      "Google Cloud Professional ML Engineer Objective Map": {
        "content": "Here is where to find the objectives covered in this book.\n\nOBJECTIVE | CHAPTER(S)\n---|---\n**Section 1: Architecting low‐code ML solutions** |\n1.1 Developing ML models by using BigQuery ML. Considerations include: | 14\n\n  * Building the appropriate BigQuery ML model (e.g., linear and binary classification, regression, time‐series, matrix factorization, boosted trees, autoencoders) based on the business problem\n\n| 14\n\n  * Feature engineering or selection by using BigQuery ML\n\n| 14\n\n  * Generating predictions by using BigQuery ML\n\n| 14\n1.2 Building AI solutions by using ML APIs. Considerations include: | 4\n\n  * Building applications by using ML APIs (e.g., Cloud Vision API, Natural Language API, Cloud Speech API, Translation)\n\n| 4\n\n  * Building applications by using industry‐specific APIs (e.g., Document AI API, Retail API)\n\n| 4\n1.3 Training models by using AutoML. Considerations include: | 4\n\n  * Preparing data for AutoML (e.g., feature selection, data labeling, Tabular Workflows on AutoML)\n\n| 4\n\n  * Using available data (e.g., tabular, text, speech, images, videos) to train custom models\n\n| 4\n\n  * Using AutoML for tabular data\n\n| 4\n\n  * Creating forecasting models using AutoML\n\n| 4\n\n  * Configuring and debugging trained models\n\n| 4\n**Section 2: Collaborating within and across teams to manage data and models** |\n2.1 Exploring and preprocessing organization‐wide data (e.g., Cloud Storage, BigQuery, Cloud Spanner, Cloud SQL, Apache Spark, Apache Hadoop). Considerations include: | 3, 5, 6, 8, 13\n\n  * Organizing different types of data (e.g., tabular, text, speech, images, videos) for efficient training\n\n| 8\n\n  * Managing datasets in Vertex AI\n\n| 8\n\n  * Data preprocessing (e.g., Dataflow, TensorFlow Extended [TFX], BigQuery)\n\n| 3, 5\n\n  * Creating and consolidating features in Vertex AI Feature Store\n\n| 13\n\n  * Privacy implications of data usage and/or collection (e.g., handling sensitive data such as personally identifiable information [PII] and protected health information [PHI])\n\n| 6\n2.2 Model prototyping using Jupyter notebooks. Considerations include: | 6, 8\n\n  * Choosing the appropriate Jupyter backend on Google Cloud (e.g., Vertex AI Workbench notebooks, notebooks on Dataproc)\n\n| 8\n\n  * Applying security best practices in Vertex AI Workbench\n\n| 6\n\n  * Using Spark kernels\n\n| 8\n\n  * Integration with code source repositories\n\n| 8\n\n  * Developing models in Vertex AI Workbench by using common frameworks (e.g., TensorFlow, PyTorch, sklearn, Spark, JAX)\n\n| 8\n2.3 Tracking and running ML experiments. Considerations include: | 5, 12\n\n  * Choosing the appropriate Google Cloud environment for development and experimentation (e.g., Vertex AI Experiments, Kubeflow Pipelines, Vertex AI TensorBoard with TensorFlow and PyTorch) given the framework\n\n| 5, 12\n**Section 3: Scaling prototypes into ML models** |\n3.1 Building models. Considerations include: | 7\n\n  * Choosing ML framework and model architecture\n\n| 7\n\n  * Modeling techniques given interpretability requirements\n\n| 7\n3.2 Training models. Considerations include: | 7, 8\n\n  * Organizing training data (e.g., tabular, text, speech, images, videos) on Google Cloud (e.g., Cloud Storage, BigQuery)\n\n| 8\n\n  * Ingestion of various file types (e.g., CSV, JSON, images, Hadoop, databases) into training\n\n| 8\n\n  * Training using different SDKs (e.g., Vertex AI custom training, Kubeflow on Google Kubernetes Engine, AutoML, tabular workflows)\n\n| 8\n\n  * Using distributed training to organize reliable pipelines\n\n| 7, 8\n\n  * Hyperparameter tuning\n\n| 8\n\n  * Troubleshooting ML model training failures\n\n| 8\n3.3 Choosing appropriate hardware for training. Considerations include: | 4, 8\n\n  * Evaluation of compute and accelerator options (e.g., CPU, GPU, TPU, edge devices)\n\n| 4\n\n  * Distributed training with TPUs and GPUs (e.g., Reduction Server on Vertex AI, Horovod)\n\n| 8\n**Section 4: Serving and scaling models** |\n---|---\n4.1 Serving models. Considerations include: | 5, 10\n\n  * Batch and online inference (e.g., Vertex AI, Dataflow, BigQuery ML, Dataproc)\n\n| 5, 10\n\n  * Using different frameworks (e.g., PyTorch, XGBoost) to serve models\n\n| 10\n\n  * Organizing a model registry\n\n| 10\n\n  * A/B testing different versions of a model\n\n| 10\n4.2 Scaling online model serving. Considerations include: | 4, 5, 6, 10, 13\n\n  * Vertex AI Feature Store\n\n| 13\n\n  * Vertex AI public and private endpoints\n\n| 6\n\n  * Choosing appropriate hardware (e.g., CPU, GPU, TPU, edge)\n\n| 4\n\n  * Scaling the serving backend based on the throughput (e.g., Vertex AI Prediction, containerized serving)\n\n| 10\n\n  * Tuning ML models for training and serving in production (e.g., simplification techniques, optimizing the ML solution for increased performance, latency, memory, throughput)\n\n| 5, 10\n**Section 5: Automating and orchestrating ML pipelines** |\n5.1 Developing end‐to‐end ML pipelines. Considerations include: | 2, 3, 10, 11\n\n  * Data and model validation\n\n| 2, 3\n\n  * Ensuring consistent data pre‐processing between training and serving\n\n| 3\n\n  * Hosting third‐party pipelines on Google Cloud (e.g., MLFlow)\n\n| 10\n\n  * Identifying components, parameters, triggers, and compute needs (e.g., Cloud Build, Cloud Run)\n\n| 11\n\n  * Orchestration framework (e.g., Kubeflow Pipelines, Vertex AI Managed Pipelines, Cloud Composer)\n\n| 11\n\n  * Hybrid or multicloud strategies\n\n| 11\n\n  * System design with TFX components or Kubeflow DSL (e.g., Dataflow)\n\n| 11\n5.2 Automating model retraining. Considerations include: | 13\n\n  * Determining an appropriate retraining policy\n\n| 13\n\n  * Continuous integration and continuous delivery (CI/CD) model deployment (e.g., Cloud Build, Jenkins)\n\n| 13\n5.3 Tracking and auditing metadata. Considerations include: | 12\n\n  * Tracking and comparing model artifacts and versions (e.g., Vertex AI Experiments, Vertex ML Metadata)\n\n| 12\n\n  * Hooking into model and dataset versioning\n\n| 12\n\n  * Model and data lineage\n\n| 12\n**Section 6: Monitoring ML solutions** |\n6.1 Identifying risks to ML solutions. Considerations include: | 6, 9\n\n  * Building secure ML systems (e.g., protecting against unintentional exploitation of data or models, hacking)\n\n| 6, 9\n\n  * Aligning with Google's Responsible AI practices (e.g., biases)\n\n| 9\n\n  * Assessing ML solution readiness (e.g., data bias, fairness)\n\n| 9\n\n  * Model explainability on Vertex AI (e.g., Vertex AI Prediction)\n\n| 9\n6.2 Monitoring, testing, and troubleshooting ML solutions. Considerations include: | 12, 13\n\n  * Establishing continuous evaluation metrics (e.g., Vertex AI Model Monitoring, Explainable AI)\n\n| 12,13\n\n  * Monitoring for training‐serving skew\n\n| 12\n\n  * Monitoring for feature attribution drift\n\n| 12\n\n  * Monitoring model performance against baselines, simpler models, and across the time dimension\n\n| 12\n\n  * Common training and serving errors\n\n| 13\n\n* * *\n\nExam domains and objectives are subject to change at any time without prior notice and at Google's sole discretion. Please visit its website (`https://cloud.google.com/certification/machine-learning-engineer`) for the most current information.\n\n* * *",
        "subsections": {},
        "summary": "**Machine Learning Engineer Objectives**\n=====================================\n\n### Section 1: Architecting Low-Code ML Solutions\n---------------------------------------------\n\n*   **Model Development**: Building, training, and deploying ML models using BigQuery ML and industry-specific APIs.\n    *   _Choose the right model architecture for business problems._\n    *   _Implement feature engineering and selection techniques._\n\n*   **AutoML and Model Training**: Using AutoML to train custom models, managing data preprocessing, and troubleshooting training failures.\n    *   _Prepare data for AutoML by selecting features and labeling datasets._\n    *   _Configure and debug trained models._\n\n### Section 2: Collaborating Within and Across Teams\n---------------------------------------------\n\n*   **Data Management and Preprocessing**: Exploring, organizing, and prepping organization-wide data using Cloud Storage, BigQuery, and other tools.\n    *   _Organize different types of data for efficient training._\n    *   _Create and consolidate features in Vertex AI Feature Store._\n\n*   **Model Prototyping and Testing**: Using Jupyter notebooks to prototype models, applying security best practices, and integrating with code repositories.\n    *   _Choose the right Jupyter backend on Google Cloud for model prototyping._\n    *   _Integrate with code source repositories._\n\n### Section 3: Scaling Prototypes into ML Models\n-------------------------------------------\n\n*   **Model Building and Training**: Choosing the right framework and architecture, modeling techniques, training data organization, and hyperparameter tuning.\n    *   _Choose an ML framework and model architecture for interpretability requirements._\n    *   _Train models using various SDKs._\n\n### Section 4: Serving and Scaling Models\n------------------------------------\n\n*   **Model Serving**: Batch and online inference, serving models with different frameworks, organizing a model registry, and A/B testing.\n    *   _Use Vertex AI Feature Store to serve models._\n\n*   **Scaling Online Model Serving**: Choosing the right hardware, scaling serving backend based on throughput, and tuning ML models for production.\n\n### Section 5: Automating and Orchestrating ML Pipelines\n---------------------------------------------\n\n*   **Developing End-to-End ML Pipelines**: Data and model validation, consistent data preprocessing between training and serving, hosting third-party pipelines, and identifying components and parameters.\n    *   _Ensure consistent data preprocessing between training and serving._\n\n*   **Automating Model Retraining**: Determining retraining policies, continuous integration and delivery, and tracking metadata.\n\n### Section 6: Monitoring ML Solutions\n----------------------------------\n\n*   **Identifying Risks to ML Solutions**: Building secure ML systems, aligning with Google's Responsible AI practices, assessing model readiness, and model explainability.\n    *   _Align with Google's Responsible AI practices._\n\n*   **Monitoring, Testing, and Troubleshooting**: Establishing continuous evaluation metrics, monitoring for training-serving skew, feature attribution drift, and common errors."
      },
      "How to Contact the Publisher": {
        "content": "If you believe you have found a mistake in this book, please bring it to our attention. At John Wiley & Sons, we understand how important it is to provide our customers with accurate content, but even with our best efforts an error may occur.\n\nIn order to submit your possible errata, please email it to our Customer Service Team at wileysupport@wiley.com with the subject line “Possible Book Errata Submission.”",
        "subsections": {},
        "summary": "* To report a mistake in this book, send an email to wileysupport@wiley.com with \"Possible Book Errata Submission\" as the subject. \n    * Errors may occur despite our best efforts to ensure accuracy.\n    * Email address and subject line are required for submission."
      },
      "Assessment Test": {
        "content": "1. How would you split the data to predict a user lifetime value (LTV) over the next 30 days in an online recommendation system to avoid data and label leakage? (Choose three.)\n     1. Perform data collection for 30 days.\n     2. Create a training set for data from day 1 to day 29.\n     3. Create a validation set for data for day 30.\n     4. Create random data split into training, validation, and test sets.\n  2. You have a highly imbalanced dataset and you want to focus on the positive class in the classification problem. Which metrics would you choose?\n     1. Area under the precision‐recall curve (AUC PR)\n     2. Area under the curve ROC (AUC ROC)\n     3. Recall\n     4. Precision\n  3. A feature cross is created by ________________ two or more features.\n     1. Swapping\n     2. Multiplying\n     3. Adding\n     4. Dividing\n  4. You can use Cloud Pub/Sub to stream data in GCP and use Cloud Dataflow to transform the data.\n     1. True\n     2. False\n  5. You have training data, and you are writing the model training code. You have a team of data engineers who prefer to code in SQL. Which service would you recommend?\n     1. BigQuery ML\n     2. Vertex AI custom training\n     3. Vertex AI AutoML\n     4. Vertex AI pretrained APIs\n  6. What are the benefits of using a Vertex AI managed dataset? (Choose three.)\n     1. Integrated data labeling for unlabeled, unstructured data such as video, text, and images using Vertex data labeling.\n     2. Track lineage to models for governance and iterative development.\n     3. Automatically splitting data into training, test, and validation sets.\n     4. Manual splitting of data into training, test, and validation sets.\n  7. Masking, encrypting, and bucketing are de‐identification techniques to obscure PII data using the Cloud Data Loss Prevention API.\n     1. True\n     2. False\n  8. Which strategy would you choose to handle the sensitive data that exists within images, videos, audio, and unstructured free‐form data?\n     1. Use NLP API, Cloud Speech API, Vision AI, and Video Intelligence AI to identify sensitive data such as email and location out of box, and then redact or remove it.\n     2. Use Cloud DLP to address this type of data.\n     3. Use Healthcare API to hide sensitive data.\n     4. Create a view that doesn't provide access to the columns in question. The data engineers cannot view the data, but at the same time the data is live and doesn't require human intervention to de‐identify it for continuous training.\n  9. You would use __________________ when you are trying to reduce features while trying to solve an overfitting problem with large models.\n     1. L1 regularization\n     2. L2 regularization\n     3. Both A and B\n     4. Vanishing gradient\n  10. If the weights in a network are very large, then the gradients for the lower layers involve products of many large terms leading to exploding gradients that get too large to converge. What are some of the ways this can be avoided? (Choose two.)\n     1. Batch normalization\n     2. Lower learning rate\n     3. The ReLU activation function\n     4. Sigmoid activation function\n  11. You have a Spark and Hadoop environment on‐premises, and you are planning to move your data to Google Cloud. Your ingestion pipeline is both real time and batch. Your ML customer engineer recommended a scalable way to move your data using Cloud Dataproc to BigQuery. Which of the following Dataproc connectors would you _not_ recommend?\n     1. Pub/Sub Lite Spark connector\n     2. BigQuery Spark connector\n     3. BigQuery connector\n     4. Cloud Storage connector\n  12. You have moved your Spark and Hadoop environment and your data is in Google Cloud Storage. Your ingestion pipeline is both real time and batch. Your ML customer engineer recommended a scalable way to run Apache Hadoop or Apache Spark jobs directly on data in Google Cloud Storage. Which of the following Dataproc connector would you recommend?\n     1. Pub/Sub Lite Spark connector\n     2. BigQuery Spark connector\n     3. BigQuery connector\n     4. Cloud Storage connector\n  13. Which of the following is _not_ a technique to speed up hyperparameter optimization?\n     1. Parallelize the problem across multiple machines by using distributed training with hyperparameter optimization.\n     2. Avoid redundant computations by pre‐computing or cache the results of computations that can be reused for subsequent model fits.\n     3. Use grid search rather than random search.\n     4. If you have a large dataset, use a simple validation set instead of cross‐validation.\n  14. Vertex AI Vizier is an independent service for optimizing complex models with many parameters. It can be used only for non‐ML use cases.\n     1. True\n     2. False\n  15. Which of the following is _not_ a tool to track metrics when training a neural network?\n     1. Vertex AI interactive shell\n     2. What‐If Tool\n     3. Vertex AI TensorBoard Profiler\n     4. Vertex AI hyperparameter tuning\n  16. You are a data scientist working to select features with structured datasets. Which of the following techniques will help?\n     1. Sampled Shapley\n     2. Integrated gradient\n     3. XRAI (eXplanation with Ranked Area Integrals)\n     4. Gradient descent\n  17. Variable selection and avoiding target leakage are the benefits of feature importance.\n     1. True\n     2. False\n  18. A TensorFlow SavedModel is what you get when you call __________________. Saved models are stored as a directory on disk. The file within that directory, `saved_model.pb`, is a protocol buffer describing the functional tf.Graph.\n     1. `tf.saved_model.save()`\n     2. `tf.Variables`\n     3. `tf.predict()`\n     4. `Tf.keras.models.load_model`\n  19. What steps would you recommend a data engineer trying to deploy a TensorFlow model trained locally to set up real‐time prediction using Vertex AI? (Choose three.)\n     1. Import the model to Model Registry.\n     2. Deploy the model.\n     3. Create an endpoint for deployed model.\n     4. Create a model in Model Registry.\n  20. You are an MLOps engineer and you deployed a Kubeflow pipeline on Vertex AI pipelines. Which Google Cloud feature will help you track lineage with your Vertex AI pipelines?\n     1. Vertex AI Model Registry\n     2. Vertex AI Artifact Registry\n     3. Vertex AI ML metadata\n     4. Vertex AI Model Monitoring\n  21. What is _not_ a recommended way to invoke a Kubeflow pipeline?\n     1. Using Cloud Scheduler\n     2. Responding to an event, using `Pub/Sub` and `Cloud Functions`\n     3. Cloud Composer and Cloud Build\n     4. Directly using BigQuery\n  22. You are a software engineer working at a start‐up that works on organizing personal photos and pet photos. You have been asked to use machine learning to identify and tag which photos have pets and also identify public landmarks in the photos. These features are not available today and you have a week to create a solution for this. What is the best approach?\n     1. Find the best cat/dog dataset and train a custom model on Vertex AI using the latest algorithm available. Do the same for identifying landmarks.\n     2. Find a pretrained cat/dog dataset (available) and train a custom model on Vertex AI using the latest deep neural network TensorFlow algorithm.\n     3. Use the cat/dog dataset to train a Vertex AI AutoML image classification model on Vertex AI. Do the same for identifying landmarks.\n     4. Vision AI already identifies pets and landmarks. Use that to see if it meets the requirements. If not, use the Vertex AI AutoML model.\n  23. You are building a product that will accurately throw a ball into the basketball net. This should work no matter where it is placed on the court. You have created a very large TensorFlow model (size more than 90 GB) based on thousands of hours of video. The model uses custom operations, and it has optimized the training loop to not have any I/O operations. What are your hardware options to train this model?\n     1. Use a TPU slice because the model is very large and has been optimized to not have any I/O operations.\n     2. Use a TPU pod because the model size is larger than 50 GB.\n     3. Use a GPU‐only instance.\n     4. Use a CPU‐only instance to build your model.\n  24. You work in the fishing industry and have been asked to use machine learning to predict the age of lobster based on size and color. You have thousands of images of lobster from Arctic fishing boats, from which you have extracted the size of the lobster that is passed to the model, and you have built a regression model for predicting age. Your model has performed very well in your test and validation data. Users want to use this model from their boats. What are your next steps? (Choose three.)\n     1. Deploy the model on Vertex AI, expose a REST endpoint.\n     2. Enable monitoring on the endpoint and see if there is any training‐serving skew and drift detection. The original dataset was only from Arctic boats.\n     3. Also port this model to BigQuery for batch prediction.\n     4. Enable Vertex AI logging and analyze the data in BigQuery.\n  25. You have built a custom model and deployed it in Vertex AI. You are not sure if the predictions are being served fast enough (low latency). You want to measure this by enabling Vertex AI logging. Which type of logging will give you information like time stamp and latency for each request?\n     1. Container logging\n     2. Time stamp logging\n     3. Access logging\n     4. Request‐response logging\n  26. You are part of a growing ML team in your company that has started to use machine learning to improve your business. You were initially building models using Vertex AI AutoML and providing the trained models to the deployment teams. How should you scale this?\n     1. Create a Python script to train multiple models using Vertex AI.\n     2. You are now in level 0, and your organization needs level 1 MLOps maturity. Automate the training using Vertex AI Pipelines.\n     3. You are in the growth phase of the organization, so it is important to grow the team to leverage more ML engineers.\n     4. Move to Vertex AI custom models to match the MLOps maturity level.\n  27. What is _not_ a reason to use Vertex AI Feature Store?\n     1. It is a managed service.\n     2. It extracts features from images and videos and stores them.\n     3. All data is a time‐series, so you can track when the features values change over time.\n     4. The features created by the feature engineering teams are available during training time but not during serving time. So this helps in bridging that.\n  28. You are a data analyst in an organization that has thousands of insurance agents, and you have been asked to predict the revenue by each agent for the next quarter. You have the historical data for the last 10 years. You are familiar with all AI services on Google Cloud. What is the most efficient way to do this?\n     1. Build a Vertex AI AutoML forecast, deploy the model, and make predictions using REST API.\n     2. Build a Vertex AI AutoML forecast model, import the model into BigQuery, and make predictions using BigQuery ML.\n     3. Build a BigQuery ML ARIMA+ model using data in BigQuery, and make predictions in BigQuery.\n     4. Build a BigQuery ML forecast model, export the model to Vertex AI, and run a batch prediction in Vertex AI.\n  29. You are an expert in Vertex AI Pipelines, Vertex AI training, and Vertex AI deployment and monitoring. A data analyst team has built a highly accurate model, and this has been brought to you. Your manager wants you to make predictions using the model and use those predictions. What do you do?\n     1. Retrain the model on Vertex AI with the same data and deploy the model on Vertex AI as part of your CD.\n     2. Run predictions on BigQuery ML and export the predictions into GCS and then load into your pipeline.\n     3. Export the model from BigQuery into the Vertex AI model repository and run predictions in Vertex AI.\n     4. Download the BigQuery model, and package into a Vertex AI custom container and deploy it in Vertex AI.\n  30. Which of the following statements about Vertex AI and BigQuery ML is incorrect?\n     1. BigQueryML supports both unsupervised and supervised models.\n     2. BigQuery ML is very portable. Vertex AI supports all models trained on BigQuery ML.\n     3. Vertex AI model monitoring and logs data is stored in BigQuery tables.\n     4. BigQuery ML also has algorithms to predict recommendations for users.",
        "subsections": {},
        "summary": "* **Splitting Data for LTV Prediction**: To avoid data and label leakage, perform data collection for 30 days, create a training set for day 1-29, and create a validation set for day 30.\n    * Create a separate dataset for the last day to validate model performance\n    * Avoid mixing new and old data in the same model\n* **Metrics for Imbalanced Classification**: Choose AUC PR when focusing on the positive class.\n    * Measures both precision and recall\n    * Better than AUC ROC or precision/recall alone\n* **Feature Cross**: Swapping two or more features to create a feature cross.\n    * Helps with feature engineering and reducing dimensionality\n* **Cloud Services for Data Ingestion**: You can use Cloud Pub/Sub to stream data and Cloud Dataflow to transform the data.\n    * Streamline data processing and integration\n* **Choosing Vertex AI Service**: Recommend BigQuery ML for SQL-based coding preferences.\n    * Easy integration with SQL code\n* **Benefits of Managed Dataset**: Choose integrated data labeling, track lineage, and split data into training, test, and validation sets.\n    * Simplifies model development and deployment\n* **De-Identification Techniques**: Masking, encrypting, and bucketing are supported by Cloud Data Loss Prevention API.\n    * Protect sensitive PII data from unauthorized access\n* **Handling Sensitive Data in Images/Videos/Audio**: Use NLP API, Cloud Speech API, Vision AI, or Video Intelligence AI for identifying sensitive data and redacting it.\n    * Advanced image/video/audio analysis capabilities\n* **Reducing Features with Regularization**: L1 regularization is used to reduce features while avoiding exploding gradients.\n    * Reduces model complexity while maintaining performance\n* **Avoiding Exploding Gradients**: Use batch normalization, lower learning rate, or ReLU activation function to avoid exploding gradients.\n    * Stabilize training process and prevent gradient explosion\n* **Dataproc Connectors for BigQuery**: Pub/Sub Lite Spark connector is not recommended for real-time data ingestion.\n    * Choose the BigQuery Spark connector for efficient data transfer\n* **Dataproc Connector for Real-Time Ingestion**: Cloud Storage connector is suitable for both batch and real-time data ingestion.\n    * Handles both types of data efficiently\n* **Techniques to Speed Up Hyperparameter Optimization**: Use parallel computing, avoid redundant computations, and grid search rather than random search.\n    * Accelerate hyperparameter optimization process\n* **Vertex AI Vizier Limitations**: Vertex AI Vizier is not limited to non-ML use cases.\n    * Can be used for various machine learning tasks\n* **Logging Mechanisms in Vertex AI**: Request-response logging provides timestamps and latency information for each request.\n    * Essential for monitoring model performance and latency\n* **Scaling MLOps using Vertex AI Pipelines**: Automate training using Vertex AI Pipelines to achieve level 1 maturity.\n    * Streamline model development and deployment process"
      },
      "Answers to Assessment Test": {
        "content": "1. A, B, C. In case of time‐series data, the best way to perform a split is to do a time‐based split rather than a random split to avoid the data or label leakage. For more information, see Chapter 2.\n  2. A. In the case of an imbalanced class, precision‐recall curves (PR curves) are recommended for highly skewed domains. For more information, see Chapter 3.\n  3. B. A feature cross, or synthetic feature, is created by multiplying (crossing) two or more features. It can be multiplying the same feature by itself [A * A] or it can be multiplying values of multiple features such as [A * B * C]. In machine learning, feature crosses are usually performed on one‐hot encoded features. For example, binned_latitude × binned_longitude. For more information, see Chapter 3.\n  4. A, True. Cloud Pub/Sub creates a pipeline for streaming the data and Cloud Dataflow is used for data transformation. For more information, see Chapter 5.\n  5. A. If you want to perform ML using SQL, BigQuery ML is the right approach. For more information, see Chapter 5.\n  6. A, B, C. As stated in options A, B, and C, the advantages of using a managed dataset are to have integrated data labeling, data lineage, and automatic labeling features. For more information, see Chapter 5.\n  7. A. Cloud DLP uses all the mentioned techniques to obscure the PII data. For more information, see Chapter 6.\n  8. A. Cloud DLP only applies to data with a defined pattern for masking. If you have image data and a pattern of masking is not defined (for example, you want to redact faces from images), you would use Vision AI to identify the image and then redact the bounding box of the image using Python code. For more information, see Chapter 6.\n  9. A. You will use L1 when you are trying to reduce features and L2 when you are looking for a stable model. Vanishing gradients for the lower layers (closer to the input) can become very small. When the gradients vanish toward 0 for the lower layers, these layers train very slowly or they do not train at all. For more information, see Chapter 7.\n  10. A, B. Batch normalization and lower learning rate can help prevent exploding gradients. The ReLU activation function can help prevent vanishing gradients. For more information, see Chapter 7.\n  11. D. You will not use the Cloud Storage connector as the data is on premises. You would need a connector to move data directly to BigQuery. For more information, see Chapter 8.\n  12. D. The premise of the question is that you've moved the data to Cloud Storage for use. The Cloud Storage connector will allow you to use that data in your Hadoop/Spark jobs without it having to be moved onto the machines in the cluster. For more information, see Chapter 8.\n  13. C. You can improve performance by using a random search algorithm since it uses fewer trails. Options A, B, and D are all correct ways to improve optimization. For more information, see Chapter 8.\n  14. B. Vertex AI Vizier is an independent service for optimizing complex models with many parameters. It can be used for both ML and non‐ML use cases. For more information, see Chapter 8.\n  15. D. Vertex AI hyperparameter tuning is not a tool to track metrics when training a neural network; rather, it is used for tuning hyperparameters. For more information, see Chapter 8.\n  16. A. Sampled Shapley is the only method with Explainable AI, which can help explain tabular or structured datasets. For more information, see Chapter 9.\n  17. A. Feature importance is a technique that explains the features that make up the training data using a score (importance). It indicates how useful or valuable a feature is relative to other features. For more information, see Chapter 9.\n  18. A. After TensorFlow model training, you get a SavedModel. A SavedModel contains a complete TensorFlow program, including trained parameters (i.e., `tf.Variable``s`) and computation. For more information, see Chapter 10.\n  19. A, B, C. You need to import your models to the Model Registry in Vertex AI and then deploy the model before creating an endpoint. For more information, see Chapter 10.\n  20. C. Vertex ML Metadata lets you record the metadata and artifacts produced by your ML system and query that metadata to help analyze, debug, and audit the performance of your ML system or the artifacts that it produces. For more information, see Chapter 10.\n  21. D. You cannot invoke a Kubeflow pipeline using BigQuery as they are used for ETL workloads and not for MLOps. For more information, see Chapter 11.\n  22. D. The easiest approach is to use Vision AI as it is pretrained and already available. Options A, B, and C are all valid but they are unnecessarily complex given that Vision AI already achieves that. The key point to note is that you only have a week to do this task, so choose the fastest option. For more information, see Chapter 4.\n  23. C. TPU cannot be used for this case because it has custom TensorFlow operations. So options A and B are not valid. Option C is the best option because it is a large model. Using CPU only is going to be very slow. For more information, see Chapter 4.\n  24. A, B, D. There is no need to port into BigQuery for batch processing. Based on the question, batch is not a requirement; only online prediction is a requirement. The other options of deploying the model on Vertex AI, creating an endpoint and monitoring and logging, are valid. For more information, see Chapter 12.\n  25. C. Container logging gives you stderr and stdout from the container. Request‐response logs a sample of online predictions. There is no such thing as time stamp logging. Access logging is the correct answer. For more information, see Chapter 12.\n  26. B. Option B is the correct answer because it recognizes the right level of MLOps maturity and recommends automating the training with pipeline. Option A is wrong because methods like “scripting” methods are considered “ad hoc,” not mature enough for this level. Option C is wrong because it does not address the technical nature of the problem. Option D is wrong because moving from AutoML to custom models does not really help in any way here. For more information, see Chapter 13.\n  27. B. A Vertex AI Feature Store does not extract features from images. It deals only with structured data. For more information, see Chapter 13.\n  28. C. While all answers are possible, the most efficient is to build a model in BigQueryML and predict in BigQuery. For more information, see Chapter 14.\n  29. C. Option A is most redundant so not recommended. Option B is a roundabout way of doing this. Option C is the most efficient because it uses an important portability feature between BigQuery and the Vertex AI model registry. Option D is wrong because you don't have to package the model; BigQuery models don't need to be “packaged” into a container. For more information, see Chapter 14.\n  30. B. Option B is incorrect because you cannot port a model with BigQuery ML TRANSFORM into Vertex AI. All other choices are true. For more information, see Chapter 14.",
        "subsections": {},
        "summary": "* **Machine Learning Fundamentals**\n  * Use time-based splits for time-series data to avoid label leakage\n  * Precision-recall curves (PR curves) recommended for imbalanced classes\n\n* **Feature Engineering and Data Preparation**\n  * Feature cross: multiply two or more features to create a new feature\n  * BigQuery ML is the right approach for ML using SQL\n\n* **Cloud Services and Integration**\n  * Cloud DLP obscures PII data, while Vision AI redacts images with defined patterns\n  * Use Vertex ML Metadata to record metadata and artifacts produced by your ML system\n\n* **Model Training and Optimization**\n  * L1 is used to reduce features, while L2 is used for stable models\n  * Batch normalization and lower learning rate can help prevent exploding gradients\n  * Random search algorithm improves performance by using fewer trails\n\n* **Hyperparameter Tuning and Model Serving**\n  * Vertex AI Vizier optimizes complex models with many parameters\n  * Sampled Shapley provides Explainable AI for tabular or structured datasets\n\n* **Cloud Services and Integration (continued)**\n  * Access logging is the correct answer for container logging\n  * A Vertex AI Feature Store deals only with structured data"
      }
    },
    "summary": ""
  },
  "Chapter 1Framing ML Problems": {
    "content": "",
    "subsections": {
      "Translating Business Use Cases": {
        "content": "The goal of this chapter is to help you to first identify the impact, success criteria, and data available for a use case. Then, match this with a machine learning approach (an algorithm and a metric) as shown in Figure 1.1. We will look at how to fit the ML project into the budget and timeline.\n\n**FIGURE 1.1** Business case to ML problem\n\nNow, imagine you are being tasked with using ML to solve a problem. You first need to identify the use case and fit it to a machine learning problem.\n\nFor example, say you are trying to predict house prices, you can use a regression model. The performance requirements of the model will be determined by the business.\n\nSay, you have had a discussion with the key people and understood the use case; you now need to identify the key _stakeholders_ , the people related to the use case. The stakeholders can be executives, the CFO, data engineers, tech support staff who may have to approve the project to proceed. Each of these stakeholders might have very different expectations of this ML project, and your ability to communicate the value could make the difference between approval or rejection. Executives are looking for impact to business, CFOs are typically interested in the budget of the solution, managers might be keen on timelines, and data managers might be interested in data privacy and security. If you are able to understand these five aspects, your pathway to approvals will be smooth.\n\nThe stakeholders will help you measure the _impact_ of this use case for your company and the end user. The impact could be increasing profit, reducing fraud, improving the quality of life, or even saving lives. The impact is probably the most important element of the use case.\n\nFor example, say your company has a learning management system (LMS), a platform where students subscribe to courses. You have data of students' activities and using this you want to improve the experience using machine learning. You could do several things:\n\n  * Create a recommendation engine to show new courses for students.\n  * Churn prediction to see if a student is going to quit the course.\n  * Churn prediction to see if a teacher is not going to come back.\n  * Identify what makes a course interesting for students (sample questions, more images, more tables, short videos, etc.).\n  * Identify what kind of learning a student prefers (auditory, visual, or kinesthetic).\n\nWhich of these would be most impactful is a question that can be answered only by the business owner.\n\nOnce you have identified the use case with the highest impact on your business, you need to identify the outcome of your machine learning solution. In short, what would happen if you implemented your solution? Sometimes, your model would make accurate predictions, but the environment might react in a counterproductive way to these predictions. This is because the environment is seldom static; the users could adapt or the users could get confused with the behavior of the predictions.\n\nFor example, say your company has a video sharing website, and you have millions of videos. You are trying to build an ML model to recommend videos to your users. You could choose from among the following:\n\n  * An ML model to recommend unseen videos from popular video creators. The problem is that this is not personalized. What if the user does not like some of the creators?\n  * An ML model to recommend videos that get a lot of clicks. But what if these are just clickbait, where people click and regret wasting time?\n  * An ML model to recommend videos that have been watched fully by similar users. This would lead to improving the user experience.\n\nIn this example, you need to have a good understanding of the use case, the overall goal, and the end user to be able to find the right fit.\n\nNext, find out if the problem is even solvable using machine learning. Business leaders hear inspiring stories in the media about how a business solves a problem with ML and it sounds magical and the business leaders would love to use it to solve their business problems. They need an expert like you to figure out if it is even feasible to solve their problem using ML. This is not as easy as it sounds; it depends on several things, like existing technology, available data, and budget. For example, natural language processing has advanced leaps and bounds and has made it possible to do things that were impossible just a few years ago, such as using ML to answer a question from a piece of text. Familiarity with the latest advancements in natural language processing would help you identify easier, faster, and better ML methods to solve your business problems.\n\nAs the next step, you will need to identify an ML learning approach that fits your use case.",
        "subsections": {},
        "summary": "* _Identify the impact, success criteria, and data available for a use case_ \n  * Understand stakeholders' expectations and priorities\n  * Determine the most impactful aspect of the use case\n* **Match the use case with a machine learning approach**\n  * Choose an algorithm and metric suitable for the problem\n  * Consider feasibility based on existing technology, data, and budget"
      },
      "Machine Learning Approaches": {
        "content": "Many machine learning problems have been well researched and have elegant solutions, but some algorithms are not perfect and some problems can be solved in multiple ways. Sometimes, a use case will fit perfectly with an ML framework and other times not so well. You need to be aware of the landscape of ML problems. There are several approaches to machine learning methods. Some of these approaches have been studied for decades, and others are fairly new. There are hundreds if not thousands of ways to apply machine learning techniques. To help us get a grasp of the breadth of these methods, we organize them into categories (also called methods, or types or approaches or problems). Each of these approaches solves a specific class of problems, distinguished by the type of data, the type of prediction, and so on.\n\n* * *\n\nOn the exam, you will be given the details of a use case and will be expected to understand the nature of the problem and find the appropriate machine learning approach to solve it. To accomplish that, you need to have wide knowledge of the landscape of these machine learning approaches.\n\n* * *\n\nWe will look at the different ways to classify the approaches in the following sections.",
        "subsections": {
          "Supervised, Unsupervised, and Semi‐supervised Learning": {
            "content": "A common method of classifying machine learning approaches is based on the type of learning. When you have a labeled dataset that you can use to train your model, it is called _supervised learning_. For example, supervised learning would be trying to build a model to classify images of dogs or cats and having the ability to use a dataset of images that have been labeled accordingly.\n\nThere are some cases where you have only unlabeled data, such as a set of images (without any labels or tags), and you will be asked to classify or group them. This would be an _unsupervised_ ML model. Clustering algorithms are a suite of algorithms that belong to this type and are used to group and/or classify data. Autoencoders are also a family of algorithms that belong to this type. Autoencoders are used to reduce the dimensionality of input data, a preprocessing step in many machine learning models.\n\nAnother popular unsupervised ML use case is _topic modeling_ , a type of document clustering problem. The algorithm takes documents and classifies them into N number of classes based on the commonality of words and sentences in the texts. Comparing this to how a human being would classify books, say, in a library, you may classify them into fiction, nonfiction, science, history, and so on. In other times, you may classify the books based on languages (for example, English, Chinese, Hindi). Similarly, an unsupervised algorithm may or may not classify in the way you expected. The output of unsupervised learning methods cannot be fully controlled, and it is almost never perfect and so requires careful tuning to get required results. Table 1.1 provides the details of some of the popular ML model types that are readily available in Google Cloud.\n\n**TABLE 1.1** ML problem types\n\nSource: Adapted from Google cloud/ `https://cloud.google.com/vertex-ai/docs/training-overview` last accessed December 16, 2022.\n\nName | Data Type | Supervised/Unsupervised\n---|---|---\nRegression – Tables | Tabular | Supervised\nClassification – Tables | Tabular | Supervised\nForecasting | Series | Supervised\nImage classification | Image | Supervised\nImage segmentation | Image | Supervised\nObject detection | Image | Supervised\nVideo classification | Video | Supervised\nVideo object tracking | Video | Supervised\nVideo action recognition | Video | Supervised\nSentiment analysis | Text | Supervised\nEntity extraction | Text | Supervised\nTranslation | Text | Supervised\nK‐means clustering | Tabular | Unsupervised\nPrincipal component analysis | Tabular | Unsupervised\nTopic modeling | Text | Unsupervised\nCollaborative filtering/recommendations | Mixed | Supervised/Unsupervised\n\nTo solve the problem of uncertainty in unsupervised learning, there is a hybrid solution called _semi‐supervised learning_ , where some data is labeled and other data is not. This is like guiding the algorithm toward the clusters that you want to see. While semi‐supervised models are interesting research topics and have some utility, in a majority of use cases, supervised models are used.\n\nThere are many other kinds of machine learning models beyond these, including reinforcement learning (where the algorithm is not given data but is given an environment that the agent explores and learns) and active learning algorithm, but they are beyond the scope of the certificate exam.\n\nAnother way to classify the machine learning algorithms is based on the type of prediction. The type of data the model will predict determines several aspects of the machine learning algorithm and the method used. We will explore that next.",
            "subsections": {},
            "summary": "* **Machine Learning Approaches**: \n  * _Supervised Learning_: Using labeled datasets to train models, e.g., image classification.\n  * _Unsupervised Learning_: Classifying or grouping data without labels, e.g., clustering algorithms and topic modeling.\n    * *_Clustering Algorithms_*: Grouping similar data points together.\n    * *_Autoencoders_*: Reducing dimensionality of input data.\n    * *_Topic Modeling_*: Classifying documents into topics based on word and sentence patterns."
          },
          "Classification, Regression, Forecasting, and Clustering": {
            "content": "_Classification_ is the process of predicting the “labels” or “classes” or “categories.” Given a picture of a pet, classifying dogs versus cats is a classification problem. If there are just two labels, it is called _binary classification_ , and if there are more labels, it is called _multiclass classification_. You could have a classification with thousands of labels; for example, the Cloud Vision API can classify millions of different objects in a picture, which is a more difficult problem to solve. You cannot apply the same model for binary classification, multiclass classification, and classification with thousands of classes.\n\nIn _regression_ , the ML model predicts a number—for example, prediction of house price (given the number of bedrooms, square footage, zip code), prediction of the amount of rainfall (given temperature, humidity, location). Here the predicted value's range depends on the use case. The ML algorithms used for regression are usually different from classification. Typically, you would find structured data (data in rows and columns), as shown in Table 1.2, being used for regression problems.\n\n**TABLE 1.2** Structured data\n\nStudent ID | Age | Exam Scores (Out of 100)\n---|---|---\n1 | 34 | 75\n2 | 23 | 59\n3 | 36 | 92\n4 | 31 | 67\n\n_Forecasting_ is another type where the input is time‐series data and the model predicts the future values. In a time‐series dataset (Table 1.3), you get a series of input values that are indexed in time order. For example, you have a series of temperature measurements taken every hour for 10 hours from a sensor. In this case, one temperature reading is related to the previous and next reading because they are from the same sensor, in subsequent hours, and usually only vary to a small extent by the hour, so they are not considered to be “independent” (an important distinction from other types of structured data).\n\n**TABLE 1.3** Time‐Series Data\n\n****|  Temperature\n---|---\nSeries 1 | 29, 30, 40, 39, 23, 20\nSeries 2 | 10, 11, 13, 23, 43, 34\nSeries 2 | 19, 18, 19, 20, 38, 20\nSeries 4 | 14, 17, 34, 34, 12, 43\n\nSome forecasting problems can be converted to regression problems by modifying the time‐series data into independent and identically distributed (IID) values. This is done either for convenience or availability of data or for preference for a certain type of ML model. In other cases, regression problems can be converted into classification problems by bucketizing the values. We will look into details in the following chapters. There is an art to fitting an ML model to a use case.\n\n_Clustering_ is another type of problem, where the algorithm creates groups in the data based on inherent similarities and differences among the different data points. For example, if we are given the latitude and longitude of every house on Earth, the algorithm might group each of these data points into clusters of cities based on the distances between groups of houses. K‐means is a popular algorithm in this type.",
            "subsections": {},
            "summary": "* **Classification**: predicting labels or classes from input data (e.g., dogs vs. cats) \n    * Binary classification: 2-class problem (dogs vs. cats)\n    * Multiclass classification: problem with multiple classes\n* **Regression**: predicting continuous values from input data (e.g., house price, rainfall amount)\n* **Forecasting**: predicting future values in a time-series dataset (e.g., temperature measurements) \n    * Can be converted to regression or classification problems"
          }
        },
        "summary": "**Machine Learning Landscape**\n\n* Many ML problems are well-researched with elegant solutions\n* Some algorithms are not perfect and some problems can be solved in multiple ways\n* There are hundreds of ways to apply machine learning techniques, requiring a broad knowledge of approaches\n* Each approach solves a specific class of problems distinguished by data type and prediction type"
      },
      "ML Success Metrics": {
        "content": "A business problem can be solved using many different machine learning algorithms, so which one to choose? An _ML metric_ (or a suite of metrics) is used to determine if the trained model is accurate enough. After you train the model (supervised learning), you will predict the values (y) for, say, N data points for which you know the actual value (y). We will use a formula to calculate the metric from these N predictions.\n\nThere are several metrics with different properties. If so, what is our metric? What is the formula for calculating the metric? Does the metric align with the business success criteria? To answer these questions, let us look at each class of problems, starting with classification.\n\nSay you are trying to detect a rare fatal disease from an X‐ray. This is a binary classification problem with two possible outcomes: positive/negative. You are given a set of a million labeled X‐ray images with only 1 percent of the cases with the disease, a positive data point. In this case, a wrong negative (false negative), where we predict that the patient does not have the disease when they actually do have it, might cause the patient to not take timely action and cause harm due to inaction. But a wrong positive prediction (false positive), where we predict that the patient has the disease when in fact they do not, might cause undue concern for the patient. This will result in further medical tests to confirm the prediction. In this case, accuracy (the percentage of correct prediction) is not the correct metric.\n\nLet us now consider an example with prediction numbers for a binary classification for an unbalanced dataset, shown in Table 1.4.\n\n**TABLE 1.4** Confusion matrix for a binary classification example\n\n****|  Predicted\n---|---\n**Actual** |  | **Positive Prediction** | **Negative Prediction**\n**Positive Class** | 5 | 2\n**Negative Class** | 3 | 990\n\nThere are two possible prediction classes, positive and negative. Usually the smaller class (almost always the more important class) is represented as the positive class. In Table 1.4, we have a total of 1,000 data points and have predictions for each. We have tabulated the predictions against the actual values. Out of 1,000 data points, there are 7 belonging to the positive class and 993 belonging to the negative class. The model has predicted 8 to be in the positive class and 992 in the negative class. The bottom right represents true negatives (990 correctly predicted negatives) and the top left represents true positives (5 correctly predicted positives). The bottom left represents false positives (3 incorrectly predicted as positive) and the top right represents false negatives (2 incorrectly predicted as negative). Now, using the numbers in this confusion matrix, we can calculate various metrics based on our needs.\n\nIf this model is to detect cancer, we do not want to miss detecting the disease; in other words, we want a low false negative rate. In this case, _recall_ is a good metric.\n\nIn our case, recall = 5/(5 **+** 2) = 0.714. If false positives are higher, the recall metric will be lower because false negative is in the denominator. Recall can range from 0 to 1, and a higher score is better. Intuitively, recall is the measure of what percentage of the positive data points the model was able to predict correctly.\n\nOn the other hand, if this is a different use case and you are trying to reduce false positives, then you can use the precision metric.\n\nIn our case, we have 3 false positives, so our precision score is 5/(5 + 3) = 0.625. Intuitively, precision quantifies the percentage of positive predictions that were actually correct.\n\nSometimes, your use case might be interested in reducing both false positives and false negatives simultaneously. In that case, we use a harmonic mean of both precision and recall, and it is called the F1 score. (There is a more general Fβ score depending on how you wish to weight precision and recall and F1 is just one case.)\n\nIn our example, we get 2 x (0.625 x 0.714)/(0.625 + 0.714) = 0.666. Here again, F1 ranges from 0 to 1, and a higher score indicates a higher‐quality model. The three metrics are summarized in Table 1.5.\n\n**TABLE 1.5** Summary of metrics\n\n****|  Scenario | Formula\n---|---|---\nPrecision | Lower false positive |\nRecall | Lower false negative |\nF1 | Lower false positive and false negative together |",
        "subsections": {
          "Area Under the Curve Receiver Operating Characteristic (AUC ROC)": {
            "content": "ROC stands for receiver operating characteristic curve (it comes from the field of signal processing) and is a graphical plot that summarizes the performance of a binary classification model (Figure 1.2). The x‐axis is the false positive rate, and the y‐axis is the true positive rate, and the plot is generated at different classification thresholds. The ideal point for this plot is the top‐left corner, which has 100 percent true positive and 0 percent false positive, but in practice you will never see this. You can also calculate the precision, recall, and F1 at each point on the curve. When you visually inspect the curve, a diagonal line is the worst case, and we want the curve to stretch as far from the diagonal as possible.\n\nWhen you have two models, you get two ROC curves, and the way to compare them is to calculate the area under the curve (AUC).\n\nOnce you have chosen the model based on AUC, you can find the threshold point that maximizes your F1 (as indicated in Figure 1.2).\n\n**FIGURE 1.2** AUC\n\nThis method has the following advantages:\n\n  * **Scale‐invariant:** It measures how well the predictions are ranked and not their absolute values.\n  * **Classification threshold‐invariant:** It helps you measure the model irrespective of what threshold is chosen.\n\n* * *\n\nClassification threshold invariance is not always desirable because sometimes there are huge disparities between false positives and false negatives. Therefore, AUC is not usually the best metric for picking a model when there is class imbalance.\n\n* * *",
            "subsections": {},
            "summary": "**ROC Curve Summary**\n\n* **Definition:** Receiver Operating Characteristic curve plots binary classification model performance\n* \n  * _X-axis: False positive rate_\n  * _Y-axis: True positive rate_\n* \n  * _Ideal point: Top-left corner (100% true positive, 0% false positive)_\n*"
          },
          "The Area Under the Precision‐Recall (AUC PR) Curve": {
            "content": "The area under the precision‐recall curve is a graphical plot that illustrates the relationship between a precision‐recall pair (Figure 1.3). The x‐axis is the recall and the y‐axis is the precision. The best AUC PR curve is a horizontal line across the top. In this curve, the optimal point is the top‐right corner, which has 100 percent precision and 100 percent recall, which is never seen in practice but always aimed at.\n\n**FIGURE 1.3** AUC PR\n\nIf the dataset is highly imbalanced, the AUC PR is preferred because a high number of true negatives can cause the AUC curve to be skewed.",
            "subsections": {},
            "summary": "* **Precision-Recall Curve**: Graphical plot showing relationship between recall and precision, with x-axis representing recall and y-axis representing precision.\n* _AUC PR Best Performance_: Horizontal line at top of graph, indicating 100% precision and 100% recall, which is never achieved but aims for.\n* _Dataset Imbalance_: AUC PR preferred over other metrics when dataset is highly imbalanced to avoid skewing."
          },
          "Regression": {
            "content": "Regression predicts a numerical value. The metric should try to show the quantitative difference between the actual value and the predicted value.\n\n  * **MAE** The mean absolute error (MAE) is the average absolute difference between the actual values and the predicted values.\n  * **RMSE** The root‐mean‐squared error (RMSE) is the square root of the average squared difference between the target and predicted values. If you are worried that your model might incorrectly predict a very large value and want to penalize the model, you can use this. Ranges from 0 to infinity.\n  * **RMSLE** The root‐mean‐squared logarithmic error (RMSLE) metric is similar to RMSE, except that it uses the natural logarithm of the predicted and actual values +1. This is an asymmetric metric, which penalizes under prediction (value predicted is lower than actual) rather than over prediction.\n  * **MAPE** Mean absolute percentage error (MAPE) is the average absolute percentage difference between the labels and the predicted values. You would choose MAPE when you care about proportional difference between actual and predicted value.\n  * **R 2 ** R‐squared (R2) is the square of the Pearson correlation coefficient (r) between the labels and predicted values. This metric ranges from zero to one; and generally a higher value indicates a better fit for the model.",
            "subsections": {},
            "summary": "* *_Metrics for Evaluating Regression Models_*\n  * *_Mean Absolute Error (MAE)_*: Average absolute difference between actual and predicted values.\n  * *_Root-Mean-Squared Error (RMSE)_*: Square root of average squared difference between target and predicted values.\n  * *_Root-Mean-Squared Logarithmic Error (RMSLE)_*: Asymmetric metric using natural logarithm, penalizes under prediction."
          }
        },
        "summary": "* **Choosing an ML Metric**: To determine if a trained model is accurate enough, use an _ML metric_ (or a suite of metrics) to evaluate its performance.\n* **Classification Metrics**: For binary classification problems like detecting a rare disease from X-rays, use:\n\t* **Recall** for low false negative rates\n\t* **Precision** for reducing false positives\n\t* **F1 score** as the harmonic mean of precision and recall"
      },
      "Responsible AI Practices": {
        "content": "AI and machine learning are powerful new tools, and with power comes responsibility. You should consider fairness, interpretability, privacy, and security in your ML solution. You can borrow from best practices in software engineering in tandem with considerations unique to machine learning.\n\n  * **General Best Practices** Always have the end user in mind as well as their user experience. How does your solution change someone's life? Solicit feedback early in the design process. Engage and test with a diverse set of users you would expect to use your solution. This will build a rich variety of perspectives and will allow you to adjust early in the design phase.\n  * **Fairness** Fairness is very important because machine learning models can reflect and reinforce unfair biases. Fairness is also difficult in practice because there are several definitions of fairness from different perspectives (academic, legal, cultural, etc.). Also, it is not possible to apply the same “fairness” to all situations as it is very contextual. To start with, you can use statistical methods to measure bias in datasets and to test ML models for bias in the evaluation phase.\n  * **Interpretability** Some popular state‐of‐the‐art machine learning models like neural networks are too complex for human beings to comprehend, so they are treated as black boxes. The lack of visibility creates doubt and could have hidden biases. _Interpretability_ is the science of gaining insights into models and predictions. Some models are inherently more interpretable (like linear regression, decision trees) and others are less interpretable (deep learning models). One way to improve interpretability is to use _model explanations_. Model explanations quantify the contributions of each input feature toward making a prediction. However, not all algorithms support model explanations. In some domains, model explanations are mandated, so your choice of algorithms is restricted.\n  * **Privacy** The only connection between the training data and prediction is the ML model. While the model only provides predictions from input values, there are some cases where it can reveal some details about the training data. This becomes a serious issue if you trained with sensitive data like medical history, for example. Although the science of detecting and preventing data leakage is still an area of active research, fortunately there are now techniques to minimize leakage in a precise and principled fashion.\n  * **Security** The threat of cybersecurity is very much applicable to machine learning. In addition to the usual threats to any digital application, there are some unique security challenges to machine learning applications. These threats are ever present, from the data collection phase (poison data), training phase (leakage of training data), and deployment phase (stealing of models). It is important to identify potential threats to the system, keep learning to stay ahead of the curve, and develop approaches to combat these threats.\n\nYou can read more at `https://ai.google/responsibilities`.",
        "subsections": {},
        "summary": "* **Key Considerations for ML Solutions**\n    * Fairness: mitigate bias in models using statistical methods\n    * Interpretability: improve visibility into model predictions with model explanations\n    * Privacy and Security: detect and prevent data leakage and cybersecurity threats"
      },
      "Summary": {
        "content": "In this chapter, you learned how to take a business use case and understand the different dimensions to an ask and to frame a machine learning problem statement as a first step.",
        "subsections": {},
        "summary": "* *Business Use Case Understanding*: Learn to break down a business question into its core components.\n* *Machine Learning Problem Statement*: Frame a problem in terms of data, actions, and outcomes for a machine learning solution.\n* *First Steps*: Understand the ask's dimensions, including what, how, and why."
      },
      "Exam Essentials": {
        "content": "* **Translate business challenges to machine learning.** Understand the business use case that wants to solve a problem using machine learning. Understand the type of problem, the data availability, expected outcomes, stakeholders, budget, and timelines.\n  * **Understand the problem types.** Understand regression, classification, and forecasting. Be able to tell the difference in data types and popular algorithms for each problem type.\n  * **Know how to use ML metrics.** Understand what a metric is, and match the metric with the use case. Know the different metrics for each problem type, like precision, recall, F1, AUC ROC, RMSE, and MAPE.\n  * **Understand Google's Responsible AI principles.** Understand the recommended practices for AI in the context of fairness, interpretability, privacy, and security.",
        "subsections": {},
        "summary": "* **Machine Learning for Business Challenges**\nTranslate business challenges to machine learning by understanding:\n* _Problem types_: regression, classification, and forecasting\n* ML metrics: precision, recall, F1, AUC ROC, RMSE, MAPE\n* _Responsible AI principles_: fairness, interpretability, privacy, security"
      },
      "Review Questions": {
        "content": "1. When analyzing a potential use case, what are the first things you should look for? (Choose three.)\n     1. Impact\n     2. Success criteria\n     3. Algorithm\n     4. Budget and time frames\n  2. When you try to find the best ML problem for a business use case, which of these aspects is not considered?\n     1. Model algorithm\n     2. Hyperparameters\n     3. Metric\n     4. Data availability\n  3. Your company wants to predict the amount of rainfall for the next 7 days using machine learning. What kind of ML problem is this?\n     1. Classification\n     2. Forecasting\n     3. Clustering\n     4. Reinforcement learning\n  4. You work for a large company that gets thousands of support tickets daily. Your manager wants you to create a machine learning model to detect if a support ticket is valid or not. What type of model would you choose?\n     1. Linear regression\n     2. Binary classification\n     3. Topic modeling\n     4. Multiclass classification\n  5. You are building an advanced camera product for sports, and you want to track the ball. What kind of problem is this?\n     1. Not possible with current state‐of‐the‐art algorithms\n     2. Image detection\n     3. Video object tracking\n     4. Scene detection\n  6. Your company has millions of academic papers from several research teams. You want to organize them in some way, but there is no company policy on how to classify the documents. You are looking for any way to cluster the documents and gain any insight into popular trends. What can you do?\n     1. Not much. The problem is not well defined.\n     2. Use a simple regression problem.\n     3. Use binary classification.\n     4. Use topic modeling.\n  7. What metric would you never chose for linear regression?\n     1. RMSE\n     2. MAPE\n     3. Precision\n     4. MAE\n  8. You are building a machine learning model to predict house prices. You want to make sure the prediction does not have extreme errors. What metric would you choose?\n     1. RMSE\n     2. RMSLE\n     3. MAE\n     4. MAPE\n  9. You are building a plant classification model to predict variety1 and variety2, which are found in equal numbers in the field. What metric would you choose?\n     1. Accuracy\n     2. RMSE\n     3. MAPE\n     4. R2\n  10. You work for a large car manufacturer and are asked to detect hidden cracks in engines using X‐ray images. However, missing a crack could mean the engine could fail at some random time while someone is driving the car. Cracks are relatively rare and happen in about 1 in 100 engines. A special camera takes an X‐ray image of the engine as it comes through the assembly line. You are going to build a machine learning model to classify if an engine has a crack or not. If a crack is detected, the engine would go through further testing to verify. What metric would you choose for your classification model?\n     1. Accuracy\n     2. Precision\n     3. Recall\n     4. RMSE\n  11. You are asked to build a classification model and are given a training dataset but the data is not labeled. You are asked to identify ways of using machine learning with this data. What type of learning will you use?\n     1. Supervised learning\n     2. Unsupervised learning\n     3. Semi‐supervised learning\n     4. Reinforcement learning\n  12. You work at a company that hosts millions of videos and you have thousands of users. The website has a Like button for users to click, and some videos get thousands of “likes.” You are asked to create a machine learning model to recommend videos to users based on all the data collected to increase the amount of time users spend on your website. What would be your ML approach?\n     1. Supervised learning to predict based on the popularity of videos\n     2. Deep learning model based on the amount of time users watch the videos\n     3. Collaborative filtering method based on explicit feedback\n     4. Semi‐supervised learning because you have some data about some videos\n  13. You work for the web department of a large hardware store chain. You have built a visual search engine for the website. You want to build a model to classify whether an image contains a product. There are new products being introduced on a weekly basis to your product catalog and these new products must be incorporated into the visual search engine. Which of the following options is a bad idea?\n     1. Create a pipeline to automate the step: take the dataset, train a model.\n     2. Create a golden dataset and do not change the dataset for at least a year because creating a dataset is time‐consuming.\n     3. Extend the dataset to include new products frequently and retrain the model.\n     4. Add evaluation of the model as part of the pipeline.\n  14. Which of the following options is not a type of machine learning approach?\n     1. Supervised learning\n     2. Unsupervised learning\n     3. Semi‐supervised learning\n     4. Hyper‐supervised learning\n  15. Your manager is discussing a machine learning approach and is asking you about feeding the output of one model to another model. Select two statements that are true about this kind of approach.\n     1. There are many ML pipelines where the output of one model is fed into another.\n     2. This is a poor design and never done in practice.\n     3. Never feed the output of one model into another model. It may amplify errors.\n     4. There are several design patterns where the output of one model (like encoder or transformer) is passed into a second model and so on.\n  16. You are building a model that is going to predict credit‐worthiness and will be used to approve loans. You have created a model and it is performing extremely well and has high impact. What next?\n     1. Deploy the model.\n     2. Deploy the model and integrate it with the system.\n     3. Hand it over to the software integration team.\n     4. Test your model and data for biases (gender, race, etc.).\n  17. You built a model to predict credit‐worthiness, and your training data was checked for biases. Your manager still wants to know the reason for each prediction and what the model does. What do you do?\n     1. Get more testing data.\n     2. The ML model is a black box. You cannot satisfy this requirement.\n     3. Use model interpretability/explanations.\n     4. Remove all fields that may cause bias (race, gender, etc.).\n  18. Your company is building an Android app to add funny moustaches on photos. You built a deep learning model to detect the location of a face in a photo, and your model had very high accuracy based on a public photo dataset that you found online. When integrated into an Android phone app, it got negative feedback on accuracy. What could be the reason?\n     1. The model was not deployed properly.\n     2. Android phones could not handle a deep learning model.\n     3. Your dataset was not representative of all users.\n     4. The metric was wrong.\n  19. You built a deep learning model to predict cancer based on thousands of personal records and scans. The data was used in training and testing. The model is secured behind a firewall, and all cybersecurity precautions have been taken. Are there any privacy concerns? (Choose two.)\n     1. No. There are no privacy concerns. This does not contain photographs, only scans.\n     2. Yes. This is sensitive data being used.\n     3. No. Although sensitive data is used, it is only for training and testing.\n     4. The model could reveal some detail about the training data. There is a risk.\n  20. You work for an online shoe store and the company wants to increase revenue. You have a large dataset that includes the browsing history of thousands of customers, and also their shopping cart history. You have been asked to create a recommendation model. Which of the following is not a valid next step?\n     1. Use your ML model to recommend products at checkout.\n     2. Creatively use all the data to get maximum value because there is no privacy concern.\n     3. Periodically retrain the model to adjust for performance and also to include new products.\n     4. In addition to the user history, you can use the data about product (description, images) in training your model.",
        "subsections": {},
        "summary": "* **Analyzing a potential use case**\n    * Look for *_Impact_*, *_Success criteria_*, and *_Budget and time frames_* \n    * Machine learning problems involve *_model algorithm_*, *_hyperparameters_*, *_metric_*, and *_data availability_* \n\n* **Predicting rainfall**\n    * This is an example of *_Forecasting_* \n\n* **Support ticket classification model**\n    * A *_binary classification_* model would be suitable\n\n* **Tracking the ball with a camera**\n    * This involves *_Video object tracking_* \n\n* **Organizing academic papers**\n    * You can use *_topic modeling_* to cluster documents and gain insight into popular trends\n\n* **Choosing a metric for linear regression**\n    * *_RMSE_* is not typically used for linear regression, instead *_MAE_* or *_MAPE_* are common choices\n\n* **Predicting house prices with minimal error**\n    * You would choose *_RMSLE_* as your metric\n\n* **Plant classification model**\n    * A suitable metric for this problem would be *_Accuracy_* \n\n* **Detecting hidden cracks in engines**\n    * For a classification model, you would choose to use *_Precision_* and *_Recall_*  metrics\n\n* **Using the output of one model in another model**\n    * This is a common practice in ML pipelines and is used in many designs patterns.\n\n* **Deploying a credit-worthiness model**\n    * After deployment, you should test for biases (gender, race, etc.) and use *_model interpretability/explanations_* to understand the reasoning behind each prediction\n\n* **Improving a deep learning model's accuracy on Android app**\n    * Poor performance might be due to a lack of representative data or issues with the dataset itself.\n\n* **Privacy concerns with cancer diagnosis models**\n    * There are privacy concerns even when using sensitive data for training and testing, as the model could reveal some detail about the training data.\n\n* **Creating a recommendation model in an online shoe store**\n    * You should not use the data to get maximum value without considering *_privacy concerns_*."
      }
    },
    "summary": ""
  },
  "Chapter 2Exploring Data and Building Data Pipelines": {
    "content": "",
    "subsections": {
      "Visualization": {
        "content": "_Data visualization_ is a data exploratory technique to find trends and outliers in the data. Data visualization helps in the data cleaning process because you can find out whether your data is imbalanced by visualizing the data on a chart. It also helps in the feature engineering process because you can select features and discard features and see how a feature will influence your model by visualizing it.\n\nThere are two ways to visualize data:\n\n  * **Univariate Analysis** In this analysis, each of the features is analyzed independently, such as the range of the feature and whether outliers exist in the data. The most common visuals used for this are box plots and distribution plots.\n  * **Bivariate Analysis** In this analysis, we compare the data between two features. This analysis can be helpful in finding correlation between features. Some of the ways you can perform this analysis are by using line plots, bar plots, and scatterplots.",
        "subsections": {
          "Box Plot": {
            "content": "A box plot helps visualize the division of observations into defined intervals known as _quartiles_ and how that compares to the entire observation. It represents the data as 25th, 50th, and 75th quartiles. It consists of the body, or interquartile range, where maximum observations are present. Whiskers or straight lines represent the maximum and minimum. Points that lie outside the whiskers will be considered _outliers_.\n\nFigure 2.1 shows a box plot.\n\n**FIGURE 2.1** Box plot showing quartiles",
            "subsections": {},
            "summary": "* A box plot displays the distribution of data using 25th, 50th, and 75th quartiles.\n* The body represents the interquartile range where most observations are present.\n* Whiskers show maximum and minimum values, with outliers beyond these points."
          },
          "Line Plot": {
            "content": "A line plot plots the relationship between two variables and is used to analyze the trends for data changes over time.\n\nFigure 2.2 shows a line plot.\n\n**FIGURE 2.2** Line plot",
            "subsections": {},
            "summary": "* A line plot displays the relationship between two variables, analyzing trends in data over time.\n* It is used to identify patterns and changes in data as it moves over time.\n* *Visual representation of data changes over time.*"
          },
          "Bar Plot": {
            "content": "A bar plot is used for analyzing trends in data and comparing categorical data such as sales figures every week, the number of visitors to a website, or revenue from a product every month.\n\nFigure 2.3 shows a bar plot.\n\n**FIGURE 2.3** Bar plot",
            "subsections": {},
            "summary": "* A bar plot displays **trends and comparisons** between categorical data\n* Common applications include analyzing sales figures, website visitors, and product revenue over time\n* Used for _visualizing_ trends and patterns in data."
          },
          "Scatterplot": {
            "content": "A scatterplot is the most common plot used in data science and is mostly used to visualize clusters in a dataset and show the relationship between two variables.",
            "subsections": {},
            "summary": "* **Scatterplot**: Visualizes relationships between 2 variables, showing clusters and patterns in a dataset.\n    * Used for:\n        * _Visualizing data distributions_\n        * Identifying correlations between variables\n        * Detecting outliers and anomalies"
          }
        },
        "summary": "* *_Data Visualization_* \n    * Helps in data cleaning by identifying imbalances\n    * Facilitates feature engineering to understand feature influence on models\n    * Two main types:\n        * *_Univariate Analysis_*: Independent analysis of features (box plots, distribution plots)\n        * *_Bivariate Analysis_*: Comparison of data between two features (line plots, bar plots, scatterplots)"
      },
      "Statistics Fundamentals": {
        "content": "In statistics, we have three measures of central tendency: mean, median, and mode. They help us describe the data and can be used to clean data statistically.",
        "subsections": {
          "Mean": {
            "content": "Mean is the accurate measure to describe the data when we do not have any outliers present.",
            "subsections": {},
            "summary": "Mean is the accurate measure to describe the data when we do not have any outliers present."
          },
          "Median": {
            "content": "Median is used if there is an outlier in the dataset. You can find the median by arranging data values from the lowest to the highest value.\n\nIf there are even numbers, the median is the average of two numbers in the middle, and if there are odd numbers, the median is the middle value. For example, in the dataset 1, 1, 2, 4, 6, 6, 9, the median is 4. For the dataset 1, 1, 4, 6, 6, 9, the median is 5. Take the mean of 4 and 6, or (4+6) / 2 = 5.",
            "subsections": {},
            "summary": "* **Median**: calculated when data contains an outlier; found by arranging values from lowest to highest\n* _Example_: if even numbers are present, the median is the average of two middle numbers; for odd numbers, it's the single middle value.\n* **Calculation**: take mean of two middle numbers (if even) or single middle number (if odd)."
          },
          "Mode": {
            "content": "Mode is used if there is an outlier and the majority of the data is the same. Mode is the value or values in the dataset that occur most.\n\nFor example, for the dataset 1, 1, 2, 5, 5, 5, 9, the mode is 5.",
            "subsections": {},
            "summary": "* *_Mode_*: The value(s) that appear most frequently in a dataset\n    * It is used when the majority of data points are the same, but there's an outlier\n    * Example: Dataset 1, 1, 2, 5, 5, 5, 9 where 5 appears most"
          },
          "Outlier Detection": {
            "content": "Mean is the measure of central tendency that is affected by the outliers, which in turn impacts standard deviation.\n\nFor example, consider the following small dataset:\n\n  * [15, 18, 7, 13, 16, 11, 21, 5, 15, 10, 9, 210]\n\nBy looking at it, one can quickly say 210 is an outlier that is much larger than the other values.\n\nAs you can see from Table 2.1, there has been a significant change in mean by adding an outlier compared to median and mode. _Variance_ is the average of the squared differences from the mean.\n\n**TABLE 2.1** Mean, median, and mode for outlier detection\n\nWith Outlier | Without Outlier\n---|---\nMean: 12.72 | Mean: 29.16\nMedian: 13 | Median: 14\nMode: 15 | Mode: 15",
            "subsections": {},
            "summary": "* **Effect of outliers on mean**: The presence of an outlier can significantly impact the mean, making it less representative of the dataset's central tendency.\n    * _Outliers_ have a large influence on the mean, making it more susceptible to changes in data.\n        + Adding an outlier like 210 skews the mean from 29.16 (without outlier) to 12.72 (with outlier)."
          },
          "Standard Deviation": {
            "content": "Standard deviation is the square root of the variance. Standard deviation is an excellent way to identify outliers. Data points that lie more than one standard deviation from the mean can be considered unusual.\n\n* * *\n\nCovariance is a measure of how much two random variables vary from each other.\n\n* * *",
            "subsections": {},
            "summary": "### Understanding Statistical Measures\n\n#### **Standard Deviation**\n* _Measure of data spread_\n* Square root of variance\n* Identifies outliers as points more than 1 standard deviation from the mean\n\n#### **Covariance**\n* Measure of how two variables vary together"
          },
          "Correlation": {
            "content": "Correlation is simply a normalized form of covariance. The value of the correlation coefficient ranges from –1 to +1. The correlation coefficient is also known as Pearson's correlation coefficient.\n\n  * **Positive Correlation** When we increase the value of one variable, the value of another variable increases respectively; this is called _positive correlation_.\n  * **Negative Correlation** When we increase the value of one variable, the value of another variable decreases respectively; this is called _negative correlation_.\n  * **Zero Correlation** When the change in the value of one variable does not impact the other substantially, then it is called _zero correlation_.\n\n* * *\n\nCorrelation is helpful in detecting label leakage. For highly correlated labels, for example, if you are training a cancer prediction model, you are using hospital name as a feature, which is highly correlated with the target variable, whether a person has cancer. This correlation can cause your model to learn on hospital names. Refer to this video for more details: `https://developers.google.com/machine-learning/crash-course/cancer-prediction`.\n\n* * *",
            "subsections": {},
            "summary": "Correlation is a measure of how two variables are related, ranging from -1 to +1.\n* _Types of Correlation_:\n  * **Positive Correlation**: Increase in one variable results in increase in another\n  * **Negative Correlation**: Increase in one variable results in decrease in another\n  * **Zero Correlation**: No substantial impact between variables\n* Correlation helps detect label leakage, where high correlation between features and target can lead to biased models."
          }
        },
        "summary": "* *_Three Measures of Central Tendency_* \n    * **Mean**: Average value of a dataset\n    * **Median**: Middle value in a sorted dataset\n    * **Mode**: Most frequently occurring value"
      },
      "Data Quality and Reliability": {
        "content": "The quality of your model is going to depend on the quality and reliability (or feasibility) of the data. Your model quality will also depend on the size of the training data.\n\n_Reliability_ is the degree to which you can trust your data. If your data is unreliable, that means it has missing values, duplicate values, and bad features; you can consider it as unclean data. If you train a model on unclean data, you are less likely to get useful predictions. To ensure your data is reliable, you can do the following:\n\n  * Check for label errors as sometimes humans do labeling and we do make mistakes.\n  * Check for noise in features, such as, for example, GPS measurements.\n  * Check for outliers and data skew.\n\nIt's important to have a concrete definition of quality while collecting the data. We will discuss several parameters of data quality in the following sections.",
        "subsections": {
          "Data Skew": {
            "content": "_Data skew_ means when the normal distribution curve is not symmetric, the data is skewed. It means that there are outliers in the data or the data distribution is not even.\n\nThe skewness for a normal distribution is 0.\n\nThe data can be right skewed or left skewed (see Figure 2.4). You can analyze skew by knowing the statistical measure such as mean and median and standard deviation from the dataset.\n\n**FIGURE 2.4** Data skew\n\nFor right‐skewed data, a real‐world example can be income data because most people will have an average income and only 0.01 percent will have income higher than rest of the population (billionaires such as Jeff Bezos), leading to outliers, or right skew.\n\nSkewed data does not work well with models because having extreme outliers affects the model's capability to predict well. With several transformations such as log transformation and normalization, you can transform skewed distribution to normal distribution by removing outliers. If the skewness is in the target variable, you can use the Synthetic Minority Oversampling Technique (SMOTE), undersampling, or oversampling.",
            "subsections": {},
            "summary": "* *_Data Skew_*: when the normal distribution curve is not symmetric, resulting in uneven data distribution.\n  * Causes of skew include outliers and extreme values.\n  * Types of skew: right-skewed and left-skewed data\n    * Right-skewed data: occurs with outlier-prone distributions like income data"
          },
          "Data Cleaning": {
            "content": "The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model. (See `https://developers.google.com/machine-learning/data-prep/transform/normalization`.)",
            "subsections": {},
            "summary": "* **What is Normalization?**: transforms features to be on a similar scale\n* *Improves model performance and training stability*\n* **Why is it used?**: To prevent features with large ranges from dominating the model"
          },
          "Scaling": {
            "content": "Scaling means converting floating‐point feature values from their natural range into a standard range—for example, from 1,000–5,000 to 0 to 1 or –1 to +1. Scaling is useful when a feature set consists of multiple features. It has the following benefits:\n\n  * In deep neural network training, scaled features help gradient descent converge better than non‐scaled features.\n  * Scaling removes the possibility of “NaN traps” as every number value is scaled to a range of numbers.\n  * Without scaling, the model will give too much importance to features having a wider range.\n\n* * *\n\nYou would use scaling when your data is uniformly distributed or has no skew with few or no outliers; for example, age can be scaled because every range will have a uniform number of people representing age.\n\n* * *",
            "subsections": {},
            "summary": "## Scaling in Feature Values\nScaling converts floating-point feature values to a standard range (e.g., 0-1) to improve model performance.\n### Benefits:\n* **Improved Gradient Descent Convergence**\n* **Removal of NaN Traps**\n* **Reduced Impact of Outliers**\n\n### Use Cases:\n* Uniformly distributed data with no skew\n* No outliers in the data"
          },
          "Log Scaling": {
            "content": "Log scaling is used when some of the data samples are in the power of law, or very large. For example, you would use log scaling when some of the sample is 10,000 while some is in the range 0–100.\n\nSo, taking a log will bring them to same range. For example, log of (100,000) = 100 and log of (100) = 10. Therefore, your data will be scaled to the 0 to 100 range with log scaling.",
            "subsections": {},
            "summary": "* *_Log Scaling_*: scales data to a common range by taking the logarithm of values.\n    * Used for data with varying scales, such as large and small numbers.\n    * Example: log(10,000) = 4, log(100) = 2"
          },
          "Z‐score": {
            "content": "This is another scaling method where the value is calculated as standard deviations away from the mean. You would calculate the z‐score as follows when you have a few outliers:\n\n  * Scaled value = (value − mean) / stddev\n\nFor example, given\n\n  * Mean = 100\n  * Standard deviation = 20\n  * Original value = 130\n\nthe scaled value is 1.5. The z‐score lies between –3 to +3, so anything outside of that range will be an outlier.",
            "subsections": {},
            "summary": "* **Z-score**: calculated as (value - mean) / stddev\n* *_Outliers_*: values beyond ±3 standard deviations from the mean\n* _Example_: scaled value = 1.5 for original value = 130, mean = 100, stddev = 20"
          },
          "Clipping": {
            "content": "In the case of extreme outliers, you can cap all feature values above or below to a certain fixed value. You can perform feature clipping before or after other normalization techniques.",
            "subsections": {},
            "summary": "* **Feature Clipping**: capping extreme outlier values to a fixed limit\n    * _Pre-clip_ vs. _Post-clip_: performed before or after other normalization techniques\n    * Purpose: prevent outliers from dominating the data"
          },
          "Handling Outliers": {
            "content": "An outlier is a value that is the odd one out or an observation that lies far from the rest of the data points because it is too large or too small. They may exist in data due to human error or skew.\n\nYou need to use the following visualization techniques and statistical techniques (some of which were discussed in previous sections) to detect outliers:\n\n  * Box plots\n  * Z‐score\n  * Clipping\n  * Interquartile range (IQR)\n\nOnce an outlier is detected, you can either remove it from the dataset so that it does not affect model training or impute or replace outlier data to either mean, median, mode, or boundary values.",
            "subsections": {},
            "summary": "* *_Outliers_*: Values that are significantly different from others in a dataset.\n    * **Detection Techniques**: Box plots, Z-score, Clipping, Interquartile Range (IQR)\n        * _Methods for handling outliers_: Removing, Imputing with mean/mode/boundary values"
          }
        },
        "summary": "* *_Data Quality Matters_* \n  * Unreliable data can lead to poor model performance\n  * Consider data with missing values, duplicates, and bad features as \"unclean\"\n  * Ensure data is reliable by checking for label errors, noise in features, and outliers"
      },
      "Establishing Data Constraints": {
        "content": "The data analysis and exploration process leads to key insights and outcomes such as data quality issues (missing values, outliers, and type conversions).\n\nTo have a consistent and reproducible check, you need to set up the data constraint by defining a schema for your ML pipeline.\n\nA schema with defined metadata describes the property of your data, such as data type (numerical vs. categorical), allowed range, format, and distribution of values of the data. A schema is an output of the data analysis process.\n\nThe following are the advantages of having a schema:\n\n  * For feature engineering and data transformation, your categorical data and numerical data needs to be transformed. Having a schema enables metadata‐driven preprocessing.\n  * You can validate new data using the data schema and catch anomalies such as skews and outliers during training and prediction.",
        "subsections": {
          "Exploration and Validation at Big‐Data Scale": {
            "content": "The volume of data is growing at a fast pace. To train deep neural networks, a large amount of data is needed. The challenge to validate these large datasets is that the data needs to be validated in memory. You will need multiple machines to scale data validation for large datasets.\n\nTensorFlow Data Validation (TFDV) can be used to understand, validate, and monitor ML data at scale (see Figure 2.5).\n\nTFDV is used for detecting data anomalies and schema anomalies in the data. It is a part of the _TensorFlow Extended (TFX)_ platform and provides libraries for data validation and schema validation for large datasets in an ML pipeline. The key TFX libraries are `TensorFlow Data Validation`, `TensorFlow Transform`, used for data preprocessing and feature engineering, `TensorFlow Model Analysis` for ML model evaluation and analysis, and `TensorFlow Serving` for serving ML models.\n\n**FIGURE 2.5** TensorFlow Data Validation\n\nYou can use TFDV in these ways:\n\n  * **Exploratory Data Analysis Phase** You can use TFDV to produce a data schema to understand the data for your ML pipeline. This schema can act as a defined contract between your ML pipeline and data. Whenever your schema is violated, you need to fix either your data or the pipeline.\n  * **Production Pipeline Phase** After your model is deployed, this schema can be used to define a baseline to detect any new data causing skew or drift in the model during training and serving.",
            "subsections": {},
            "summary": "* **TensorFlow Data Validation (TFDV)**\n    * Helps validate large ML datasets in memory\n    * Detects data anomalies and schema anomalies\n    * Part of the _TensorFlow Extended (TFX)_ platform \n        * Provides libraries for data validation, schema validation, and more"
          }
        },
        "summary": "* **Defining a Data Schema**\n  * A schema describes the property of your data, including data type, range, format, and distribution\n  * A schema is an output of the data analysis process, providing a consistent and reproducible check for data quality issues\n    * _Advantages:_\n      * Enables metadata-driven preprocessing for feature engineering and data transformation\n      * Validates new data and catches anomalies like skews and outliers during training and prediction"
      },
      "Running TFDV on Google Cloud Platform": {
        "content": "TFDV core application programming interfaces (APIs) are built on the Apache Beam open source software development kit (SDK) for building batch and streaming pipelines. Dataflow is a managed service that runs Apache Beam data processing pipelines at scale.\n\nDataflow integrates natively with the data warehousing serverless service BigQuery and data lakes (Google Cloud Storage) as well as `Vertex AI Pipelines` machine learning.",
        "subsections": {},
        "summary": "* **TFDV Core**: Built on Apache Beam SDK for batch & streaming pipelines\n* *Talent is not required, Dataflow runs pipelines at scale using Apache Beam*\n* *Integrates with BigQuery, Google Cloud Storage and Vertex AI Pipelines*"
      },
      "Organizing and Optimizing Training Datasets": {
        "content": "You learned in the previous section how to check for data quality. In this section, we will talk about data sampling, imbalanced data, and how to split your dataset. We generally divide the data into training, test, and validation dataset (see Figure 2.6).\n\n**FIGURE 2.6** Dataset representation\n\n  * **Training Dataset** This is the actual dataset that we use to train the model, and our model learns from this data.\n  * **Validation Dataset** This is a subset of data that is used for the evaluation of the model. This data is used for hyperparameter tuning. The model does not use this data to learn from it, unlike training data. It is used for improving the model behavior after training.\n  * **Test Dataset** This is the sample of data used to test or evaluate the performance of your model. The test set is different from the training and validation sets as it's used after model training and model validation. The test set should not contain the data samples in the validation or training set because it might cause data leakage. Also, you should never train your data using the test set.",
        "subsections": {
          "Imbalanced Data": {
            "content": "When two classes in a dataset are not equal, the result is imbalanced data. In the example shown in Figure 2.7, there is less chance of credit card fraud in the dataset compared to No fraud.\n\nIn the case of credit card transactions, suppose, out of all transactions, 1,000 are not fraud examples and only five are fraud transactions. This is a classic representation of imbalanced data. In this scenario, we do not have enough transactions to train the model to classify whether a credit card transaction is fraud. The training model will spend more time on no‐fraud scenarios.\n\nIn a random sampling, you can perform either _oversampling_ , which means duplicating samples from the minority class, or _undersampling_ , which means deleting samples from the majority class. Both of these approaches include bias because they introduce either more samples or fewer samples to remove imbalance.\n\n**FIGURE 2.7** Credit card data representation\n\n* * *\n\nAn effective way to handle imbalanced data is to downsample and upweight the majority class.\n\n* * *\n\nLet's now consider the previous example and downsample the majority class, which is no‐fraud examples. Downsampling by 10 will improve the balance (see Figure 2.8).\n\n**FIGURE 2.8** Downsampling credit card data\n\nNow let's upweight the downsampled class. Since we downsampled by a factor of 10, the example weight should be 10. _Example weight_ means counting an individual example as more important during training. An example weight of 10 means the model treats the example as 10 times as important (when computing loss) as it would for an example of weight 1. Refer to `https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data` to learn more.\n\nOne of the advantages of downsampling and upweighting the downsampled class is faster model convergence because we have more of the minority class examples compared to the data before downsampling and upweighting.",
            "subsections": {},
            "summary": "* **Imbalanced Data**: When two classes are unequal, it results in imbalanced data.\n    * _Causes problems_ for training models\n        * May spend too much time on majority class\n    * Downsampled minority class can improve balance\n        * Example: downsampling by 10 improves balance"
          },
          "Data Splitting": {
            "content": "Mostly with general data cleaning, you would start with random splitting of the data. For example, consider datasets having naturally clustered examples.\n\nSay you want your model to classify the topics in the text of a book. The topics can be horror, love story, and drama. A random split would be a problem in that case.\n\nWhy would having a random split cause a skew? It can cause a skew because stories with the same type of topic are written on the same timeline. If the data is split randomly, the test set and training set might contain the same stories.\n\nTo fix this, try splitting the data based on the time the story was published. For example, you can put stories written in June in the training set and stories published in July in the test set.\n\nAnother simple approach to fixing this problem would be to split the data based on when the story was published. For example, you could train on stories for the month of April and then use the second week of May as the test set to prevent overlap.",
            "subsections": {},
            "summary": "* **Data Preprocessing**: Start with random splitting of data\n*   *Avoids skew due to naturally clustered examples*\n*   However, it may cause skew when data points share similar characteristics (e.g., same topic)\n*   Example: Training and testing sets might contain the same stories with the same topic"
          },
          "Data Splitting Strategy for Online Systems": {
            "content": "For online systems, it's recommended to split the data by time as the training data is older than the serving data. By splitting data by time, you ensure your validation set mirrors the lag of time between training and prediction.\n\nHere are the steps:\n\n  1. Perform data collection for 30 days.\n  2. Create a training set for data from day 1 to day 29.\n  3. Create a validation set for data for day 30.\n\nGenerally, time‐based splits work best with very large datasets, such as, for example, data with millions of examples.\n\nWhen it is clustered data—for example, a book or an online system—you have to make sure that you split the data in a way that your model will not get trained using information not available at prediction time. That's why you should use a time‐based approach rather than a random split. For data splitting, you should always use domain knowledge to understand when a random split will work versus when to do a time‐based split.",
            "subsections": {},
            "summary": "* **Splitting Data for Time-Based Models**\n  * Split data by time to mirror the lag between training and prediction\n  * Use a time-based approach with large datasets (e.g., millions of examples)\n  * Avoid random splits; use domain knowledge instead"
          }
        },
        "summary": "* *_Key Components of a Dataset_* \n  * **Training Dataset**: used to train the model\n  * **Validation Dataset**: used for hyperparameter tuning and evaluation\n  * **Test Dataset**: used to evaluate the performance of the trained model"
      },
      "Handling Missing Data": {
        "content": "Why do we have missing data? In real‐world scenarios, you would have missing data as a result of failure to record data, or it could be due to data corruption. Some examples of missing data in structured data can be NaN values or a null value for a given feature. So, to train your model effectively, you would need to think of ways to handle missing data.\n\nThe following list includes some of the ways to handle missing data:\n\n  * Delete the rows or columns with missing values such as null or NaN. If a column has more than half of the rows as null, then the entire column can be dropped. The rows that have one or more column values as null can also be dropped. The disadvantage of this approach is that information is lost and the model will perform poorly if a large percentage of data is missing in a complete dataset.\n  * Replace missing values for numeric continuous columns or features such as age or interest rate in your dataset having missing values. You can replace missing values with the mean, median, or mode of remaining values in the column. This method can prevent the loss of data compared to removing or deleting the columns or rows. This method works well with small datasets. Also, this method does not factor in the covariance between features and can cause data leakage, which we will discuss in the next section.\n  * For missing values in categorical columns (string or numerical), the missing values can be replaced with the most frequent category. This method is called the imputation method. If the number of missing values is very large, then missing values can be replaced with a new category. The disadvantage of this method is encoding the extra added features while one‐hot encoding.\n  * You can use the last valid observation to fill in the missing value. This is known as the _last observation carried forward (LOCF)_ method. This method can reduce bias.\n  * For the time‐series dataset, you can use the interpolation of the variable before and after a time stamp for a missing value.\n  * ML algorithms that can ignore missing values in data can also be used. For example, the k‐nearest neighbors (k‐NN) algorithm can ignore a column from a distance measure when a value is missing. Naive Bayes can also support missing values when making a prediction.\n\nAlso, the Random Forest algorithm works well on nonlinear and categorical data. It adapts to the data structure, taking into consideration the high variance or the bias, producing better results on large datasets.\n\n  * Use machine learning to predict missing values. The correlation between the variable containing the missing value and other variables can be used to predict missing values. The regression or classification model can be used for the prediction of missing values based on non‐missing variables.",
        "subsections": {},
        "summary": "* **Handling Missing Data**: \n    * Deleting rows/columns with missing values, or replacing with mean/median/mode.\n    * Imputing categorical values with most frequent category, using _last observation carried forward (LOCF)_ method for time-series datasets.\n    * Using machine learning algorithms that can ignore or predict missing values."
      },
      "Data Leakage": {
        "content": "Data leakage happens when you expose your machine learning model to the test data during training. As a result, your model performs great during training and testing, but when you expose the model to unseen data, it underperforms. Data leakage leads to overfitting in the model as the model has already learned from the test and training data.\n\nThe following are some of the reasons for data leakage:\n\n  * The target variable is the output that your model is trying to predict, and features are the data that is fed into the model to make a prediction or predict the target variable. The cause of data leakage is that by mistake you have added your target variable as your feature.\n  * While splitting the test data and the training data for model training, you have included the test data with the training data.\n  * The presence of features that expose the information about the target variable will not be available after the model is deployed. This is also called label leakage and can be detected by checking the correlation between the target variable and the feature.\n  * Applying preprocessing techniques (normalizing features, removing outliers, etc.) to the entire dataset will cause the model to learn not only the training set but also the test set, which leads to data leakage.\n\nA classic example of data leakage is time‐series data. For example, when dealing with time‐series data, if we use data from the future when doing computations for current features or predictions, we would highly likely end up with a leaked model. It generally happens when the data is randomly split into train and test subsets.\n\nThese are the situations where you might have data leakage:\n\n  * If the model's predicted output is as good as actual output, it might be because of a data leakage. This means the model might be somehow memorizing the data or might have been exposed to the actual data.\n  * While doing the exploratory data analysis, having features that are very highly correlated with the target variable might be a data leakage.\n\nData leakage happens primarily because of the _way_ we split our data and _when_ we split our data. Now, let's understand how to prevent data leakage:\n\n  * Select features that are not correlated with a given target variable or that don't contain information about the target variable.\n  * Split the data into test, train, and validation sets. The purpose of the validation set is to mimic the real‐life scenario and it will help identify any possible case of overfitting.\n  * Preprocess the training and test data separately. You would perform normalization on training data rather than on the complete dataset to avoid any leakage.\n  * In case of time‐series data, have a cutoff value on time as it prevents you from getting any information after the time of prediction.\n  * Cross‐validation is another approach to avoid data leakage when you have limited data. However, if data leakage still happens, then scale or normalize the data and compute the parameters on each fold of cross‐validation separately.\n\nFurthermore, the difference in production data versus training data must be reflected in the difference between the validation data split and the training data split and between the testing data split and the validation data split.\n\nFor example, if you are planning on making predictions about user lifetime value (LTV) over the next 30 days, then make sure that the data in your validation data split is from 30 days after the data in your training data split and that the data in your testing data split is from 30 days before your validation data split.",
        "subsections": {},
        "summary": "* **Data Leakage**: occurs when model exposed to test data during training, leading to overfitting\n* *Causes of Data Leakage*: \n  * Incorrectly adding target variable as feature\n  * Including test data in training data\n  * Applying preprocessing techniques to entire dataset\n* **Prevention Measures**:\n  * Selecting uncorrelated features with the target variable\n  * Splitting data into test, train, and validation sets\n  * Preprocessing training and test data separately"
      },
      "Summary": {
        "content": "In this chapter, we discussed why we need to visualize data and the various ways to visualize data, such as using box plots, line plots, and scatterplots. Then we covered statistical fundamentals such as mean, median, mode, and standard deviation and why they are relevant when finding outliers in data. Also, you learned how to check data correlation using a line plot.\n\nYou learned about various data cleaning and normalizing techniques such as log scaling, scaling, clipping, and using a z‐score to improve the quality of data.\n\nWe also discussed establishing data constraints and why it's important to define a data schema in an ML pipeline and the need to validate data. We covered using TFDV for validating data at scale and why you need TFDV to validate data schema for large‐scale deep learning systems. Then we discussed the strategy used for splitting the data and spoke about the data splitting strategy for an imbalanced dataset. We covered splitting based on time for online systems and clustered data.\n\nLast, we covered strategies for how to deal with missing data and data leakage.",
        "subsections": {},
        "summary": "* **Data Visualization**: Visualizing data using box plots, line plots, and scatterplots to understand data distribution.\n    * *_Key statistics_*: Mean, median, mode, and standard deviation to identify outliers.\n* **Data Preprocessing**\n    * *_Scaling techniques_*: Log scaling, scaling, clipping, z-score to improve data quality\n    * *_Data validation_*: Establishing data constraints and defining a data schema for ML pipelines."
      },
      "Exam Essentials": {
        "content": "* **Be able to visualize data.** Understand why we need to visualize data and various ways to do so, such as using box plots, line plots, and scatterplots.\n  * **Understand the fundamentals of statistical terms.** Be able to describe mean, median, mode, and standard deviation and how they are relevant in finding outliers in data. Also know how to check data correlation using a line plot.\n  * **Determine data quality and reliability or feasibility.** Understand why you want data without outliers and what data skew is, and learn about various data cleaning and normalizing techniques such as log scaling, scaling, clipping, and z‐score.\n  * **Establish data constraints.** Understand why it's important to define a data schema in an ML pipeline and the need to validate data. Also, you need to understand TFDV for validating data at scale.\n  * **Organize and optimize training data.** You need to understand how to split your dataset into training data, test data, and validation data and how to apply the data splitting technique when you have clustered and online data. Also understand the sampling strategy when you have imbalanced data.\n  * **Handle missing data.** Know the various ways to handle missing data, such as removing missing values; replacing missing values with mean, median, or mode; or using ML to create missing values.\n  * **Avoid data leaks.** Know the various ways data leakage and label leakage can happen in the data and how to avoid it.",
        "subsections": {},
        "summary": "* _Data Visualization:_ Understanding box plots, line plots, scatterplots, and statistical terms like mean, median, mode, standard deviation, and correlation to visualize and analyze data.\n  * **Statistics Fundamentals:** Describing outliers, skewness, and data cleaning techniques to ensure accurate data representation.\n  * _Data Quality and Constraints:_ Validating data, establishing a schema, and applying data normalization and scaling techniques to define data constraints.\n\n* _Data Preprocessing:_\n  * **Data Splitting:** Dividing datasets into training, test, and validation sets for optimal model performance.\n  * **Handling Imbalanced Data:** Applying sampling strategies to balance imbalanced datasets.\n  * **Missing Data Handling:** Removing, replacing, or generating missing values using various techniques."
      },
      "Review Questions": {
        "content": "1. You are the data scientist for your company. You have a dataset that includes credit card transactions, and 1 percent of those credit card transactions are fraudulent. Which data transformation strategy would likely improve the performance of your classification model?\n     1. Write your data in TFRecords.\n     2. Z‐normalize all the numeric features.\n     3. Use one‐hot encoding on all categorical features.\n     4. Oversample the fraudulent transactions.\n  2. You are a research scientist building a cancer prediction model from medical records. Features of the model are patient name, hospital name, age, vitals, and test results. This model performed really well on held‐out test data but performed poorly on new patient data. What is the reason for this?\n     1. Strong correlation between feature hospital name and predicted result.\n     2. Random splitting of data between all the features available.\n     3. Missing values in the feature hospital name and age.\n     4. Negative correlation between the feature hospital name and age.\n  3. Your team trained and tested a deep neural network model with 99 percent accuracy. Six months after model deployment, the model is performing poorly due to change in input data distribution. How should you address input data distribution?\n     1. Create alerts to monitor for skew and retrain your model.\n     2. Perform feature selection and retrain the model.\n     3. Retrain the model after hyperparameter tuning.\n     4. Retrain your model monthly to detect data skew.\n  4. You are an ML engineer who builds and manages a production system to predict sales. Model accuracy is important as the production model has to keep up with market changes. After a month in production, the model did not change but the model accuracy was reduced. What is the most likely cause of the reduction in model accuracy?\n     1. Accuracy dropped due to poor quality data.\n     2. Lack of model retraining.\n     3. Incorrect data split ratio in validation, test, and training data.\n     4. Missing data for training.\n  5. You are a data scientist in a manufacturing firm. You have been asked to investigate failure of a production line based on sensor readings. You realize that 1 percent of the data samples are positive examples of a faulty sensor reading. How will you resolve the class imbalance problem?\n     1. Generate 10 percent positive examples using class distribution.\n     2. Downsample the majority data with upweighting to create 10 percent samples.\n     3. Delete negative examples until positive and negative examples are equal.\n     4. Use a convolutional neural network with the softmax activation function.\n  6. You are the data scientist of a meteorological department asked to build a model to predict daily temperatures. You split the data randomly and then transform the training and test datasets. Temperature data for model training is uploaded hourly. During testing, your model performed with 99 percent accuracy; however, in production, accuracy dropped to 70 percent. How can you improve the accuracy of your model in production?\n     1. Split the training and test data based on time rather than a random split to avoid leakage.\n     2. Normalize the data for the training and test datasets as two separate steps.\n     3. Add more data to your dataset so that you have fair distribution.\n     4. Transform data before splitting, and cross‐validate to make sure the transformations are applied to both the training and test sets.\n  7. You are working on a neural‐network‐based project. The dataset provided to you has columns with different ranges and a lot of missing values. While preparing the data for model training, you discover that gradient optimization is having difficulty moving weights. What should you do?\n     1. Use feature construction to combine the strongest features.\n     2. Use the normalization technique to transform data.\n     3. Improve the data cleaning step by removing features with missing values.\n     4. Change the hyperparameter tuning steps to reduce the dimension of the test set and have a larger training set.\n  8. You are an ML engineer working to set a model in production. Your model performs well with training data. However, the model performance degrades in production environment and your model is overfitting. What can be the reason for this? (Choose three.)\n     1. Applying normalizing features such as removing outliers to the entire dataset\n     2. High correlation between the target variable and the feature\n     3. Removing features with missing values\n     4. Adding your target variable as your feature",
        "subsections": {},
        "summary": "* *Improving performance in classification models with imbalanced data*: \n    * Oversample the minority class (fraudulent transactions).\n    * Use cost-sensitive learning or class weights.\n    * Use ensemble methods such as bagging or boosting.\n\n* *Poor model performance on new patient data*: \n    * Strong correlation between hospital name and predicted result.\n    * Missing values in hospital name and age features.\n    * Poor feature selection or engineering.\n\n* *Model underperforming due to change in input distribution*: \n    * Retrain the model after monitoring skewness and hyperparameter tuning.\n    * Implement online learning or incremental retraining.\n    * Use domain adaptation techniques.\n\n* *Model accuracy reduction over time*: \n    * Lack of model retraining or updates with new data.\n    * Incorrect data split ratio in validation, test, and training data.\n    * Poor quality data used for initial training.\n\n* *Class imbalance problem in sensor reading data*: \n    * Generate synthetic examples to balance the classes.\n    * Downsample the majority class using upweighting.\n    * Use oversampling techniques such as SMOTE or ADASYN."
      }
    },
    "summary": ""
  },
  "Chapter 3Feature Engineering": {
    "content": "",
    "subsections": {
      "Consistent Data Preprocessing": {
        "content": "Consistent data preprocessing is needed as you can apply transformations while generating the data either on disk or within the model. Let's discuss the approaches:\n\n  * **Pretraining Data Transformation** This means data transformation is performed before model training on a complete dataset. Your transformation code lives separate from your machine learning model. Advantages of this approach is that computation is performed only once and it can look at the entire dataset to determine the transform.\n\nSome of the disadvantages of this approach are that the same transformation needs to be reproduced at prediction time, and if data changes at prediction, it can lead to skew, especially in the case of online serving or prediction. Another challenge is updating the transformation. If you want to update the data transformation, you would have to rerun the transform on the entire dataset, leading to slow iterations and more compute time.\n\n  * **Inside Model Data Transformation** In this, the transformation is part of the model code as the model takes in untransformed or raw data as input and transforms it within the model before training. One of the advantages of this approach is that it is easy to decouple your data and transformation. Even if your transformation technique changes, you can still use the same data. You are also using the same transformation during training and serving since it's part of the model. A disadvantage of this method is that if the transform is large or computation heavy, it can increase model latency. Transformations are done by data batches, which can be a problem if you are trying to normalize the data by setting mean values with batches of data and not entire datasets.\n\nThis can be solved by _tf.Transform_ , which we will cover in the section “TensorFlow Transform” later in this chapter.",
        "subsections": {},
        "summary": "* **Data Transformation Approaches**\n  * Pretraining Data Transformation: Transformation performed before model training, advantages include one-time computation and dataset analysis.\n    * Disadvantages include reproduction at prediction time and updating challenges.\n  * Inside Model Data Transformation: Transformation within the model code, decoupling data and transformation is easy, but can increase model latency.\n    * Example solution using _tf.Transform_ ."
      },
      "Encoding Structured Data Types": {
        "content": "A good feature should be related to business objectives, be known at prediction time, be numeric with magnitude, and have enough examples. Let's look at various types of data in feature engineering that we are going to use throughout this chapter.\n\n  * **Categorical Data** This data type defines the category, and the data takes only a number of values—for example, yes/no type of data in your data column, male/female as values in the gender category, and so on.\n  * **Numeric Data** This represents data in the form of scalar value or continuous data such as observations, recordings, and measurements.",
        "subsections": {
          "Why Transform Categorical Data?": {
            "content": "Some algorithms can work with categorical data directly, such as, for example, decision trees. However, most ML algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric, which is why categorical data must be converted to numeric data. If the categorical variable is an output variable, you would have to convert the numeric output back to categorical data during predictions.\n\n* * *",
            "subsections": {},
            "summary": "*_Categorical Data Limitation_* \n\n* Most ML algorithms require numeric input and output variables.\n* Categorical data must be converted to numeric data for use in these algorithms."
          },
          "Mapping Numeric Values": {
            "content": "Integer and floating‐point data don't need special encoding because they can be multiplied by a numeric weight. You may need to apply two kinds of transformations to numeric data: normalizing and bucketing.",
            "subsections": {
              "Normalizing": {
                "content": "We covered normalization techniques such as scaling, log scaling, z score, and clipping in the Data Cleaning section of Chapter 2, “Exploring Data and Building Data Pipelines.” You would perform normalization in two cases with numeric data:\n\n  * **Numeric features that have distinctly different ranges (for example, age and income):** In this case, the model gradient descent can slow down convergence due to various data ranges. That is why AdaGrad and `Adam` optimization techniques can help, because they create a separate learning rate for each feature.\n  * **Numeric features that cover a wide range such as a city:** This type of dataset model will generate a NaN data error if it is not normalized. In this situation, even optimizers such as Adam and AdaGrad can't prevent NaN errors when there is a wide range of values in a single feature.",
                "subsections": {},
                "summary": "* Normalization techniques are used to handle **differently scaled numeric features**, improving model convergence\n    * Techniques like **AdaGrad** and **Adam** help by creating separate learning rates for each feature\n        * This prevents data ranges from slowing down gradient descent"
              },
              "Bucketing": {
                "content": "Bucketing is transforming numeric data to categorical data. For example, latitude can be a floating‐point value, and it's difficult to represent it while predicting house price with respect to location. Two ways to do bucketing are as follows:\n\n  * **Creating buckets with equal‐spaced boundaries:** You create a range of buckets and some buckets might have more data points compared to others. For example, to represent rainfall, you can bucket by range (0–50 cm, 50–100 cm); you might have more data points in the rainfall range 0–50 cm in an area with less intense rainfall.\n  * **Buckets with quantile boundaries:** Each bucket has the same number of points. The boundaries are not fixed and could encompass a narrow or wide span of values.\n\n* * *\n\nBucketing with equally spaced boundaries is an easy method that works for a lot of data distributions. For skewed data, however, try bucketing with quantile bucketing.\n\n* * *",
                "subsections": {},
                "summary": "**Transforming Numeric Data to Categorical**\n_bucketing simplifies numeric data into categories_\n\n* _Methods:_\n  * *_Equal-spaced boundaries:_* Divide range into discrete buckets (e.g., rainfall ranges)\n  * *_Quantile boundaries:_* Equal number of points in each bucket with flexible boundaries"
              }
            },
            "summary": "* *_Data Encoding_*: Integer and floating-point data do not require special encoding as it can be scaled using a numeric weight.\n* *Transformations for Numeric Data*: Two types of transformations are applied: **_normalization_** and **_bucketing_**."
          },
          "Mapping Categorical Values": {
            "content": "There are ways to convert or transform your categorical data into numerical data so that your model can understand it.",
            "subsections": {
              "Label Encoding or Integer Encoding": {
                "content": "In label encoding, you would use indexing using a vocabulary. If the number of categories of a data field is small, such as breeds of dog or days of the week, you can make a unique feature for each category and assign an integer value. For example, ratings can be assigned such as “satisfactory” is 1, “good” is 2, and “best” is 3. You can apply integer ratings only if the categories represent limited ranges or ordinal values.",
                "subsections": {},
                "summary": "* *_Label Encoding_* \n    * _Assigns unique integers to distinct categories_\n    * _Useful for small datasets with limited categories_"
              },
              "One‐Hot Encoding": {
                "content": "One‐hot encoding is the process of creating dummy variables. This technique is used for categorical variables where order does not matter. The one‐hot encoding technique is used when the features are nominal (do not have any order). Let's discuss what is an ordinal relationship in categorical data first; for example, ordinal variables can be socioeconomic status (low income, middle income, high income), education level (high school, BS, MS, PhD), income level (less than 50K, 50K–100K, over 100K), satisfaction rating (extremely dislike, dislike, neutral, like, extremely like). For categorical variables where no such ordinal relationship exists, such integer encoding is not enough.\n\nIn one‐hot encoding, for every categorical feature, a new variable is created. It is a process by which categorical variables are converted into binary representation for the integer encoding or integer values. Let's look at the example of two colors, as shown in Table 3.1. Each can be represented by binary values.\n\n**TABLE 3.1** One‐hot encoding example\n\nCategorical Value | Integer Encoding or Creating a Vocabulary Mapping | One‐Hot Encoding\n---|---|---\nRed | 0 | 00\nBlue | 1 | 01\n\nSometimes you have large binary spaces to represent, which might lead to sparse representation with lots of 0s.",
                "subsections": {},
                "summary": "* _**One-hot encoding is a technique for converting categorical variables into binary representation**_\n    * _It creates new variables for each unique value in the categorical feature, representing it as a binary vector._\n    * _Used when the features are nominal and no ordinal relationship exists, to convert categorical data into a numerical format._"
              },
              "Out of Vocab (OOV)": {
                "content": "For situations where categorical data contains outliers, you can create a single category called _out of vocabulary_ for all the outliers rather than giving them unique representation because they occur very rarely in a dataset. When you use this approach, the ML system won't waste time training on each of those rare outliers.",
                "subsections": {},
                "summary": "* **Handling Outliers**: Create a single category `_out of vocabulary_` to group rare outliers together, reducing unnecessary training on individual outliers.\n \n*  Using this approach simplifies model training and improves performance.\n* This technique is often used in situations where outliers occur very infrequently in the dataset."
              },
              "Feature Hashing": {
                "content": "Hashing works by applying a `hash function` to the categorical features and using their hash values as indices directly rather than looking into vocabulary. Hashing often causes collisions, and that is why for important terms, hashing can be worse than selecting a vocabulary. The advantage of hashing is that it doesn't require you to assemble a vocabulary in case the feature distribution changes heavily over time.",
                "subsections": {},
                "summary": "* **Hashing**: applies a hash function to categorical features and uses their hash values as indices\n* _Advantage_: no need to reassemble vocabulary if feature distribution changes\n* **Collision problem**: hashing can cause inconsistencies (collisions) for important terms"
              },
              "Hybrid of Hashing and Vocabulary": {
                "content": "In this approach you can use a vocabulary for the most important categories in your data and replace the out‐of‐bucket vocabulary categories with categories by hashing. The advantage of this approach is that each out‐of‐box category is represented with hashing and the model still learns the categories outside your vocabulary.",
                "subsections": {},
                "summary": "* *Hashing-based approach to represent categories outside of pre-defined vocabulary*\n    * Reduces complexity by mapping non-vocabulary categories to hash values\n    * Enables model to learn categories not included in the predefined vocabulary"
              },
              "Embedding": {
                "content": "Embedding is a categorical feature represented as a continuous‐valued feature. Deep learning models frequently convert the indices from an index to an embedding. Mostly, embeddings are used for text classification or document classification where you have a bag of words or a document of words.",
                "subsections": {},
                "summary": "* **What is Embedding**: \n    * _Continuous-valued feature_ that represents categories\n    * Often created by converting index values into a numerical vector"
              }
            },
            "summary": "* *_Data Conversion Techniques_* \n    * One-hot encoding\n    * Label encoding\n    * Binary encoding"
          },
          "Feature Selection": {
            "content": "_Feature selection_ means selecting a subset of features or reducing the number of input variables that are most useful to a model in order to predict the target variable.\n\n_Dimensionality reduction_ is one of the most popular techniques for reducing the number of features. The advantage of this is that it reduces the noise from data and overfitting problems. A lower number of dimensions in data means less training time and computational resources and increases the overall performance of machine learning algorithms. There are two ways this can be accomplished:\n\n  * Keep the most important features only: some of the techniques used are backward selection and Random Forest.\n  * Find a combination of new features: some of the techniques used are principal component analysis (PCA) and t‐distributed stochastic neighbor embedding (t‐SNE).",
            "subsections": {},
            "summary": "* **Feature Selection** selects a subset of useful input variables to predict a target variable, reducing noise and overfitting issues.\n    * Can be done using:\n        * Keeping only the most important features\n        * Finding a combination of new features\n        * _Techniques include: backward selection, Random Forest, PCA, t-SNE_"
          }
        },
        "summary": "* *_Feature engineering key characteristics_*\n\t+ Related to business objectives\n\t+ Known at prediction time\n\t+ Numeric with magnitude\n\t+ Enough examples\n* *_Types of data used in feature engineering_*\n\t+ **Categorical Data**: defines category with limited values (e.g. yes/no, male/female)\n\t+ **Numeric Data**: represents scalar value or continuous data (e.g. observations, measurements)"
      },
      "Class Imbalance": {
        "content": "We covered data class imbalance in Chapter 2 and in this section, we will talk about class imbalance specific to classification models.\n\nClassification models rely on some key outcomes we covered in Chapter 1, “Framing ML”:\n\n  * **A true positive** is an outcome when the model correctly predicts the positive class; for example, the patients who took tests were actually sick with the virus.\n  * **True negative** is an outcome when the model correctly predicts the negative class, which means the patient who took the test has negative results and they actually are not sick.\n  * A **false positive** is an outcome where the model incorrectly predicts the positive class, meaning the patients were actually not sick but the test predicted them as sick.\n  * **False negative** is an outcome where the model incorrectly predicts the negative class, meaning the patients were sick but the test determined them as not sick.\n\nIn this scenario, false negatives are a problem because you do not want sick patients identified as not sick. So you would work on minimizing the false negatives outcome in your classification model.",
        "subsections": {
          "Classification Threshold with Precision and Recall": {
            "content": "In order to map a logistic regression value to a binary category, you define a ****************classification********************************threshold**************** (also called the ****************decision threshold****************). It is a value that a human chooses, not a value chosen by model training and usually for most cases this threshold is assumed to be 0.5. A logistic regression model outputs a value between 0 and 1. Suppose the classification threshold is 0.6 set by the human and the value predicted by the model is 0.7, then the model predicts the positive class. This choice of classification threshold strongly influences the number of false positives and false negatives. Mostly, classification thresholds are problem‐specific and must be fine‐tuned. Precision Recall curve indicates how well a model predicts the positive class. Precision refers to the number of true positives divided by the total number of positive prediction while recall also known as the true positive rate (TPR). Recall is is the percentage of data samples that a machine learning model correctly identifies as belonging to a class of interest which is the “positive class” out of the total samples for that class.\n\nBoth precision and recall are important to evaluate a model. However, if you improve precision, it will reduce recall and vice versa.\n\nIf you want to minimize false positives, then raise the classification threshold. For these problems, precision is really important to you. For example, if you would rather have your model classify a spam email as not spam than the other way around. When you care more about positives than negatives, precision should be used.\n\n* * *\n\nIf you raise the classification threshold, it will reduce false positives, thus raising precision.\n\n* * *\n\nIf you want to minimize false negatives, then lower the classification threshold. For these problems, recall is really important to you. In other words, you prefer that your model classify a non‐sick patient as sick but it will be a problem if a sick patient is identified as non‐sick. In such scenarios, if you care more about your negatives than positives, recall should be used.\n\nRecall answers the question, out of all possible positives, how many did the model correctly identify ?\n\n* * *\n\nIf you decrease or lower the classification threshold, it will reduce false negatives, thus raising recall.\n\n* * *",
            "subsections": {},
            "summary": "**Classification Threshold**\nA **classification threshold** is a chosen value that maps logistic regression output to a binary category. It's usually set between 0 and 1.\n\n* *_Increasing threshold_* reduces false positives, increasing _precision_, but decreasing recall.\n* *_Decreasing threshold_* increases false negatives, increasing recall, but decreasing precision.\n* *_Precision_ vs Recall_: improving one decreases the other_."
          },
          "Area under the Curve (AUC)": {
            "content": "In this section, we will cover two types of areas under the curve with classification problems.\n\n  * **AUC ROC** This is used for a balanced dataset in classification problems that have an equal number of examples for both the classes we are trying to predict.\n  * **AUC PR** This is used when a dataset in classification problems is imbalanced, such as, for example, credit card transactions to identify fraud or not fraud. Suppose you have 10,000 credit card transactions, the data for a fraud would be 1 in 1,000 such transactions.",
            "subsections": {
              "AUC ROC": {
                "content": "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: true positive rate and false positive rate. You can refer to this link to see what the graph looks like:\n\n`https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc`\n\nAUC ROC (area under the ROC curve) measures the two‐dimensional area underneath the entire ROC curve. It refers to a number between 0.0 and 1.0 representing a binary classification model's ability to separate positive classes from negative classes.\n\nThe closer the AUC is to 1.0, the better the model's ability to separate classes from each other. AUC ROC curves are used when the class is balanced or when you want to give equal weight to both classes (negative and positive class) prediction ability.",
                "subsections": {},
                "summary": "* **ROC Curve**: a graph showing classification model performance at all thresholds\n    * Plots true positive rate vs false positive rate\n* **AUC ROC**: measures 2D area under the curve, between 0.0 and 1.0\n    * Represents binary classification model's ability to separate classes"
              },
              "AUC PR": {
                "content": "A PR curve is a graph with Precision values on the y‐axis and Recall values on the x‐axis. The focus of the PR curve on the minority class makes it an effective diagnostic for imbalanced binary classification models.\n\nThe area under the precision‐recall (AUC PR) curve measures the two‐dimensional area underneath the precision‐recall (PR) curve. In case of an imbalanced class, precision‐recall curves (PR curves) are recommended for highly skewed domains. AUC PR gives more attention to the minority class. It can be used in conjunction with downsampling or upsampling, which we covered in Chapter 2.",
                "subsections": {},
                "summary": "* **Precision-Recall Curve**: a graph showing Precision values on the y-axis and Recall values on the x-axis\n    * _Effective for imbalanced binary classification models_, as it focuses on the minority class.\n    * The area under the curve (AUC PR) measures the 2D area underneath, providing more attention to the minority class."
              }
            },
            "summary": "* **Classification Metrics**: Two types of area under the curve used in classification problems\n    * *_AUC-ROC_*: balanced dataset with equal numbers of examples per class\n    * *_AUC-PR_*: imbalanced dataset where classes have unequal numbers of examples"
          }
        },
        "summary": "* **Classification Model Outcomes**\n  * _True Positive_: Correctly predicts positive class (e.g., patient is actually sick)\n  * True Negative: Correctly predicts negative class\n  * False Positive: Incorrectly predicts positive class (e.g., patient is not sick but test says so)\n  * **False Negative**: Incorrectly predicts negative class (patient is sick but test determines not so)"
      },
      "Feature Crosses": {
        "content": "A _feature cross_ , or synthetic feature, is created by multiplying (crossing) two or more features. It can be multiplying the same feature by itself [A * A] or it can be multiplying values of multiple features, such as [A * B * C]. In machine learning, feature crosses are usually performed on one‐hot encoded features—for example, binned_latitude × binned_longitude. (For more information, see `https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture`.)\n\nThere are two ways to use feature cross:\n\n  * A feature cross is used when a single feature on its own has less predictive ability compared to combined features. For example, we have a model that needs to predict how crowded the street‐based location of people is at a certain time of the day. If we build a feature cross from both these features, [location (market, curbside) × time of the day], then we'll end up with vastly more predictive ability than either feature on its own. For example, if there is a farmers market on Tuesday evening, it will be more crowded. If there is no farmers market on Tuesday evening, it will be less crowded.\n  * In linear problems with highly complex models, a feature cross is used to represent nonlinearity in a linear model by multiplying (crossing) two or more features. Linear learners scale well to massive data. Using feature crosses on massive datasets is one efficient strategy for learning highly complex models. Let's look at an example. As shown in Figure 3.1, a dot represents sick people, and a rectangle represents healthy people. It is difficult to separate by line or by a linear method.\n\n**FIGURE 3.1** Difficult to separate by line or a linear method\n\nThis is a linear problem, but seeing the data distribution of these variables, it's hard to separate the dots from the rectangles by using a straight line, or it's hard to classify these by using a linear method, as shown in Figure 3.2. It's difficult to separate classes by line.\n\n**FIGURE 3.2** Difficult to separate classes by line\n\nThat is why we will cross these features, dot versus rectangle, to create a feature cross—for example, AB = A * B. Please refer to this link to understand the concept of encoding nonlinearity: `https://developers.google.com/machine-learning/crash-course/feature-crosses/encoding-nonlinearity`.\n\nFigure 3.3 summarizes all the feature engineering techniques for converting numerical and categorical data we covered so far.\n\n**FIGURE 3.3** Summary of feature columnsGoogle Cloud via Coursera, www.coursera.org/professional‐certificates/preparing‐for‐google‐cloud‐machine‐learning‐engineer‐professional‐certificate",
        "subsections": {},
        "summary": "* _Feature Cross_: A synthetic feature created by multiplying two or more features.\n  * Can be used to increase predictive ability when individual features are not sufficient\n    * Example: combining location (market, curbside) and time of day\n  * Used to represent nonlinearity in linear models with complex data\n    * Example: creating a feature cross from dot and rectangle features to learn separation classes"
      },
      "TensorFlow Transform": {
        "content": "Increasing the performance of a TensorFlow model requires an efficient input pipeline. First we will discuss the TF Data API and then we'll talk about TensorFlow Transform.",
        "subsections": {
          "TensorFlow Data API (tf.data)": {
            "content": "An efficient data input pipeline can drastically improve the speed of your model execution by reducing device idle time. Consider incorporating the following best practices as detailed `here` to make your data input pipeline more efficient:\n\n  * Prefetch transformation to overlap preprocessing and model execution of a training step. This transformation decouples the time when data is produced to the time when data is consumed. The **_tf.data_ ** API provides the **_tf.data.Dataset.prefetch_ ** transformation.\n  * The **_tf.data.Dataset.interleave_ ** transformation parallelizes the data reading. By using the interleave transformation, you can mitigate the impact of the various data extraction overhead.\n  * Use the cache transformation to cache data in memory during the first epoch. The **_tf.data.Dataset.cache_ ** transformation can cache a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch.\n  * Vectorize user‐defined functions on a batch of datasets. The dataset is passed in to the map transformation.\n  * Reduce memory usage when applying the interleave, prefetch, and shuffle transformations.",
            "subsections": {},
            "summary": "* **Optimize Model Execution Speed**\n    * Use `tf.data.Dataset.prefetch` to overlap preprocessing and model execution\n    * Utilize `tf.data.Dataset.interleave` for parallel data reading and caching\n        * Apply `cache` transformation during the first epoch\n            * Reduces file opening and data reading operations"
          },
          "TensorFlow Transform": {
            "content": "The TensorFlow Transform library is part of TensorFlow Extended (TFX) and allows you to perform transformations prior to training the model and to emit a TensorFlow graph that reproduces these transformations during training. Using tf.Transform avoids the training‐serving skew. In Google Cloud, you can create transform pipelines using Cloud Dataflow. Some of the steps that TF Transform takes for transformations during training and serving are analyzing training data, transforming training data, transforming evaluation data, producing metadata, feeding the model, and serving data, as shown in Figure 3.4.\n\n**FIGURE 3.4** TensorFlow Transform\n\nSource: `Adapted from Google Cloud`\n\nYou can run the previous pipeline using Cloud Dataflow and BigQuery:\n\n  1. Read training data from BigQuery.\n  2. Analyze and transform training data using tf.Transform Cloud Dataflow.\n  3. Write transformed training data to Cloud Storage as TFRecords.\n  4. Read evaluation data from BigQuery.\n  5. Transform evaluation data using the transform_fn produced by step 2 in Cloud Dataflow.\n  6. Write transformed training data to Cloud Storage as TFRecords.\n  7. Write transformation artifacts to Cloud Storage for creating and exporting the model.\n\nTable 3.2 shows how you can run a TFX pipeline using Google Cloud Platform (GCP) services.\n\n**TABLE 3.2** Run a TFX pipeline on GCP\n\nStep | TFX library | GCP service\n---|---|---\nData extraction & validation | TensorFlow Data Validation | Cloud Dataflow\nData transformation | TensorFlow Transform | Cloud Dataflow\nModel training & tuning | TensorFlow (tf.Estimators and tf.Keras) | Vertex AI Training\nModel evaluation & validation | TensorFlow Model Analysis | Cloud Dataflow\nModel serving for prediction | TensorFlow Serving | Vertex AI Prediction",
            "subsections": {},
            "summary": "* **TensorFlow Transform**: allows transformations before training to avoid training-serving skew, creating a reproducible graph during training.\n    * Utilizes tf.Transform and Cloud Dataflow in Google Cloud.\n    * Includes steps such as data analysis, transformation, serving, and metadata production.\n* Pipeline components:\n    * **Data Ingestion & Validation**\n        * TensorFlow Data Validation\n        * Cloud Dataflow\n    * **Data Transformation**\n        * TensorFlow Transform\n        * Cloud Dataflow\n    * **Model Training & Tuning**\n        * TensorFlow Estimators/TF Keras\n        * Vertex AI Training"
          }
        },
        "summary": "* **Efficient Input Pipeline**: Key to increasing TensorFlow model performance\n* * _TF Data API_*: A unified interface for reading data from various sources, including files, databases, and APIs.\n* * _TensorFlow Transform_*: A library for data preprocessing and transformation, enabling efficient data processing and caching."
      },
      "GCP Data and ETL Tools": {
        "content": "We will cover the complete data analytics ecosystem in detail, from collecting to storing data, in Chapter 8, “Model Training and Hyperparameter Tuning.” Here are two tools that will help with data transformation and ETL in Google Cloud:\n\n  * **Cloud Data Fusion** is a code‐free web UI–based managed service that helps build and manage extract, transform, load (ETL) or extract, load, transform (ELT) pipelines from various data sources. The web UI helps you to clean and prepare data with no infrastructure management. It also supports Cloud Dataproc, which is another service to run and manage Hadoop and Spark workloads on Google Cloud. Using Dataproc as an execution environment, Cloud Data Fusion allows you to run MapReduce and Spark streaming pipelines. Refer to this link to learn more: `https://cloud.google.com/data-fusion#section-9`.\n  * **Dataprep by Trifacta** (`https://cloud.google.com/dataprep`) is a serverless intelligent tool for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning at any scale. There is no infrastructure to deploy or manage. Also, there is no infrastructure to manage as it is serverless and no need to write code as it is UI–based. It suggests your next data transformation and predicts with each UI input.",
        "subsections": {},
        "summary": "* _Data Transformation Tools in Google Cloud_\n  * **Cloud Data Fusion**: a managed service for building ETL pipelines, supporting Cloud Dataproc\n  * _Dataprep by Trifacta_: a serverless tool for visually exploring, cleaning, and preparing data"
      },
      "Summary": {
        "content": "In this chapter, we discussed feature engineering and why it's important to transform numerical and categorical features for model training and serving.\n\nThen we discussed various techniques to transform numerical features, such as bucketing and normalization. We also discussed the technique to transform categorical features such as linear encoding, one‐hot encoding, out of vocabulary, hashing, and embedding.\n\nYou learned why it's important to select features and some of the techniques for dimensionality reduction such as PCA. Then we covered class imbalance and how precision and recall impacts the classification. For imbalanced classes, AUC PR is more effective than AUC ROC.\n\nWe also discussed why feature crosses are important and the benefits of feature crossing.\n\nWe covered how to represent data for TensorFlow using tf.data and then we covered tf.Transform and how to process pipelines using tf.Transform on Google Cloud. You learned about some of the Google Cloud data processing and ETL tools such as Cloud Data Fusion and Cloud Dataprep.",
        "subsections": {},
        "summary": "* **Feature Engineering**: Transforming numerical and categorical features is crucial for model training and serving.\n    * _Key techniques:_ Bucketing, normalization, linear encoding, one-hot encoding, out-of-vocabulary hashing, and embedding\n    * _Additional concepts:_\n        * Dimensionality reduction using PCA\n        * Class imbalance and AUC-PR vs. AUC-ROC"
      },
      "Exam Essentials": {
        "content": "* **Use consistent data processing.** Understand when to transform data, either before training or during model training. Also know the benefits and limitations of transforming data before training.\n  * **Know how to encode structured data types.** Understand techniques to transform both numeric and categorical data such as bucketing, normalization, hashing, and one‐hot encoding.\n  * **Understand feature selection.** Understand why feature selection is needed and some of the techniques of feature selection, such as dimensionality reduction.\n  * **Understand class imbalance.** Understand true positive, false positive, accuracy, AUC, precision, and recall in classification problems and how to effectively measure accuracy with class imbalance.\n  * **Know where and how to use feature cross.** You need to understand why feature cross is important and the scenarios in which you would need it.\n  * **Understand TensorFlow Transform.** You need to understand TensorFlow Data and TensorFlow Transform and how to architect tf.Transform pipelines on Google Cloud using BigQuery and Cloud Data Fusion.\n  * **Use GCP data and ETL tools.** Know how and when to use tools such as Cloud Data Fusion and Cloud Dataprep. For example, in case you are looking for a no‐code solution to clean data, you would use Dataprep for data processing and, in case you are looking for a no‐code and UI–based solution for ETL (extract, transform, load), you would use Cloud Data Fusion.",
        "subsections": {},
        "summary": "* **Data Processing Consistency**: Understand when to transform data before or during training, benefits, and limitations.\n  * _Encoding structured data types_: techniques like bucketing, normalization, hashing, and one-hot encoding for numeric and categorical data.\n  * _Feature selection and cross_: dimensionality reduction, feature cross importance, and scenarios requiring it.\n  * _GCP Data Tools_: Cloud Data Fusion (ETL) and Cloud Dataprep (data processing), choosing the right tool for the task."
      },
      "Review Questions": {
        "content": "1. You are the data scientist for your company. You have a dataset that which has all categorical features. You trained a model using some algorithms. With some algorithms this data is giving good result but when you change the algorithm the performance is getting reduced. Which data transformation strategy ould likely improve the performance of your model?\n     1. Write your data in TFRecords.\n     2. Create a feature cross with categorical feature.\n     3. Use one‐hot encoding on all categorical features.\n     4. Oversample the features.\n  2. You are working on a neural network–based project. The dataset provided to you has columns with different ranges. While preparing the data for model training, you discover that gradient optimization is having difficulty moving weights to an optimized solution. What should you do?\n     1. Use feature construction to combine the strongest features.\n     2. Use the normalization technique.\n     3. Improve the data cleaning step by removing features with missing values.\n     4. Change the partitioning step to reduce the dimension of the test set and have a larger training set.\n  3. You work for a credit card company and have been asked to create a custom fraud detection model based on historical data using AutoML Tables. You need to prioritize detection of fraudulent transactions while minimizing false positives. Which optimization objective should you use when training the model?\n     1. An optimization objective that minimizes log loss.\n     2. An optimization objective that maximizes the precision at a recall value of 0.50.\n     3. An optimization objective that maximizes the area under the precision‐recall curve (AUC PR) value.\n     4. An optimization objective that maximizes the area under the curve receiver operating characteristic (AUC ROC) curve value.\n  4. You are a data scientist working on a classification problem with time‐series data and achieved an area under the receiver operating characteristic curve (AUC ROC) value of 99 percent for training data with just a few experiments. You haven't explored using any sophisticated algorithms or spent any time on hyperparameter tuning. What should your next step be to identify and fix the problem?\n     1. Address the model overfitting by using a less complex algorithm.\n     2. Address data leakage by applying nested cross‐validation during model training.\n     3. Address data leakage by removing features highly correlated with the target value.\n     4. Address the model overfitting by tuning the hyperparameters to reduce the AUC ROC value.\n  5. You are training a ResNet model on Vertex AI using TPUs to visually categorize types of defects in automobile engines. You capture the training profile using the Cloud TPU profiler plug‐in and observe that it is highly input bound. You want to reduce the bottleneck and speed up your model training process. Which modifications should you make to the `tf.data` dataset? (Choose two.)\n     1. Use the interleave option to read data.\n     2. Set the prefetch option equal to the training batch size.\n     3. Reduce the repeat parameters.\n     4. Decrease the batch size argument in your transformation.\n     5. Increase the buffer size for shuffle.\n  6. You have been asked to develop an input pipeline for an ML training model that processes images from disparate sources at a low latency. You discover that your input data does not fit in memory. How should you create a dataset following Google‐recommended best practices?\n     1. Create a tf.data.Dataset.prefetch transformation.\n     2. Convert the images into TFRecords, store the images in Cloud Storage, and then use the tf.data API to read the images for training.\n     3. Convert the images to tf.Tensor objects, and then run Dataset.from_tensor_slices{).\n     4. Convert data into TFRecords.\n  7. Different cities in California have markedly different housing prices. Suppose you must create a model to predict housing prices. Which of the following sets of features or feature crosses could learn city‐specific relationships between roomsPerPerson and housing price?\n     1. Two feature crosses: [binned latitude x binned roomsPerPerson] and [binned longitude x binned roomsPerPerson]\n     2. Three separate binned features: [binned latitude], [binned longitude], [binned roomsPerPerson]\n     3. One feature cross: [binned latitude x binned longitude x binned roomsPerPerson]\n     4. One feature cross: [latitude x longitude x roomsPerPerson]\n  8. You are a data engineer for a finance company. You are responsible for building a unified analytics environment across a variety of on‐premises data marts. Your company is experiencing data quality and security challenges when integrating data across the servers, caused by the use of a wide range of disconnected tools and temporary solutions. You need a fully managed, cloud‐native data integration service that will lower the total cost of work and reduce repetitive work. Some members on your team prefer a codeless interface for building an extract, transform, load (ETL) process. Which service should you use?\n     1. Cloud Data Fusion\n     2. Dataprep\n     3. Cloud Dataflow\n     4. Apache Flink\n  9. You work for a global footwear retailer and need to predict when an item will be out of stock based on historical inventory data. Customer behavior is highly dynamic since footwear demand is influenced by many different factors. You want to serve models that are trained on all available data but track your performance on specific subsets of data before pushing to production. What is the most streamlined, scalable, and reliable way to perform this validation?\n     1. Use the tf.Transform to specify performance metrics for production readiness of the data.\n     2. Use the entire dataset and treat the area under the receiver operating characteristic curve (AUC ROC) as the main metric.\n     3. Use the last relevant week of data as a validation set to ensure that your model is performing accurately on current data.\n     4. Use k‐fold cross‐validation as a validation strategy to ensure that your model is ready for production.\n  10. You are transforming a complete dataset before model training. Your model accuracy is 99 percent in training, but in production its accuracy is 66 percent. What is a possible way to improve the model in production?\n     1. Apply transformation during model training.\n     2. Perform data normalization.\n     3. Remove missing values.\n     4. Use tf.Transform for creating production pipelines for both training and serving.",
        "subsections": {},
        "summary": "* To improve performance after changing algorithms, use **one-hot encoding on all categorical features** to reduce dimensionality\n    * This helps models learn from a more reduced feature space\n    * Reduces overfitting when trying different algorithms \n\n* For gradient optimization difficulty in neural networks:\n    * Use _normalization technique_  to improve weight initialization and learning.\n    * Normalization stabilizes gradients during training, allowing weights to converge to an optimal solution.\n\n* For AutoML Table model for fraud detection with false positives minimization, use **optimizing objective that maximizes the area under the precision‐recall curve (AUC PR) value** \n    * A more nuanced approach helps detect true positives and minimize false positives\n\n* To address high AUC ROC value without exploration:\n    * Address _model overfitting_ by tuning hyperparameters to reduce AUC ROC value.\n    * Overfitting can be caused by a complex model that does not generalize well to unseen data.\n\n* For input pipeline with low latency on Vertex AI:\n    * Use _tf.data.Dataset.prefetch transformation_ to improve loading speed and reduce memory usage.\n    * Prefetching enables faster iteration over the dataset during training\n\n* To process images from disparate sources at low latency:\n    * Convert data into TFRecords, store images in Cloud Storage, and use tf.data API to read images for training.\n\n* For predicting housing prices with city-specific relationships:\n    * Use _one feature cross: [latitude x longitude x roomsPerPerson]_ \n    * Combines location and room usage information\n\n* To unify analytics environment with fully managed cloud service for data integration:\n    * Use _Cloud Data Fusion_, a codeless, cloud-native ETL tool.\n    * Streamlines data integration, reduces costs and repetitive work"
      }
    },
    "summary": ""
  },
  "Chapter 4Choosing the Right ML Infrastructure": {
    "content": "",
    "subsections": {
      "Pretrained vs. AutoML vs. Custom Models": {
        "content": "When you have an ML problem to solve, say image classification, you have three ways to deal with approaching it:\n\n  * Pretrained\n  * AutoML\n  * Custom\n\nFigure 4.1 shows the differences between the models, and we'll elaborate on each in this chapter.\n\nThe first method is to use any pretrained models that might be available and ready to use. These are models that have already been trained on large datasets by Google. These pretrained models are already deployed and can be readily used via APIs. Google manages the deployment of these models; therefore, as a user, you only think about the number of times you call the APIs.\n\n**FIGURE 4.1** Pretrained, AutoML, and custom models\n\nThe biggest advantage of using pretrained models is the ease of use and the speed with which you can incorporate ML into your application. In this case, you do not have to think about the algorithm, the training method, the training hardware, the deployment hardware, scalability, and so on.\n\n* * *\n\nA pretrained model should be tried first. Only if the pretrained model does not meet your requirements should you consider AutoML or custom models.\n\n* * *\n\nA pretrained model should be your first choice. Carefully consider a pretrained model and evaluate if it fits your use case. You can start by using a pretrained model in your application, and then if you think you need a better model, you can move to an AutoML or custom model.\n\nPretrained models are used by thousands of users, and with millions of requests coming in, they are maintained by a team of engineers. Of all the users looking to use ML in their application, a majority will choose pretrained models. Figure 4.1 shows these models as a pyramid. You will notice that the bottom layer, which is wide, represents the number of users who use pretrained models.\n\nYou do not even have to be an ML engineer to use pretrained models. Any developer can make use of pretrained models using APIs. They can be used with ease using the Python, Java, and Node.js SDKs. We provide a detailed list of pretrained models that are available later in this chapter.\n\nWhen using the pretrained models, you are not provisioning any cloud resources, and you are consuming only through an API. This is called a _serverless_ method, and you will be charged based on the number of times you make a request to the API. Some of the APIs have a free tier. This is one of the reasons the pretrained models using APIs are so popular.\n\nNow, while the pretrained models work well for a majority of users, sometimes they may not work for you. It is the most convenient option, but it is also the least customizable. Say you used the Vision API to classify plants but now you want to identify the exact subspecies of a plant. While the Vision API can provide thousands of labels, it might not have names for subspecies of plants. In this case, you may consider the next option: AutoML.\n\nVertex AI AutoML is a method by which you can build your own model using your own data. There are some popular machine learning problems that have been researched for decades and understood very well. These problems—such as image classification and text translation, for example—have been used many times in the past and all the nuances and variations have been identified. AutoML chooses the best ML algorithm, and the only thing that it needs is the data. Like a “just add water” cake mix, AutoML can be considered “just add data.” You then have to format the data and work on quality control.\n\nUnlike with pretrained models, you have to provision cloud resources for training and deploying the model on instances. At the time of training, you have to decide on the number of hours of instance time that you have to provision, and during deployment, you will decide on whether the model is deployed in the cloud, on field devices like Android phones, or in other IoT devices. We will look at the hardware provisioning options later in this chapter.\n\nAutoML is the second level in the pyramid shown in Figure 4.1, where users who want to train models with their own data usually go. AutoML provides you with the ability to build your own models and at the same time does not require a team of ML experts to build them.\n\n* * *\n\nAutoML should be your choice if a pretrained model does not work for you and you do not have a team of ML engineers.\n\n* * *\n\nSay you find that your use case is unique and there is no AutoML available for it. In that case, you can use custom models in Vertex AI. This is the top tier in the pyramid in Figure 4.1, and it offers you a lot of flexibility for the choice of algorithm, hardware provisioning, and data types. The reason this is in the top of the pyramid is because of the flexibility, but at the same time, it also has the smallest base because the number of customers who have the expertise to use custom models is small. We will look at the hardware provisioning options for this training method later in this chapter.",
        "subsections": {},
        "summary": "* _Pretrained Models_: \n  * Use existing models trained on large datasets by Google.\n  * Easy to use and fast integration with APIs.\n  * No need to think about algorithm, deployment, or scalability.\n\n* _AutoML_:\n  * Build custom model using own data.\n  * Chooses best ML algorithm without needing expert knowledge.\n  * Requires provisioning cloud resources for training and deployment.\n\n* _Custom Models_: \n  * Top tier in pyramid; offers flexibility in algorithm, hardware, and data types.\n  * Suitable when use case is unique and no AutoML available."
      },
      "Pretrained Models": {
        "content": "When you are trying to solve a problem with machine learning, the first step is to see if there is a pretrained model. Pretrained models are machine learning models that have been trained on extremely large datasets and perform very well in benchmark tests. These models are supported by very large engineering and research teams and are retrained frequently.\n\nAs a customer, you can start using these models in just a few minutes through the web console (or the CLI, Python, Java, or Node.js SDK).\n\nAny developer who is trying to solve a problem using machine learning should first check if there is a pretrained model available on the Google Cloud platform, and if so, use it.\n\nGoogle Cloud has several pretrained models available:\n\n  * Vision AI\n  * Video AI\n  * Natural Language AI\n  * Translation AI\n  * Speech‐to‐Text and Text‐to‐Speech\n\nIn addition to pretrained models, Google Cloud has platforms that offer solutions to certain kinds of problems and include pretrained models as well as the ability to uptrain the existing models:\n\n  * Document AI\n  * Contact Center AI",
        "subsections": {
          "Vision AI": {
            "content": "Vision AI provides you with convenient access to ML algorithms for processing images and photos without having to create a complete machine learning infrastructure. Using this service, you can perform image classification, detect objects and faces, and read handwriting (through optical character recognition).\n\nYou can also try the service quickly from the convenience of your browser at `https://cloud.google.com/vision`. In production, typically you upload an image to the service or point to an image URL to analyze.\n\nWhen you try service using your browser, you get four types of predictions, as shown in Figure 4.2. First you see objects detected in the photo.\n\n**FIGURE 4.2** Analyzing a photo using Vision AI\n\nSecond, you get a set of labels for your image; in our example, the labels we get are Table, Furniture, Plant, Houseplant, Cabinetry, Wood, and so on. Third, you get the dominant colors for these images, which can be used to organize your images based on a palette. Last, you get the “Safe Search” classification, which is whether the image falls in one of these categories: Adult, Spoof, Medical, Violence, and Racy. These classifications can be used to label images for use cases where you want to restrict sharing certain kinds of images, such as to avoid showing adult content to young audiences.",
            "subsections": {},
            "summary": "* **Vision AI**: convenient access to ML algorithms for image processing and analysis\n    * Provides services such as:\n        * Object detection and face recognition\n        * Handwriting analysis (optical character recognition)\n    * Available at `https://cloud.google.com/vision`"
          },
          "Video AI": {
            "content": "This API has pretrained machine learning models that recognize objects, places, and actions in videos. It can be applied to stored video or to streaming video where the results are returned in real time.\n\nYou can use this service to recognize more than 20,000 different objects, places, and actions in videos. You can use the results as metadata in your video that can be used to search videos from your video catalog. For example, you can use the service to tag sports videos, and more specifically the type of sport. You can also process livestreams; for example, if you have a street camera looking at traffic, you can count the number of cars that cross an intersection.\n\nHere are several examples of use cases for Video AI:\n\n  * **Use Case 1:** This API can be used to build a video recommendation system, using the labels generated by the API and a user's viewing history. This provides you with an ability to recommend based on details from within the video and not just external metadata and can greatly increase user experience.\n  * **Use Case 2:** Another use case is to create an index of your video archives using the metadata from the API. This is perfect for mass media companies that have petabytes of data that are not indexed.\n  * **Use Case 3:** Advertisements inserted into videos sometimes could be completely irrelevant to the videos. This is another use case where you can improve the user experience by comparing the time‐frame‐specific labels of the video content and the content of the advertisements.",
            "subsections": {},
            "summary": "* **Video AI API**: Recognizes objects, places, and actions in videos with pretrained machine learning models.\n* *Key benefits:*\n    - Can process stored or streaming video\n    - Recognizes over 20,000 different objects, places, and actions\n    - Returns results in real-time for livestreams\n* **Use Cases:**\n    * _Video recommendation system_: recommends videos based on user viewing history and generated labels.\n    * _Video archiving_: indexes video metadata for mass media companies.\n    * _Ad relevance improvement_: compares video content with inserted ads for improved user experience."
          },
          "Natural Language AI": {
            "content": "The Natural Language AI provides insights from unstructured text using pretrained machine learning models. The main services it provides are entity extraction, sentiment analysis, syntax analysis, and general categorization.\n\nThe entity extraction service identifies entities such as the names of people, organizations, products, events, locations, and so on. This service also enriches the entities with additional information like links to Wikipedia articles if it finds any. Although entity extraction may sound like a simple problem, it is a nontrivial task. For example, in the sentence “Mr. Wood is a good actor,” it takes a good amount of understanding that “Mr. Wood” is a person and not a type of wood.\n\nSentiment analysis provides you a positive, negative, or neutral score with magnitude for each sentence, for each entity, and the whole text. The syntax analysis can be used to identify the part of speech, dependency between words, lemma, and the morphology of text. Finally, it also classifies documents into one of more than 700 predefined categories. For more details, see `https://cloud.google.com/natural-language`.\n\nHere are some use cases for Natural Language AI:\n\n  * **Use Case 1:** Measure the customer sentiment toward a particular product. Use entity analysis to find important details from documents like emails, chat, and social media and the sentiment analysis for each of these entities to understand customer opinion on specific products.\n  * **Use Case 2:** Use Healthcare Natural Language API to understand details specific to healthcare text like clinical notes or healthcare research documents. You can use this to extract medical insights like drugs, dosage, and conditions and build powerful healthcare apps. Note: Healthcare Natural Language API is a separate service from Google's Natural Language AI.",
            "subsections": {},
            "summary": "**Google Natural Language AI Services**\n* Provides entity extraction, sentiment analysis, syntax analysis, and categorization\n    * Entity extraction identifies key entities with additional information (e.g., Wikipedia links)\n    * Sentiment analysis scores text as positive, negative, or neutral by sentence, entity, and document\n* **Use Cases:**\n  * Measure customer sentiment for specific products\n  * Extract medical insights from healthcare texts using Healthcare Natural Language API"
          },
          "Translation AI": {
            "content": "Use Translation AI to detect more than 100 languages, from Afrikaans to Zulu, and translate between any pairs of languages in that list. It uses Google Neural Machine Translation (GNMT) technology that was pioneered by Google and is now considered industry standard. For more information, refer to `https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html`.\n\nThis service has two levels, Basic and Advanced. There are many differences, but the main difference is the Advanced version can use a _glossary_ (a dictionary of terms mapped from source language to target language) and also can translate entire documents (PDFs, DOCs, etc.). There is also price difference.\n\nYou can translate text in ASCII or in UTF‐8 format. In addition, you can also translate audio in real time using the Media Translation API, typically used for streaming services.\n\nThe Media Translation API (separate from the Translation API) directly translates audio in source language into audio in target languages. This helps with low‐latency streaming applications and scales quickly.",
            "subsections": {},
            "summary": "* **Translation Service**: detects over 100 languages and translates between any pairs of languages using Google Neural Machine Translation technology\n    * *_Service Tiers_*: Basic and Advanced, with differences including glossary support and document translation capabilities\n        + *_Advanced Tier_*: also includes real-time audio translation with Media Translation API"
          },
          "Speech‐to‐Text": {
            "content": "You can use the Speech‐to‐Text service to convert recorded audio or streaming audio into text. This is a popular service for creating subtitles for video recordings and streaming video as well. This is also commonly combined with a translate service to generate subtitles for multiple languages. For more details, see `https://cloud.google.com/speech-to-text#section-10`.",
            "subsections": {},
            "summary": "* *Speech-to-Text service*: converts recorded audio or streaming audio into text\n* Commonly used for creating subtitles and translating them into multiple languages"
          },
          "Text‐to‐Speech": {
            "content": "Customers use the Text‐to‐Speech service to provide realistic speech with humanlike intonation. This is based on the state‐of‐the‐art speech synthesis expertise from DeepMind (an AI subsidiary of Google). It currently supports 220+ voices across 40+ languages and variants. You can create a unique voice to represent your brand at all your touchpoints. See here for the list of languages supported: `https://cloud.google.com/text-to-speech/docs/voices`.",
            "subsections": {},
            "summary": "* _Text‐to‐Speech service_ provides realistic speech with humanlike intonation using DeepMind's AI expertise\n    * Supports 220+ voices in 40+ languages and variants, including creating unique brand voices\n        * Available at `_https://cloud.google.com/text-to-speech/docs/voices`_"
          }
        },
        "summary": "* **Pretrained Models**: Machine learning models trained on large datasets, performing well in benchmark tests, and available for use through Google Cloud's web console or SDK.\n* **Google Cloud Pretrained Models**: Various models including:\n  * _Vision AI_\n  * _Video AI_\n  * _Natural Language AI_\n  * _Translation AI_\n  * _Speech-to-Text_ and _Text-to-Speech_\n* **Platforms Offering Solutions**: Document AI and Contact Center AI, which include pretrained models and uptrain capabilities."
      },
      "AutoML": {
        "content": "AutoML, or automated ML, is the process of automating the time‐consuming tasks of model training. AutoML is available for popular, well understood, and practically feasible ML problems like image classification, text classification, translation, and so on. You as a user only bring in the data and configure a few settings and the rest of the training is automated. You either leverage the easy‐to‐use web console or use a Python, Java, or Node.js SDK to initiate the AutoML training job.\n\nThere is AutoML training available for many data types and use cases. We can broadly categorize them into four categories:\n\n  * Structured data\n  * Images/video\n  * Natural language\n  * Recommendations AI/Retail AI",
        "subsections": {
          "AutoML for Tables or Structured Data": {
            "content": "Structured data is data that adheres to a well‐defined schema and is usually in the form of a rectangular table. With tables there are two methods of training models:\n\n  * **BigQuery ML:** This is a SQL‐based approach to training models. You can use this if you are a data analyst and are comfortable writing SQL queries. You can both train and make predictions using BigQuery and automatically add the predictions to tables. This is a serverless approach for ML training and prediction. We cover this in detail in Chapter 14, “BigQuery ML.”\n  * **Vertex AI Tables:** This is the second method to train ML models that can be triggered using Python, Java, or Node.js, or using REST API. Vertex AI tables provide you with the ability to deploy the model on an endpoint and serve predictions through a REST API.\n\nWe will cover the available AutoML Tables algorithms in Table 4.1.\n\n**TABLE 4.1** Vertex AI AutoML Tables algorithms\n\nData Type | ML Problem | Metrics\n---|---|---\nTable (IID) | Classification | AUC ROC, AUC ROC, Logloss, Precision at Recall, Recall at Precision\nTable (IID) | Regression | RMSE, RMSLE, MAE\nTime‐series data | Forecasting | RMSE, RMSLE, MAPE, Quantile loss\n\nWhile configuring the AutoML job, there are a few things that are relevant from the hardware perspective. The “budget” is the last step in the web console, where you specify the maximum number of hours that you allow the job to run (see Figure 4.3). If the training job is not completed within the budget, AutoML will use the best model that was trained within the budget. There is another setting called Enable early stopping in figure below, which will end the training job if it identifies that the model training has completed, in which case you will be charged for only the number of node hours that were used.\n\n**FIGURE 4.3** Vertex AI AutoML, providing a “budget”\n\nThere are minimum values to the budget you can provide for the AutoML job. For example, for Object Detection AutoML, the minimum is 20 hours. Another important aspect to remember is that although all AutoML jobs require you to provide node hours for the budget, not all node hours are the same. The price of each node hour is different for the different types of AutoML jobs because the hardware used for different AutoML jobs is different.\n\nFrom the model type perspective, forecasting is a special case. Vertex AI offers three model training methods for forecasting:\n\n  * **AutoML** This is the built‐in model that is good for a wide variety of forecasting use cases.\n  * **Seq2seq+** This type of model takes in a sequence and produces another sequence. This model is effective when the dataset size is less than 1 GB.\n  * **Temporal Fusion Transformer** This is a deep neural network model that also uses the attention mechanism. It is designed to produce high accuracy and interpretability for a wide range of use cases.",
            "subsections": {},
            "summary": "* *_Structured data_*: Data in a well-defined schema, typically represented as a rectangular table.\n    * *_Training methods_*:\n        + **BigQuery ML**: A SQL-based approach for training models\n        + **Vertex AI Tables**: A serverless approach that uses Python, Java, or Node.js to deploy and serve predictions\n* *_Vertex AI AutoML Tables algorithms_*:\n    * *_Classification_*: AUC ROC, AUC ROC, Logloss, Precision at Recall, Recall at Precision\n    * *_Regression_*: RMSE, RMSLE, MAE\n    * *_Time-series data_*: Forecasting RMSE, RMSLE, MAPE, Quantile loss"
          },
          "AutoML for Images and Video": {
            "content": "Machine learning problems on image and video data used to be cumbersome and time‐consuming for machine learning experts until recently. Vertex AI AutoML makes it extremely easy to build models for these kinds of problems. Table 4.2 neatly summarizes all the available AutoML algorithms for these data types.\n\n**TABLE 4.2** AutoML algorithms\n\nData Type | ML Problem | AutoML Details\n---|---|---\nImage | Image classification (single) | Predict one correct label from a list of labels provided by user during training.\nImage | Multiclass classification | Predict all the correct labels that you want assigned to an image.\nImage | Object detection | Predict all the locations of objects that you're interested in.\nImage | Image segmentation | Predict per‐pixel areas of an image with a label.\nVideo | Classification | Get label predictions for entire videos, shots, and frames.\nVideo | Action recognition | Identify the action moments in video.\nVideo | Object tracking | Get labels, tracks, and time stamps for objects you want to track in a video.\n\nOne more consideration in AutoML related to hardware is the AutoML Edge model. These models are to be deployed to the edge devices, so the models are trained and configured to use less memory and low latency. The edge devices currently supported are iPhones, Android phones (Google Pixel, Samsung Galaxy, etc.), and Edge TPU devices, and this set will continue to increase as Google supports more.\n\nWhen you choose an AutoML Edge model, there are options to help you find the right trade‐off between accuracy and latency. For Edge TPU, we see the options shown in Figure 4.4.\n\n**FIGURE 4.4** Choosing the size of model in Vertex AI",
            "subsections": {},
            "summary": "* **Vertex AI AutoML**: Simplifies building machine learning models for image and video data\n    * Supports various tasks: image classification, multiclass classification, object detection, image segmentation, video classification, action recognition, and object tracking\n        * *AutoML Edge model*: Optimized for edge devices, using less memory and low latency for iPhones, Android phones, and Edge TPU devices"
          },
          "AutoML for Text": {
            "content": "Machine learning models for text are also very well understood now, and you can build your own models easily using Vertex AI AutoML text. There are four popular problems that are solved using AutoML, including classification and sentiment analysis. Table 4.3 shows the full list.\n\n**TABLE 4.3** Problems solved using AutoML\n\nData Type | ML Problem | AutoML Details\n---|---|---\nText | Text classification | Predict the one correct label that you want assigned to a document.\nText | Multi‐label classification | Predict all the correct labels that you want assigned to a document.\nText | Entity extraction | Identify entities within your text items.\nText to text | Translation | Convert text from source language to target language.",
            "subsections": {},
            "summary": "* **Machine Learning for Text**: Easy to build models using Vertex AI AutoML text\n    * Solves 4 main problems: \n        * _Text classification_: predict label for a document\n        * Multi-label classification: predict multiple labels for a document\n        * Entity extraction: identify entities in text\n        * Translation: convert text from source language to target language"
          },
          "Recommendations AI/Retail AI": {
            "content": "GCP has an AutoML solution for the retail domain. Retail Search offers retailers a Google‐quality search that can be customized and built upon Google's understanding of user intent and context.\n\nThe Vision API Product Search (a service under Vision AI) can be trained on reference images of products in your catalog, which can then be searched using an image.\n\nThe third part of the solution is Recommendations AI, which can understand nuances behind customer behavior, context, and SKUs in order to drive engagement across channels through relevant recommendations.\n\nIn this solution, customers upload the product catalog with details about each product, photos, and other metadata. The customer then feeds in the “user events” such as what the customer clicks, views, and buys. Recommendations AI uses this data to create models. Customers are charged for training models, which are continuously fine‐tuned to include updates to the “user events” data.\n\nIn addition, when the recommendations are served and get millions of hits, the customer is charged for each 1,000 requests. This is a serverless approach to provisioning resources and the customer does not have to bother about the exact hardware behind the scenes.\n\nRecommendations AI has an easy‐to‐use automated machine learning training method. It provides several different models that serve a variety of purposes for an online retail presence. From the exam perspective, it is important to understand Table 4.4, which describes the different recommendation types.\n\n**TABLE 4.4** Summary of the recommendation types available in Retail AI\n\nSource: Adapted from Google cloud/ `https://cloud.google.com/retail/docs/models#model-types` last accessed December 16, 2022.\n\nRecommendation Type | What Does It Predict? | Usage | Data Used | Optimization Objective\n---|---|---|---|---\nOthers you may like | The next product the user is likely to buy | Product page | Customer behavior and product relevance | Click‐through rate\nFrequently bought together (shopping cart expansion) | Items frequently bought together for a specific product within the same shopping session | Checkout page | User behavior in shopping cart | Revenue per order\nRecommended for you | The next product the user likely will buy | Home page | User viewing history and context | Click‐through rate\nSimilar items | Other products with similar attributes | Product page | Product catalog | Click‐through rate",
            "subsections": {},
            "summary": "### Google Cloud Retail AI Solution\n\n* AutoML solution for retail domain offering **Google-quality search**, **image search**, and **personalized recommendations**\n* Solutions include:\n\t+ _Retail Search_: customizes Google's understanding of user intent and context\n\t+ _Vision API Product Search_: trains on product images for image-based searching\n\t+ _Recommendations AI_: drives engagement through relevant recommendations based on customer behavior and product metadata"
          },
          "Document AI": {
            "content": "When you want to extract details from documents, like digitized scanned images of old printed documents, books, or forms, you can use Document AI. These are pages that can contain text in paragraph format and also tables and pictures. Some of this text could be printed and sometimes it could be handwritten. This is also a common type of document seen in government offices like the DMV where people fill out forms.\n\nForms contain a mix of printed text, with blank spaces where people fill out their details. This could be written with different types of ink (blue or black pen, etc.), and sometimes people make mistakes while writing. So, the ML model needs to understand the structure of the document (where to expect “name” and “address”) and have a tolerance for handwritten text.\n\nAnother example is government documents like passports, driver's licenses, and tax filings. These are documents that have better structure but still have some variability.\n\nIf we can extract important details (like “firstname,” “lastname,” “address,” etc.) from forms, we have structured data, which can now be stored in a database and can be analyzed. This is the extraction phase.\n\nDocument AI is a platform that understands documents and helps you to do the following:\n\n  * Detect document quality.\n  * Deskew.\n  * Extract text and layout information.\n  * Identify and extract key/value pairs.\n  * Extract and normalize entities.\n  * Split and classify documents.\n  * Review documents (human in the loop).\n  * Store, search, and organize documents (Document AI Warehouse)\n\nDocument AI has two important concepts: _processors_ and a _Document AI Warehouse_.\n\nA Document AI processor is an interface between the document and a machine learning model that performs the actions. There are general processors, specialized processors (procurement, identity, lending, and contract documents), and custom processors. You can train the custom processor by providing a training dataset (labeled set of documents) for your custom needs. For the full list of processor types, visit `https://cloud.google.com/document-ai/docs/processors-list`.\n\nA Document AI Warehouse is a platform to store, search, organize, govern, and analyze documents along with their structured metadata.",
            "subsections": {},
            "summary": "* **Document AI**: extracts details from scanned images of old documents, forms, and government papers using machine learning models.\n    * **Key Features**:\n        * Detects document quality\n        * Extracts text and layout information\n        * Identifies and extracts key/value pairs\n    * _Processors_: interfaces between documents and machine learning models for specific tasks (general, specialized, custom)"
          },
          "Dialogflow and Contact Center AI": {
            "content": "Dialogflow is a conversational AI offering from Google Cloud that provides chatbots and voicebots. This is integrated into a telephony service and other services to provide you with a Contact Center AI (CCAI) solution. Let us look at the services provided by this solution.",
            "subsections": {
              "Virtual Agent (Dialogflow)": {
                "content": "You can rapidly develop advanced virtual agents for enterprises that can handle topic switching and supplemental questions and provide multichannel support 24/7. It should be designed in such a way that the virtual agent is able to handle a majority of the cases. The complex calls will be forwarded to a human agent.\n\n* * *\n\nThe idea is to have the virtual agent handle a majority of common cases and the human agents handle the more complex calls. This is designed using the data collected from historical calls to the data center.\n\n* * *",
                "subsections": {},
                "summary": "* **Virtual Agent Strategy**: Rapidly develop advanced virtual agents for enterprises, handling most common cases and forwarding complex ones to human agents.\n* * * \n  * _Primary Function_: Handle topic switching and supplemental questions.\n  * _Key Feature_: Multichannel support 24/7."
              },
              "Agent Assist": {
                "content": "When the human agent is handling a call, the Agent Assist can provide support by identifying intent and providing ready‐to‐send responses and answers from a centralized knowledge base as well as transcript calls in real time.",
                "subsections": {},
                "summary": "* **Agent Assist**: Identifies customer intent and provides pre-crafted responses and transcripts to assist with calls.\n* *_Real-time support_*: Agent Assist provides assistance in the moment, using a centralized knowledge base.\n* *Automated response aid*: Enhances agent capabilities by offering ready-to-send answers."
              },
              "Insights": {
                "content": "This service uses natural language processing to call drivers and measure sentiment to help leadership understand the call center operations so they can improve outcomes.",
                "subsections": {},
                "summary": "* *Natural Language Processing (NLP)*: Analyzes calls to measure driver sentiment\n    * Calls drivers to gauge customer feedback\n        + Helps leadership optimize call center performance"
              },
              "CCAI": {
                "content": "The Contact Center AI platform is a complete cloud native platform to support multichannel communications between customers and agents.\n\nAlthough Dialogflow and CCAI use advanced machine learning techniques, especially in natural language, they are mostly hidden from the machine learning engineer. An in‐depth understanding of CCAI is beyond the scope of this exam.",
                "subsections": {},
                "summary": "* *_Contact Center AI platform_*: Cloud native platform for multichannel customer-agent communications\n* *_Dialogflow and CCAI_*: Advanced machine learning techniques, but details are not within the scope of this exam."
              }
            },
            "summary": "* Dialogflow is a *conversational AI platform* offering from Google Cloud that provides chatbots and voicebots.\n    * Integrated into telephony services for a *_Contact Center AI_* solution, enabling businesses to provide intelligent customer interactions\n        * Also available as a standalone service for various applications and use cases"
          }
        },
        "summary": "* **Automated Machine Learning (AutoML)**: automates model training tasks for popular ML problems like image classification, text classification, and more.\n* AutoML uses a web console or SDK to initiate the training process with minimal user configuration.\n* Available for various data types:\n  * *_Structured data_*\n  * Images/video\n  * *_Natural language_*\n  * *_Recommendations AI/Retail AI_*"
      },
      "Custom Training": {
        "content": "When you have chosen custom training, you have full flexibility to choose a wide range of hardware options to train your model on.\n\nGraphics processing units (GPUs) can accelerate the training process of deep learning models. Models used for natural language, images, and videos need compute‐intensive operations like matrix multiplications that can benefit by running on massively parallel architectures like GPUs.\n\nIf you train a deep learning model on a single CPU, it could take days, weeks, or sometimes months to complete. However, if you can offload the heavy computation to a GPU, it can reduce the time by an order of magnitude. What used to take days might take hours to complete.\n\nTo understand the advantages of specialized hardware, let us first see how a CPU works.",
        "subsections": {
          "How a CPU Works": {
            "content": "A CPU is a processor that can run a general workload, including software applications, data applications, and so on. The CPU is designed to be very flexible and supports a large number of operations.\n\nA CPU loads data from memory, performs some operation on the value, and stores the result back into memory for every single operation. This architecture works very well for general‐purpose software applications. However, a CPU performs computation serially, which is very inefficient when trillions of calculations are required, very typical when training ML models on large datasets.",
            "subsections": {},
            "summary": "* **CPU Basics**\n  * A processor that runs various types of workloads\n  * Designed to be flexible and perform many operations\n  * Loads data from memory, performs operation, and stores result back into memory for each task\n* **Serial Computation Limitation**\n  * CPU performs computations sequentially, which can lead to inefficiency"
          },
          "GPU": {
            "content": "GPUs bring in additional firepower. A graphics processing unit (GPU) is a specialized chip designed to rapidly process data in memory to create images; it was originally intended to process movies and render images in video games. A GPU does not work alone and is a subprocessor that helps the CPU in some tasks.\n\nGPUs contain thousands of arithmetic logic units (ALUs) in a single processor. So instead of accessing the memory for each operation, a GPU loads a block of memory and applies some operation using the thousands of ALUs in parallel, thereby making it faster. Using the GPUs for large matrix multiplications and differential operations could improve the speed by an order of magnitude in time.\n\nTo use GPUs, you must use an A2 or N1 machine series. GPU currently has the following GPUs available:\n\n  * NVIDIA_TESLA_T4\n  * NVIDIA_TESLA_K80\n  * NVIDIA_TESLA_P4\n  * NVIDIA_TESLA_P100\n  * NVIDIA_TESLA_V100\n  * NVIDIA_TESLA_A100\n\nIn your `WorkerPoolSpec`, specify the type of GPU that you want to use in the `machineSpec.acceleratorType` field and the number of GPUs that you want each VM in the worker pool to use in the `machineSpec.acceleratorCount` field.\n\nWhen you are trying to configure GPUs with instance types, there are several restrictions based on instance types, instance memory, and so on. Some restrictions are as follows:\n\n  * The type of GPU that you choose must be available in the location where you are performing custom training. Not all types of GPUs are available in all regions.\n\nHere is a page that lists the available locations: `cloud.google.com/vertex-ai/docs/general/locations`.\n\n  * There are restrictions on the number of GPUs per instance. For example, you can use two or four NVIDIA TESLA_T4 GPUs on a VM but not three. There is a full table provided by Google for reference, which is subject to change as more instance types and GPUs are introduced; see the link in the tip below.\n  * The GPU configuration must have sufficient virtual CPUs and memory compared to the machine type that goes with it. For example, if you use the n1‐standard‐16 machine type in your worker pool, then each VM has 16 virtual CPUs and 60 GB of memory. Since each NVIDIA_TESLA_V100 GPU can provide up to 12 virtual CPUs and 76 GB of memory, you must use at least 2 GPUs for each n1‐standard‐16 VM to support its requirements. (One GPU provides insufficient resources, and you cannot specify 3 GPUs.)\n\n* * *\n\nGoogle provides a compatibility table that specifies the number of GPUs for each instance type for quick reference. For the exam, you do not have to remember anything but the constraints listed earlier.\n\n`https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table`\n\n* * *",
            "subsections": {},
            "summary": "**GPUs in Vertex AI**\nGPUs bring additional firepower to Vertex AI, accelerating data processing and image rendering. A GPU is a specialized chip with thousands of arithmetic logic units (ALUs) that work together in parallel.\n\n**Configuring GPUs**\n\n* To use GPUs, specify the type of GPU (`machineSpec.acceleratorType`) and number of GPUs per VM (`machineSpec.acceleratorCount`).\n* Restrictions apply to instance types, location availability, and resource compatibility.\n* * Google provides a compatibility table for quick reference: [https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table](https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table)\n\n**Key Points**\n\n* * GPUs are available in A2 and N1 machine series.\n* Not all GPU types are available in all regions.\n* Number of GPUs per instance is restricted, with tables providing guidance on compatible instances."
          },
          "TPU": {
            "content": "As ML engineers used GPUs to train very large neural networks, they started to notice the next bottleneck. The GPU is still a semi‐general‐purpose processor that has to support many different applications, including video processing software. Therefore, in this way GPUs have the same problem as CPUs. For every calculation in the thousands of ALUs, a GPU must access registers or shared memory to read operands and store the intermediate calculation results. To take performance to the next level, Google designed TPUs.\n\nTensor Processing Units (TPUs) are specialized hardware accelerators designed by Google specifically for machine learning workloads. See Figure 4.5 for the system architecture of a TPU‐v4.\n\n**FIGURE 4.5** TPU system architecture\n\nInstead of having an ALU that performs one operation at a time in a CPU, each TPU has multiple matrix multiply units (MXUs).\n\nEach MXU has 128 × 128 multiply/accumulators. Each MXU is capable of performing 16,000 multiply‐accumulate operations in each cycle using the bfloat16 number format.\n\nThe primary task for TPUs is matrix processing, which is a combination of multiply and accumulate operations. TPUs contain thousands of multiply‐accumulators that are directly connected to each other to form a large physical matrix.\n\nTPUs can perform huge operations on huge matrices, which are the core of the training loop in neural networks.",
            "subsections": {
              "How to Use TPUs": {
                "content": "TPUs can be connected in groups called Pods that scale up your workloads with little to no code changes.\n\nCloud TPU provides the following TPU configurations:\n\n  * A single TPU device\n  * A TPU Pod (a group of TPU devices connected by high‐speed interconnects)\n  * A TPU slice (a subdivision of a TPU Pod)\n  * A TPU VM\n\n* * *\n\nThe general recommendation is to start your machine learning project with a single TPU and scale out to a TPU Pod for production.\n\n* * *",
                "subsections": {},
                "summary": "**Cloud TPU Configurations**\n \n* * A single TPU device\n* * A **TPU Pod**: A group of TPU devices connected by high-speed interconnects, scaling workloads with little code changes\n* * A **TPU Slice**: A subdivision of a TPU Pod for more precise control"
              },
              "Advantages of TPUs": {
                "content": "TPUs accelerate the computational speed beyond GPUs. Models that take months to train on CPUs might take a few days to train on GPUs but might run in a matter of hours on TPUs. Simply put, TPUs could provide an order of magnitude improvement over GPUs.",
                "subsections": {},
                "summary": "* **TPUs outperform GPUs by orders of magnitude**, accelerating training times from _months (CPUs)_ to _hours (GPUs) and potentially just days (TPUs)_."
              },
              "When to Use CPUs, GPUs, and TPUs": {
                "content": "TPUs are great for specific workloads, but in some situations, it might be better to use GPUs or even CPUs. Here are the guidelines to choose the right hardware for your use case:\n\n  * CPUs\n    * Rapid prototyping that needs flexibility\n    * Models that train fast\n    * Small models that work with small batch size\n    * Custom TensorFlow operations written in C++\n    * Limited by available I/O or the networking bandwidth of the host\n  * GPUs\n    * Models for which source code does not exist or is too tedious to change\n    * Models with a significant number of custom TensorFlow operations so they need to run at least partially on a CPU\n    * Models with TensorFlow ops that are not available on TPUs\n    * Medium‐to‐large models with medium‐sized batch\n  * TPUs\n    * Models that have a majority of matrix computations\n    * Models that have no custom TensorFlow operations\n    * Models that train for weeks or months\n    * Large and very large models with very large effective batch sizes\n\nCloud TPUs are _not_ suited to the following workloads:\n\n  * Programs that require frequent branching (if/else or conditional) or are dominated element‐wise by algebra. TPUs are not designed to perform conditionals but they are designed for large‐scale matrix multiplications.\n  * Sparse data (data that has lot of zeros and only a small fraction of nonzero values), which leads to sparse memory access, is not suitable.\n  * High precision is not well suited for TPUs. Do not expect double precision operations.\n  * Deep neural networks that contain custom TensorFlow operations written in C++, especially if the custom operations in the main training loop are not suitable for TPUs.\n\nTo effectively gain from the TPUs, you should run multiple iterations of the training loop on the TPU. This is not a theoretical requirement but a practical requirement to remove dramatic inefficiency caused otherwise. You can find these guidelines and more information about TPUs in the documentation at `https://cloud.google.com/tpu/docs/tpus`.",
                "subsections": {},
                "summary": "* **Hardware Selection**: Choose CPU for rapid prototyping, small models with limited I/O, or custom TensorFlow operations in C++.\n    * _Use GPU_ when source code is too tedious to change, custom ops are needed, medium-to-large models require TPUs.\n* **Cloud TPU Limitations**: Do not use TPUs for programs requiring conditionals, sparse data, high precision, or deep neural networks with custom C++ ops.\n    * _Best suited for matrix computations and large models_\n* **TPU Training Efficiency**: Run multiple iterations of the training loop to remove dramatic inefficiency."
              },
              "Cloud TPU Programming Model": {
                "content": "The main bottleneck when using TPUs is the data transfer between the Cloud TPU and host memory. This data transfer happens through the PCIe bus that is much slower than the TPU interconnect and the on‐chip high bandwidth memory (HBM). If you do use a partial compilation of a model, where part of execution happens on TPU and the rest happens on host, the TPU will be idle, waiting for data. The right programming model is to execute much of the training loop on the TPU, ideally all of it.",
                "subsections": {},
                "summary": "* **Data Transfer Bottleneck**: Data transfer between Cloud TPU and host memory via PCIe bus is a significant bottleneck due to its slow speed compared to TPU interconnect and HBM.\n    * _Optimal Programming Model_: Executing most of the training loop on the TPU, ideally all of it, to minimize idle time."
              }
            },
            "summary": "* **GPUs Limitation**: GPUs have limitations due to their semi-general-purpose design and need for frequent register access.\n* _**TPUs Design**_: TPUs are specialized hardware accelerators designed specifically for machine learning workloads.\n    * **Matrix Processing**: TPUs excel at matrix processing, performing massive multiply-accumulate operations on large matrices."
          }
        },
        "summary": "* **Training on Custom Hardware**: With custom training, you have full flexibility to choose from a wide range of hardware options for training your model.\n    * _GPUs can significantly speed up deep learning model training_, reducing compute-intensive operations like matrix multiplications by an order of magnitude.\n        + Trained on a single CPU, model training can take days or weeks; with GPU acceleration, it can be reduced to hours."
      },
      "Provisioning for Predictions": {
        "content": "In the previous section we looked at the hardware provisioning aspects during the training phase. Now, in this section we will look at the predictions phase.\n\nThe prediction workload is very different from the training workload. During training, we will provision resources that will be utilized for a particular duration, which will be deprovisioned on completion.\n\nPrediction can happen in two methods: online and batch. Online prediction is where the model is deployed on a server, and can be queried using a convenient interface like REST. Here the response time is expected to be near real‐time. Batch prediction is where a large volume of input data is already available and stored and prediction is initiated as a “batch job.” Here, the user expects the job to be completed in near reasonable time (not real‐time) but the focus is on the cost of prediction.\n\nIn comparison to the training workload, prediction (especially online prediction) is a continuous workload, which also needs to be continuously scaled up or down based on demand. The batch prediction workload differs significantly from both as well.\n\nIn this context, the two most important considerations when provisioning for predictions are scaling behavior and finding the ideal machine type. Let us look at these two considerations in detail.",
        "subsections": {
          "Scaling Behavior": {
            "content": "If you use an autoscaling configuration, Vertex AI automatically scales to use more prediction nodes when the CPU usage of your existing nodes gets high. If you are using GPU nodes, make sure to configure the appropriate trigger because there are three resources (CPU, memory, and GPU) that have to be monitored for usage.",
            "subsections": {},
            "summary": "* _**Autoscaling in Vertex AI**: Automatically scales prediction nodes when CPU usage is high._\n* *To monitor resources on GPU nodes: CPU, Memory, and GPU usage need to be configured as triggers.*\n* *Ensure proper trigger configuration for efficient resource utilization.*"
          },
          "Finding the Ideal Machine Type": {
            "content": "To determine the ideal machine type for a custom prediction container from a cost perspective, deploy that container as a docker container to a Compute Engine instance directly, then benchmark the instance by calling prediction calls until the instance hits 90+ percent CPU utilization. Determine the _queries per second (QPS) cost per hour_ of different machine types. As an example, if a custom container contains a Python web server process that can only effectively use 1 core and that process is calling a multithreaded ML model (such as most implementations of XGBoost), as QPS increases, the web server will start to “block” XGBoost because every XGBoost prediction will wait on the web server process. If this deployed to a 2‐ or 4‐core machine shape, the container will hit the QPS limits and the CPU utilization will be high, so the container is deployed to Vertex AI, and it will autoscale effectively.\n\nBe aware of single‐threaded web server limitations in your custom container; it may not scale even when deployed in a 32‐core machine shape.\n\nSo, you need to include the model type, serving wrapper code in case of custom models, effective utilization of resources (CPU, memory, GPU), latency and throughput requirements, and price to determine the instance type to use.\n\nYou have the option of using GPUs to accelerate predictions, but there are some restrictions. The restrictions are:\n\n  * GPUs can only be used for a TensorFlow SavedModel or when you use a custom container that has been designed to take advantage of GPUs. You cannot use GPUs for scikit‐learn or XGBoost models.\n  * GPUs are not available in some regions.\n  * You can use only one type of GPU DeployedModel resource or BatchPredictionJob, and there are limitations on the number of GPUs you can add depending on which machine type you are using.\n\nThe above considerations were about predictions in the cloud. There are many use cases where the trained model needs to be deployed at the edge devices. Let us explore these use cases below.",
            "subsections": {},
            "summary": "* To determine the ideal machine type for a custom prediction container from a cost perspective, consider:\n    * _Queries per second (QPS) cost per hour_ of different machine types\n    * Resource utilization (_CPU_, _memory_, _GPU_) and limitations (_single-threaded web server_)\n    * Latency and throughput requirements\n\n* Consider using GPUs to accelerate predictions, but be aware of restrictions such as:\n    * _Only compatible with TensorFlow SavedModel or custom containers_\n    * Limited availability in some regions\n    * _One type of GPU per DeployedModel resource_"
          },
          "Edge TPU": {
            "content": "An important part of the Internet of Things (IoT) are the edge devices. These devices collect real‐time data, make decisions, take action, and communicate with other devices or with the cloud. Since such devices have limited bandwidth and sometimes may operate completely offline, there is increasing demand for running inference on the device itself, called _edge inference_.\n\nThe Google‐designed Edge TPU coprocessor accelerates ML inference on these edge devices. A single Edge TPU can perform 4 trillion operations per second (4 TOPS), on just 2 watts of power.\n\nThe Edge TPU is available for your own prototyping and production devices in several form factors, including a single‐board computer, a system‐on‐module, the Edge TPU, and all available products. This is sold under the brand name of Coral.ai.\n\n* * *\n\nTPUs are popularly used for training but usually are not used for serving in the cloud. However, Edge TPUs are used for deploying models at the edge.\n\n* * *",
            "subsections": {},
            "summary": "* **Edge Inference**\n  * Accelerates ML inference on IoT devices with limited bandwidth and offline capabilities\n  * Enables real-time data analysis and decision-making\n  * Improves device performance and reduces power consumption\n\n* **Google Edge TPU**\n  * 4 trillion operations per second (4 TOPS) with 2 watts of power\n  * Available for prototyping and production devices in various form factors"
          },
          "Deploy to Android or iOS Device": {
            "content": "ML Kit (`https://developers.google.com/ml-kit`) brings Google's machine learning expertise to mobile developers in a powerful and easy‐to‐use package. You can make your iOS and Android apps more engaging, personalized, and helpful with solutions that are optimized to run on device.\n\nYou can train your ML model on Google Cloud, use AutoML or a custom model, and deploy the model into your Android or iOS app. The prediction happens in the device for low response times to save on bandwidth and to enable prediction in offline mode as well.",
            "subsections": {},
            "summary": "* **ML Kit**: A mobile development package that brings Google's machine learning expertise to iOS and Android apps\n    * Allows training of ML models on Google Cloud, using AutoML or custom models\n    * Enables predictions on device for low latency and offline support"
          }
        },
        "summary": "* **Prediction Phase**: Focuses on deploying a model to make predictions\n    * _Methods_: Online prediction (near real-time) and batch prediction (near reasonable time)\n    * _Workload characteristics_: Continuous, with demand-driven scaling required"
      },
      "Summary": {
        "content": "In this chapter, you learned about different pretrained models that are available on Google Cloud. You also learned about AutoML models and the applicability to different scenarios. In the main part of the chapter, you learned about the different hardware options available for training your models, the difference in the training workload and prediction workload. Google Cloud provides you with a wide variety of hardware accelerators in the form of GPUs and TPUs. Finally, going beyond the cloud, you were introduced to the ideas of deploying to the edge devices.",
        "subsections": {},
        "summary": "* **Google Cloud Models**: pretrained models available on Google Cloud for various applications\n* **Hardware Options**: GPUs, TPUs, and edge computing for training and prediction workloads\n* *_Deploying Beyond the Cloud_*: using edge devices for model deployment"
      },
      "Exam Essentials": {
        "content": "* **Choose the right ML approach.** Understand the requirements to choose between pretrained models, AutoML, or custom models. Understand the readiness of the solution, the flexibility, and approach.\n  * **Provision the right hardware for training.** Understand the various hardware options available for machine learning. Also understand the requirements of GPU and TPU hardware and the instance types that support the specialized hardware. Also learn about hardware differences in training and deployment.\n  * **Provision the right hardware for predictions.** Learn the difference between provisioning during training time and during predictions. The requirements for predictions are usually scalability and the CPU and memory constraints, so CPUs and GPUs are used in the cloud. However, TPUs are used in the edge devices.\n  * **Understand the available ML solutions.** Instead of provisioning hardware, take a serverless approach by using pretrained models and solutions that are built to solve a problem in a domain.",
        "subsections": {},
        "summary": "* _**Machine Learning Approach Selection**: Choose between pretrained models, AutoML, or custom models based on solution readiness, flexibility, and approach._\n* * **Hardware Provisioning for Training**: Understand GPU, TPU, and instance type requirements for training, as well as differences in hardware for deployment._\n* * **Hardware Provisioning for Predictions**: Scalability, CPU, and memory constraints require cloud-based CPUs and GPUs for predictions, while TPUs are used in edge devices._"
      },
      "Review Questions": {
        "content": "1. Your company deals with real estate, and as part of a software development team, you have been asked to add a machine learning model to identify objects in photos uploaded to your website. How do you go about this?\n     1. Use a custom model to get best results.\n     2. Use AutoML to create object detection.\n     3. Start with Vision AI, and if that does not work, use AutoML.\n     4. Combine AutoML and a custom model to get better results.\n  2. Your company is working with legal documentation (thousands of pages) that needs to be translated to Spanish and French. You notice that the pretrained model in Google’s Translation AI is good, but there are a few hundred domain‐specific terms that are not translated in the way you want. You don't have any labeled data and you have only a few professional translators in your company. What do you do?\n     1. Use Google's translate service and then have a human in the loop (HITL) to fix each translation.\n     2. Use Google AutoML Translation to create a new translation model for your case.\n     3. Use Google's Translation AI with a “glossary” of the terms you need.\n     4. Not possible to translate because you don't seem to have data.\n  3. You are working with a thousand hours of video recordings in Spanish and need to create subtitles in English and French. You already have a small dataset with hundreds of hours of video for which subtitles have been created manually. What is your first approach?\n     1. There is no “translated subtitle” service so use AutoML to create a “subtitle” job using the existing dataset and then use that model to create translated subtitles.\n     2. There is no “translated subtitle” service, and there is no AutoML for this so you have to create a custom model using the data and run it on GPUs.\n     3. There is no “translated subtitle” service, and there is no AutoML for this so you have to create a custom model using the data and run it on TPUs.\n     4. Use the pretrained Speech‐to‐Text (STT) service and then use the pretrained Google Translate service to translate the text and insert the subtitles.\n  4. You want to build a mobile app to classify the different kinds of insects. You have enough labeled data to train but you want to go to market quickly. How would you design this?\n     1. Use AutoML to train a classification model, with AutoML Edge as the method. Create an Android app using ML Kit and deploy the model to the edge device.\n     2. Use AutoML to train a classification model, with AutoML Edge as the method. Use a Coral.ai device that has edge TPU and deploy the model on that device.\n     3. Use AutoML to train an object detection model with AutoML Edge as the method. Use a Coral.ai device that has edge TPU and deploy the model on that device.\n     4. Use AutoML to train an image segmentation model, with AutoML Edge as the method. Create an Android app using ML Kit and deploy the model to the edge device.\n  5. You are training a deep learning model for object detection. It is taking too long to converge, so you are trying to speed up the training. While you are trying to launch an instance (with GPU) with Deep Learning VM Image, you get an error that the “NVIDIA_TESLA_V100 was not found.” What could be the problem?\n     1. GPU was not available in the selected region.\n     2. GPU quota was not sufficient.\n     3. Preemptible GPU quota was not sufficient.\n     4. GPU did not have enough memory.\n  6. Your team is building a convolutional neural network for an image segmentation problem on‐prem on a CPU‐only machine. It takes a long time to train, so you want to speed up the process by moving to the cloud. You experiment with VMs on Google Cloud to use better hardware. You do not have any code for manual placements and have not used any custom transforms. What hardware should you use?\n     1. A deep learning VM with n1‐standard‐2 machine with 1 GPU\n     2. A deep learning VM with more powerful e2‐highCPU‐16 machines\n     3. A VM with 8 GPUs\n     4. A VM with 1 TPU\n  7. You work for a hardware retail store and have a website where you get thousands of users on a daily basis. You want to display recommendations on the home page for your users, using Recommendations AI. What model would you choose?\n     1. “Others you may like”\n     2. “Frequently bought together”\n     3. “Similar items”\n     4. “Recommended for you”\n  8. You work for a hardware retail store and have a website where you get thousands of users on a daily basis. You want to increase your revenue by showing recommendations while customers check out. What type of model in Recommendations AI would you choose?\n     1. “Others you may like”\n     2. “Frequently bought together”\n     3. “Similar items”\n     4. “Recommended for you”\n  9. You work for a hardware retail store and have a website where you get thousands of users on a daily basis. You have a customer's browsing history and want to engage the customer more.\n\nWhat model in Recommendations AI would you choose?\n\n     1. “Others you may like”\n     2. “Frequently bought together”\n     3. “Similar items”\n     4. “Recommended for you”\n  10. You work for a hardware retail store and have a website where you get thousands of users on a daily basis. You do not have browsing events data. What type of model in Recommendations AI would you choose?\n     1. “Others you may like”\n     2. “Frequently bought together”\n     3. “Similar items”\n     4. “Recommended for you”\n  11. You work for a hardware retail store and have a website where you get thousands of users on a daily basis. You want to show details to increase cart size. You are going to use Recommendations AI for this. What model and optimization do you choose?\n     1. “Others you may like” with “click‐through rate” as the objective\n     2. “Frequently bought together” with “revenue per order” as the objective\n     3. “Similar items” with “revenue per order” as the objective\n     4. “Recommended for you” with “revenue per order” as the objective\n  12. You are building a custom deep learning neural network model in Keras that will summarize a large document into a 50‐word summary. You want to try different architectures and compare the metrics and performance. What should you do?\n     1. Create multiple AutoML jobs and compare performance.\n     2. Use Cloud Composer to automate multiple jobs.\n     3. Use the pretrained Natural Language API first.\n     4. Run multiple jobs on the AI platform and compare results.\n  13. You are building a sentiment analysis tool that collates the sentiment of all customer calls to the call center. The management is looking for something to measure the sentiment; it does not have to be super accurate, but it needs to be quick. What do you think is the best approach for this?\n     1. Use the pretrained Natural Language API to predict sentiment.\n     2. Use Speech‐to‐Text (STT) and then pass through the pretrained Natural Language API to predict sentiment.\n     3. Build a custom model to predict the sentiment directly from voice calls, which captures the intonation.\n     4. Convert Speech‐to‐Text and extract sentiment using BERT algorithm.\n  14. You have built a very large deep learning model using some custom TensorFlow operations written in C++ for object tracking in videos. Your model has been tested on CPU and now you want to speed up training. What would you do?\n     1. Use TPU‐v4 in default setting because it involves using very large matrix operations.\n     2. Customize the TPU‐v4 size to match with the video and recompile the custom TensorFlow operations for TPU.\n     3. Use GPU instances because TPUs do not support custom operations.\n     4. You cannot use GPU or TPU because neither supports custom operations.\n  15. You want to use GPUs for training your models that need about 50 GB of memory. What hardware options do you have?\n     1. n1‐standard‐64 with 8 NVIDIA_TESLA_P100\n     2. e2‐standard‐32 with 4 NVIDIA_TESLA_P100\n     3. n1‐standard‐32 with 3 NVIDIA_TESLA_P100\n     4. n2d‐standard‐32 with 4 NVIDIA_TESLA_P100\n  16. You have built a deep neural network model to translate voice in real‐time cloud TPUs and now you want to push it to your end device. What is the best option?\n     1. Push the model to the end device running Edge TPU.\n     2. Models built on TPUs cannot be pushed to the edge. The model has to be recompiled before deployment to the edge.\n     3. Push the model to any Android device.\n     4. Use ML Kit to reduce the size of the model to push the model to any Android device.\n  17. You want to use cloud TPUs and are looking at all options. Which of the below are valid options? (Choose two.)\n     1. A single TPU VM\n     2. An HPC cluster of instances with TPU\n     3. A TPU Pod or slice\n     4. An instance with both TPU and GPU to give additional boost\n  18. You want to train a very large deep learning TensorFlow model (more than 100 GB) on a dataset that has a matrix in which most values are zero. You do not have any custom TensorFlow operations and have optimized the training loop to not have an I/O operation. What are your options?\n     1. Use a TPU because you do not have any custom TensorFlow operations.\n     2. Use a TPU Pod because the size of the model is very large.\n     3. Use a GPU.\n     4. Use an appropriately sized TPUv4 slice.\n  19. You have been tasked to use machine learning to precisely predict the amount of liquid (down to the milliliter) in a large tank based on pictures of the tank. You have decided to use a large deep learning TensorFlow model. The model is more than 100 GB and trained on a dataset that is very large. You do not have any custom TensorFlow operations and have optimized the training loop to not have I/O operations. What are your options?\n     1. Use a TPU because you do not have any custom TensorFlow operations.\n     2. Use a TPU Pod because the size of the model is very large.\n     3. Use a GPU.\n     4. Use TPU‐v4 of appropriate size and shape for the use case.\n  20. You are a data scientist trying to build a model to estimate the energy usage of houses based on photos, year built, and so on. You have built a custom model and deployed this custom container in Vertex AI. Your application is a big hit with home buyers who are using it to predict energy costs for houses before buying. You are now getting complaints that the latency is too high. To fix the latency problem, you deploy the model on a bigger instance (32‐core) but the latency is still high. What is your next step? (Choose two.)\n     1. Increase the size of the instance.\n     2. Use a GPU instance for prediction.\n     3. Deploy the model on a computer engine instance and test the memory and CPU usage.\n     4. Check the code to see if this is single‐threaded and other software configurations for any bugs.",
        "subsections": {},
        "summary": "* **Machine Learning Model in Photos**\n    * Use AutoML to create object detection with Vision AI or combine AutoML and a custom model for better results.\n    * Google's Translation AI can be used with a glossary of domain-specific terms, but may not provide accurate results without labeled data.\n    * To speed up the process of creating subtitles, use AutoML's subtitle job using an existing dataset or create a custom model using GPUs.\n\n* **Recommendations AI for Retail Store**\n    * Choose the \"Recommended for you\" model to display personalized recommendations on the home page.\n    * For increasing revenue during checkout, choose the \"Recommended for you\" model with a focus on high-value items.\n\n* **Training Large Deep Learning Model**\n    * To speed up training of large deep learning models, use TPUs or GPUs with sufficient memory and processing power.\n    * Consider using TPU Pod or slice for very large models.\n\n* **Deploying Custom Container in Vertex AI**\n    * Increase the size of the instance to improve performance.\n    * Test the model on a computer engine instance to identify memory and CPU usage issues."
      }
    },
    "summary": ""
  },
  "Chapter 5Architecting ML Solutions": {
    "content": "",
    "subsections": {
      "Designing Reliable, Scalable, and Highly Available ML Solutions": {
        "content": "To design highly reliable, scalable, and available ML solutions, you need to think through how to automate and orchestrate the various steps of an AI/ML pipeline.\n\nYour ML pipeline has the following steps:\n\n  1. Data collection\n  2. Data transform\n  3. Model training\n  4. Model tuning\n  5. Model deploying\n  6. Model monitoring\n\nMoreover, this process is an iterative process because you might need to retrain the model based on your metrics of evaluation (accuracy, confidence score, etc.).\n\nYou need a scalable storage solution to manage and store your data—for example, Google Cloud Storage. Then you need a scalable infrastructure to run transform jobs, such as, for example, running PySpark transform jobs on Google Cloud Dataflow. Once you have transformed your data, you need scalable compute to train your model. To create large models that cannot fit into your laptops, you need distributed training.\n\nFor custom model training (TensorFlow, Scikit, PyTorch, or other frameworks), you can use Google Vertex AI training because you do not have to worry about spinning up infrastructure for training and managing that infrastructure to stop when the training is over. You also have the choice to use Vertex AI AutoML and Vertex AI APIs in case you are not doing custom training using TensorFlow, Scikit, or PyTorch. Once the training is over, you might want to tune your model with various hyperparameters by running 100 of such training jobs having combinations of the various hyperparameters from your search space. This is where you can use Vertex AI hyperparameter tuning to automate running multiple training jobs in a scalable manner. You would also want to track these variables and your multiple training results in a managed, scalable, and reliable manner. This is where Vertex AI Experiments can help track the parameters of your training runs as well as the results. Since you are able to track your experiments, this helps you with faster model selection.\n\nFinally, once you pick the best training job with the best accuracy, you have to think of deploying this model in production. You also need to worry about scaling your production model; Vertex AI Prediction can help scale your prediction endpoints in production in a fully managed way.\n\nOnce your model is deployed in production, you need to think about monitoring the hosted model to check for any kind of model drift. This is where Vertex AI Model Monitoring will help. You also need to think about reproducing this complete pipeline from data collection to model deployment because ML is an iterative process. Vertex AI Pipelines can help here.\n\nTable 5.1 summarizes the services we have mentioned for architecting a highly available scalable managed solution.\n\n**TABLE 5.1** ML workflow to GCP services mapping\n\nML Workflow | Google Cloud Service\n---|---\nData collection | Google Cloud storage, Pub/Sub (streaming data), BigQuery\nData transformation | Dataflow\nModel training | Custom models (Vertex AI Training and Vertex AutoML)\nTuning and experiment tracking | Vertex AI hyperparameter tuning and Vertex AI Experiments\nDeployment and monitoring | Vertex AI Prediction and Vertex AI Model Monitoring\nOrchestration and CI/CD | Vertex AI Pipelines\nExplanations and responsible AI | Vertex Explainable AI, model cards",
        "subsections": {},
        "summary": "* **ML Pipeline**:\n  * Data collection: Google Cloud storage, Pub/Sub (streaming data), BigQuery\n  * Data transformation: Dataflow\n  * Model training: Custom models (Vertex AI Training and Vertex AutoML)\n  * Tuning and experiment tracking: Vertex AI hyperparameter tuning and Vertex AI Experiments\n* **Automating the pipeline**: \n  * Orchestration and CI/CD: Vertex AI Pipelines\n  * Model deployment and monitoring: Vertex AI Prediction and Vertex AI Model Monitoring"
      },
      "Choosing an Appropriate ML Service": {
        "content": "Google Cloud ML services and solutions are divided into three layers based on ease of use and implementation, as shown in Figure 5.1.\n\n**FIGURE 5.1** Google AI/ML stack\n\nThe top layer are AI solutions such as Document AI, Contact Center AI, and Enterprise Translation Hub. These are managed Software as a Service (SaaS) offerings, which are easier to implement and manage with no code. The AI solutions are built on top of the middle layer of Vertex AI services.\n\nThe middle layer consists of Vertex AI, which includes the following:\n\n  * Vertex AI pretrained APIs for most common use cases in sight, language, conversation, and structured data. The APIs are serverless and scalable.\n  * Vertex AI AutoML for enriching the vertex AI API use cases with your own data to create models specific to your business use case. For example, you can train an AutoML Vision model by providing some examples of your company logo to detect images of trucks with your company logo on the road.\n  * Vertex AI Workbench, which is a development environment for the entire data science workflow. This can range from data labeling, training, tuning, deploying, and monitoring your own custom model, which does not fit any use case of using AI services and AutoML. An example is building a text summarization model.\n\nThe bottom layer consists of infrastructure such as a compute instance and containers (Google Kubernetes Engine) with a choice of TPUs, GPUs, and storage. You would need to manage the infrastructure yourself for scalability and reliability.\n\nGoogle Cloud provides BigQuery ML and Vertex AI AutoML to help automate creation of custom models for some common problem domains. The rest of the tools provided by Vertex AI help you build fully custom models for any problem domain. We are going to cover BigQuery ML in detail in Chapter 14, “BigQuery ML.”. Table 5.2 discusses when to use AutoML versus BigQuery ML versus a Vertex AI custom model.\n\n**TABLE 5.2** When to use BigQuery ML vs. AutoML vs. a custom model\n\nGCP Service | When to Use\n---|---\nBigQuery ML | You have structured data stored in a BigQuery data warehouse because BigQuery ML requires a tabular dataset. You are comfortable with SQL and the models available in BigQuery ML match the problem you are trying to solve. We are going to cover all the models in Chapter 14.\nAutoML (in the context of Vertex AI) | Your problem fits into one of the types that AutoML supports, such as classification, object detection, sentiment analysis, and translation.\nYour data (text, video, images, and tabular) matches the format and fits within the limits set by each type of AutoML model.\nVertex AI custom‐trained models | Your problem does not match the criteria listed in this table for BigQuery ML or AutoML.\nYou are already running training on‐premises or on another cloud platform, and you need consistency across the platforms.\n\n* * *\n\nYou can train AutoML tabular models from the BigQuery ML because BigQuery supports a tabular data environment. You can also get a custom‐trained TensorFlow model and use it in BigQuery ML on tabular data stored in BigQuery tables.\n\n* * *",
        "subsections": {},
        "summary": "* **Google Cloud ML Services Layers**\n  * _Top Layer: AI Solutions (Managed SaaS)_\n    * Document AI, Contact Center AI, Enterprise Translation Hub\n  * _Middle Layer: Vertex AI Services_\n    * Pretrained APIs for common use cases\n    * AutoML for enriching models with business data\n    * Workbench for custom model development\n  * _Bottom Layer: Infrastructure (Managed by User)_\n    * Compute instance and containers"
      },
      "Data Collection and Data Management": {
        "content": "Google Cloud provides several data stores to handle your combination of latency, load, throughput, and size requirements for features:\n\n  * Google Cloud Storage\n  * BigQuery\n  * `Vertex AI's datasets` to manage training and annotation sets\n  * Vertex AI Feature Store\n  * NoSQL data store",
        "subsections": {
          "Google Cloud Storage (GCS)": {
            "content": "Google Cloud Storage is a service for storing your objects in Google Cloud. You can use GCS for storing image, video, audio, and unstructured data. You can combine these individual data types into large files of size at least 100 MB (https://cloud.google.com/architecture/ml‐on‐gcp‐best‐practices#store‐image‐video‐audio‐and‐unstructured‐data‐on‐cloud‐storage) and in between 100 to 10,000 shards to improve read and write throughput. This applies to sharded TFRecord files if you're using TensorFlow or Avro files if you're using any other framework.",
            "subsections": {},
            "summary": "* _Google Cloud Storage (GCS)_ is a service for storing various types of data\n    * Supports images, videos, audio, and unstructured data\n        * Can combine these into large files >= 100 MB with up to 10,000 shards for improved read/write throughput"
          },
          "BigQuery": {
            "content": "The best practice is to store tabular data in BigQuery. For training data it's better to store the data as tables instead of views for better speed. BigQuery functionality is available by using the following:\n\n  * The Google Cloud console, search for BigQuery\n  * The `bq` command‐line tool\n  * The BigQuery REST API\n  * Vertex AI Jupyter Notebooks using BigQuery Magic or BigQuery Python client.\n\nWe are going to cover BigQuery ML in Chapter 14.\n\nTable 5.3 lists Google Cloud tools that make it easier to use the API.\n\n**TABLE 5.3** Google Cloud tools to read BigQuery data\n\nFramework | Google Cloud tool to read data from BigQuery\n---|---\nTensorFlow or Keras | `tf.data.dataset reader for BigQuery` and tfio.BigQuery.BigQueryClient()\n(`www.tensorflow.org/io/api_docs/python/tfio/BigQuery/BigQueryClient`)\nTFX | `BigQuery client`\nDataflow | `BigQuery I/O connector`\nAny other framework | `BigQuery Python Client library`",
            "subsections": {},
            "summary": "* **Store data in BigQuery**: Use BigQuery for tabular data storage, especially for training data.\n    * _Use tables instead of views_ to improve query speed.\n* **Access BigQuery functionality**:\n  * Google Cloud console: Search for BigQuery\n  * `bq` command-line tool\n  * BigQuery REST API\n  * Vertex AI Jupyter Notebooks with BigQuery Magic or Python client"
          },
          "Vertex AI Managed Datasets": {
            "content": "Google Cloud recommends using Vertex AI managed datasets to train custom models instead of writing your training application to ingest training data directly from storage such as Google Cloud Storage or from local storage. Primarily four data formats are supported: image, video, tabular (CSV, BigQuery tables), and text.\n\nThe advantages of using managed storage are as follows:\n\n  * Manage datasets in a central location.\n  * Integrated data labeling for unlabeled unstructured data such as video, text, and images using Vertex AI data labeling.\n  * Easy to track lineage to models for governance and iterative development.\n  * Compare model performance by training AutoML and custom models using the same datasets.\n  * Generate data statistics and visualizations.\n  * Automatically split data into training, test, and validation sets.\n\n* * *\n\nManaged datasets are not required if you want more control over splitting your data in your training code or if lineage between your data and model isn't critical to your application.\n\n* * *\n\nWhen you have unlabeled and unstructured data, you can use the Vertex AI data labeling service to label the data in Google Cloud Storage or Vertex AI–managed datasets. This is a service just to label the data and it does not store data. You can use third‐party crowd‐sourced human labelers or your own labelers to label the data.",
            "subsections": {},
            "summary": "* **Use managed datasets for custom models**\n  * Manage datasets in a central location.\n  * Easily track lineage and compare model performance\n* *_Alternative:_* Use your own storage and control over data splitting if needed.\n* *_Data labeling:_* Use Vertex AI's data labeling service to label unlabeled and unstructured data."
          },
          "Vertex AI Feature Store": {
            "content": "Vertex AI Feature Store is a fully managed centralized repository for organizing, storing, and serving ML features. You can use Vertex AI Feature Store independently or as part of Vertex AI workflows. For example, you can fetch data from Vertex AI Feature Store to train custom or AutoML models in Vertex AI. When you're training a model with structured data, users can define features and then ingest (import) feature values from various data sources. Any permitted user can search and retrieve values from the Feature Store. For example, you can find features and then do a batch export to get training data for ML model creation or you can create a new feature if the feature does not exist. You can also retrieve feature values in real time to perform fast online predictions.\n\nThe benefit of using Feature Store is that you do not have to compute feature values and save them in various locations such as in tables in BigQuery and as files in Google Cloud Storage. Moreover, Feature Store can help detect drifts and mitigate data skew because features are created in a centralized manner.",
            "subsections": {},
            "summary": "* **Vertex AI Feature Store**: A fully managed centralized repository for ML features\n* *_Benefits_*:\n\t+ No need to compute feature values and save them in various locations\n\t+ Helps detect drifts and mitigate data skew due to centralized creation of features\n\t+ Enables fast online predictions with real-time retrieval of feature values"
          },
          "NoSQL Data Store": {
            "content": "For static feature lookup during prediction, analytical data stores such as BigQuery are not engineered for low‐latency singleton read operations such as where the result is a single row with many columns. An example of a query like this is “Select 100 columns from several tables for a specific customer ID.” Therefore, static reference features are collected, prepared, and stored in a NoSQL database that is optimized for singleton lookup operations.\n\nTable 5.4 shows the NoSQL data store options for managed data stores in Google Cloud. As ML specialists, we have worked with Memorystore for use cases needing submillisecond latency. We have worked with data stores in use cases to store user login details in data stores where latency can be in milliseconds and Bigtable for millisecond latency with dynamic changing data.\n\n**TABLE 5.4** NoSQL data store options\n\n****|  Memorystore | Datastore | Bigtable\n---|---|---|---\nDescription | Memorystore is a managed in‐memory database. When you use its Redis offering, you can store intermediate data for submillisecond read access. Keys are binary‐safe strings, and values can be of different data structures. | Datastore is a fully managed, scalable NoSQL document database built for automatic scaling, high performance, and ease of application development. Data objects in Datastore are known as entities. An entity has one or more named properties in which you store the feature values required by your model or models. | Bigtable is a massively scalable NoSQL database service engineered for high throughput and for low‐latency workloads. It can handle petabytes of data, with millions of reads and writes per second at a latency that's on the order of milliseconds. The data is structured as a sorted key‐value map. Bigtable scales linearly with the number of nodes.\nRetrieval | Submillisecond retrieval latency on a limited amount of quickly changing data, retrieved by a few thousand clients. | Millisecond retrieval latency on slowly changing data where storage scales automatically. | Millisecond retrieval latency on dynamically changing data, using a store that can scale linearly with heavy reads and writes.\nUse cases | User‐feature lookup in real‐time bidding that requires submillisecond retrieval time.\nMedia and gaming applications that use precomputed predictions.\nStoring intermediate data for a real‐time data pipeline for creating input features. | Product recommendation system on an e‐commerce site that's based on information about logged‐in users. | Fraud detection that leverages dynamically aggregated values. Applications in Fintech and Adtech are usually subject to heavy reads and writes.\nAd prediction that leverages dynamically aggregated values over all ad requests and historical data.\nBooking recommendation based on the overall customer base's recent bookings.\n\n* * *\n\nYou should avoid storing data in block storage such as a Network File System (NFS) or a virtual machine (VM) hard disk as it's harder to manage them and tune performance than in Google Cloud Storage or BigQuery. Also avoid reading data directly from databases such as Cloud SQL; instead, store data in BigQuery, Google Cloud Storage, or a NoSQL data store for performance.\n\n* * *",
            "subsections": {},
            "summary": "### Static Feature Lookup\n\nFor static feature lookup during prediction, use:\n* **NoSQL database**: Optimized for singleton lookup operations\n* * _Use Memorystore_, *_Datastore_ or *_Bigtable_* *\n* _Avoid block storage_ such as NFS or VM hard disk, and _avoid reading from databases directly_"
          }
        },
        "summary": "* _Data Stores in Google Cloud_\n    * **Google Cloud Storage**: object-based storage for large files and blobs\n    * *_NoSQL Data Store_*: flexible schema, high scalability, and performance\n        * **Vertex AI's datasets**: manage training and annotation sets for machine learning\n            * **Vertex AI Feature Store**: stores and manages model artifacts and features"
      },
      "Automation and Orchestration": {
        "content": "Machine learning workflows define which phases are implemented during a machine learning project. The typical phases include data collection, data preprocessing, building datasets, model training and refinement, evaluation, and deployment to production. To integrate an ML system in a production environment, you need to orchestrate the steps in your ML pipeline. In addition, you need to automate the execution of the pipeline for the continuous training of your models.\n\nML pipelines are there to connect the various steps of your ML solution. Kubeflow is a machine learning toolkit that provides a pipeline solution called Kubeflow Pipelines, built atop Kubernetes. Google introduced Vertex AI Pipelines because maintaining Kubernetes can be challenging and time‐intensive. It's a serverless product to run pipelines, so your machine learning team can focus on what they're there to do: ML.",
        "subsections": {
          "Use Vertex AI Pipelines to Orchestrate the ML Workflow": {
            "content": "Vertex AI Pipelines is a managed service that helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts such as training data, hyperparameters, and code that were used to create the model. Vertex AI Pipelines can run pipelines built using the Kubeflow Pipelines SDK v1.8.9 or higher or TensorFlow Extended v0.30.0 or higher.\n\nFor TensorFlow, use TensorFlow Extended to define your pipeline and the operations for each step, then execute it on Vertex AI's serverless pipelines system.\n\nFor all other frameworks, use Kubeflow Pipelines with Vertex AI Pipelines. Use Vertex AI to launch and interact with the platform.\n\n* * *\n\nManaged pipeline steps can be `calls to a Google Cloud service`. Vertex AI Pipelines supports experiments, which is GA now in Google Cloud Platform. Kubeflow Pipelines already supports Kubeflow experiments. So you might get questions on the exam on experiment tracking; Kubeflow will be the correct answer because exam questions were written keeping in mind GA services.\n\n* * *\n\nThis has two outcomes:\n\n  * You can use pipelines regardless of the ML environment you choose.\n  * You need a small number of nodes with modest CPU and RAM since most work will happen within a managed service.",
            "subsections": {},
            "summary": "**Vertex AI Pipelines**\nManaged service for automating, monitoring, and governing ML systems\n* Orchestrates ML workflow in serverless manner\n* Stores workflow artifacts using Vertex ML Metadata\n* Supports Kubeflow Pipelines SDK v1.8.9 and higher or TensorFlow Extended v0.30.0 and higher\n\n**Key Benefits**\n\n* Simplifies pipeline management across different ML environments\n* Reduces the need for on-premises infrastructure with a small number of nodes"
          },
          "Use Kubeflow Pipelines for Flexible Pipeline Construction": {
            "content": "`Kubeflow` is an open source `Kubernetes` framework for developing and running portable ML workloads. `Kubeflow Pipelines` is a Kubeflow service that lets you compose, orchestrate, and automate ML systems. You can choose to deploy your Kubernetes workloads locally, on‐premises, or to a cloud environment such as Google Cloud or other cloud platforms. Kubeflow Pipelines SDK is recommended for most users who want to author managed pipelines. Kubeflow Pipelines is flexible, letting you use simple code to construct pipelines, and it provides Google Cloud Pipeline Components, which lets you include Vertex AI functionality like AutoML in your pipeline.\n\nKubeflow Pipelines lets you orchestrate and automate a production ML pipeline by executing the required Google Cloud services. In Figure 5.2, Cloud SQL serves as the ML metadata store for Kubeflow Pipelines.\n\n**FIGURE 5.2** Kubeflow Pipelines and Google Cloud managed services\n\nKubeflow Pipelines components aren't limited to executing TensorFlow Extended (TFX)–related services on Google Cloud. These components can execute `Dataproc` for Spark ML jobs, `AutoML`, and other compute workloads.",
            "subsections": {},
            "summary": "* _Kubeflow_ is an open source Kubernetes framework for developing and running portable machine learning (ML) workloads.\n    * **Kubeflow Pipelines** lets you compose, orchestrate, and automate ML systems using a flexible and user-friendly approach.\n        * Supports deployment on-premises, locally, or in the cloud with services like Google Cloud."
          },
          "Use TensorFlow Extended SDK to Leverage Pre‐built Components for Common Steps": {
            "content": "TensorFlow provides prebuilt components for common steps in the Vertex AI workflow like data ingestion, data validation, and training. TFX provides a bunch of frameworks, libraries, and components for defining, launching, and monitoring machine learning models in production. TensorFlow Extended SDK is recommended if any of the following is true:\n\n  * You already use TensorFlow.\n  * You use structured and textual data.\n  * You work with a lot of data.\n\nWe covered Google Cloud implementation of a TFX pipeline in Chapter 3, “Feature Engineering.” See also this page:\n\n`https://neptune.ai/blog/deep-dive-into-ml-models-in-production-using-tfx-and-kubeflow`",
            "subsections": {},
            "summary": "* **TFX vs TensorFlow Extended SDK**: \n  * TFX for general machine learning model deployment\n  * TensorFlow Extended SDK when using:\n    * _Structured data_\n    * _Large datasets_\n    * Existing TensorFlow usage"
          },
          "When to Use Which Pipeline": {
            "content": "Vertex AI Pipelines can run pipelines built using the Kubeflow Pipelines SDK v1.8.9 or higher or TensorFlow Extended v0.30.0 or higher.\n\nIf you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX. By default, TFX creates a directed acyclic graph (DAG) of your ML pipeline. It uses `Apache Beam` under the hood for managing and implementing pipelines, and this can be easily executed on distributed processing backends like `Apache Spark`, `Google Cloud Dataflow`, and `Apache Flink`.\n\nWhile TFX running Apache Beam using Cloud Dataflow is cool, it is difficult to configure, monitor, and maintain defined pipelines and workflows. This gave rise to tools we call _orchestrators_.\n\nOrchestrators like Kubeflow make it easy to configure, operate, monitor, and maintain ML pipelines. They mostly come with GUIs that you can easily understand. You can use Kubeflow Pipelines to schedule and orchestrate your TFX pipeline.\n\nFor other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK.\n\nWhile you could consider other orchestrators like `Cloud Composer` (see Apache `Airflow`), Vertex AI Pipelines is a better choice because it includes built‐in support for common ML operations and tracks ML–specific metadata and lineage. Lineage is especially important for validating that your pipelines are operating correctly in production.",
            "subsections": {},
            "summary": "* **Summary of Vertex AI Pipelines**\n  * Supported platforms: Kubeflow Pipelines v1.8.9+, TensorFlow Extended v0.30.0+\n  \n  * _Orchestrators_ like Kubeflow make ML pipeline management easier with GUIs and scheduling capabilities\n  * Recommended for TFX users to simplify pipeline creation, operation, and maintenance\n  \n  * Vertex AI Pipelines provide built-in support for common ML operations and lineage tracking"
          }
        },
        "summary": "* **Machine Learning Workflow**: Machine learning workflows define the phases of a project, including data collection, model training, evaluation, and deployment.\n* _Key Integration Challenge_: Integrating an ML system into a production environment requires orchestrating pipeline steps and automating execution for continuous model training.\n* * **Pipeline Solutions**: Kubeflow Pipelines and Vertex AI Pipelines offer automated pipeline solutions to streamline machine learning workflows."
      },
      "Serving": {
        "content": "After you train, evaluate, and tune a machine learning (ML) model, the model is deployed to production for predictions. An ML model can provide predictions in two ways: offline prediction and online prediction.",
        "subsections": {
          "Offline or Batch Prediction": {
            "content": "You perform offline or batch prediction when you are getting your data in batches and you run a batch job pointing to the trained model to predict offline. Some of the use cases for offline or batch processing can be recommendations, demand forecasting, segment analysis, and classifying large batches of text to determine the topic to which they belong. You can use Vertex AI batch prediction to run a batch prediction job for your data stored in BigQuery or Google Cloud Storage.\n\nFigure 5.3 shows a typical high‐level architecture on Google Cloud for performing offline batch prediction.\n\n**FIGURE 5.3** Google Cloud architecture for performing offline batch prediction",
            "subsections": {},
            "summary": "* **Offline Batch Prediction**: Performs prediction on batches of data without real-time interaction\n* *Use cases:* Recommendations, demand forecasting, segment analysis, and text classification\n* **Vertex AI Integration**: Runs batch prediction jobs on BigQuery or Google Cloud Storage"
          },
          "Online Prediction": {
            "content": "This prediction happens in near real time when you send a request to your deployed model endpoint and you get the predicted response back. This can be a model deployed to an HTTPS endpoint, and you can use microservice architecture to call this endpoint from your web applications or mobile applications. Use cases where you need response in near real time while making ML predictions are real‐time bidding and real‐time sentiment analysis of Twitter feeds.\n\nThere are two ways you can have online predictions, described here:\n\n  * **Synchronous** In this the caller waits until it receives the prediction from the ML service before performing the subsequent steps. You can use Vertex AI online predictions to deploy your model as a real‐time HTTPS endpoint. You can also use App Engine or GKE (Google Kubernetes Engine) as an ML gateway to perform some feature preprocessing before sending your request from client applications, as shown in Figure 5.4.\n\n**FIGURE 5.4** Google Cloud architecture for online prediction\n\n  * **Asynchronous** In this the end user may not query a model endpoint directly. The end user may get notified or may poll a Feature Store or data store for prediction in real time. There are two methods by which this can happen.\n    * **Push** The model generates predictions and pushes them to the end user as a notification. For example, in fraud detection, you would use this method when you want to notify other systems to take action when a potentially fraudulent transaction is identified. See Figure 5.5 for reference.\n    * **Poll** The model generates predictions and stores them in a low latency read database such as a NoSQL data store. The end user periodically polls the database for available predictions. An example is targeted marketing, where the system checks the propensity scores predicted in real time for active customers in order to decide whether to send an email with a promotion or a retention offer.\n\n**FIGURE 5.5** Push notification architecture for online prediction\n\nMinimizing latency to serve prediction is important for real‐time use cases. This can happen at these two levels:\n\n  * **Minimize latency at the model level:** You need to minimize the time your model takes to make a prediction when invoked with a request. You can minimize latency by building a smaller model because it will have less neural network layers and will need less compute to process the prediction.\n\nYou can use accelerators such as Cloud GPU and `TPU` to serve your model endpoint. Typically, you use TPUs only when you have large deep‐learning models and large batch sizes.\n\n  * **Minimize latency at the serving level:** This is where you minimize the time your system takes to serve the prediction when it receives the request. This includes storing your input features in a low latency read lookup data store, precomputing predictions in an offline batch‐scoring job, and caching the predictions.\n\n* * *\n\nFor training TensorFlow models, you can optimize the SavedModel using the Graph Transformation Tools. Refer to the blog “Optimizing TensorFlow Models for Serving” at `https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf`.\n\n* * *",
            "subsections": {},
            "summary": "**Online Predictions**\n\n* **Synchronous**: Caller waits for prediction from ML service before performing subsequent steps.\n  * Deploy model as HTTPS endpoint using Vertex AI\n  * Use App Engine or GKE as ML gateway for feature preprocessing\n\n* **Asynchronous**\n  * **Push**: Model generates predictions and notifies end user directly (e.g. fraud detection)\n    * Example: Google Cloud architecture for push notifications\n  * **Poll**: Model stores predictions in low-latency read database, end user polls periodically (e.g. targeted marketing)\n    * Example: Google Cloud architecture for poll-based online prediction\n\n**Minimizing Latency**\n\n* **Model Level**: Minimize time taken by model to make prediction when invoked with request\n  * Build smaller model with fewer neural network layers and less compute required\n* **Serving Level**: Minimize time taken by system to serve prediction when receiving request\n  * Store input features in low-latency read lookup data store\n  * Precompute predictions in offline batch-scoring job"
          }
        },
        "summary": "* **Prediction Methods**: Machine learning models can make predictions in either *_offline_* or *_online_* modes.\n* _Offline_ mode makes predictions using pre-trained data\n* _Online_ mode makes predictions in real-time, adjusting to new data as it becomes available"
      },
      "Summary": {
        "content": "In this chapter, we discussed best practices for designing a reliable, scalable, and highly available ML solution on Google Cloud Platform (GCP). Then we discussed when to use which service from the three layers of the GCP AI/ML stack. We covered data collection and data management strategy for managing data in the integrated Vertex AI platform with BigQuery. We also covered data storage options for submillisecond and millisecond latency such as NoSQL data store. Then we covered automation and orchestration techniques for ML pipelines such as Vertex AI Pipelines, Kubeflow Pipelines, and TFX pipelines.\n\nWe discussed how you can serve the model using both batch mode and real time to serve the predictions. We covered a few architecture patterns to create batch predictions as well as online predictions using Vertex AI Prediction. Last, we discussed some ways to improve model latency while using real‐time serving.",
        "subsections": {},
        "summary": "* **Designing an ML Solution on GCP**\n\t+ Reliable, scalable, and highly available solutions\n\t+ Choosing the right service from the three layers of the GCP AI/ML stack\n* **Data Management and Storage**\n\t+ Data collection and management strategy for Vertex AI platform with BigQuery\n\t+ NoSQL data store options for submillisecond and millisecond latency\n* **Automation and Orchestration**\n\t+ ML pipeline automation techniques (Vertex AI Pipelines, Kubeflow Pipelines, TFX pipelines)"
      },
      "Exam Essentials": {
        "content": "* **Design reliable, scalable, and highly available ML solutions.** Understand why you need to design a scalable solution and how Google Cloud AI/ML services can help architect a scalable and highly available solution.\n  * **Choose an appropriate ML service.** Understand the AI/ML stack of GCP and when to use each layer of the stack based on your use case and expertise with ML.\n  * **Understand data collection and management.** Understand various types of data stores for storing your data for various ML use cases.\n  * **Know how to implement automation and orchestration.** Know when to use Vertex AI Pipelines vs. Kubeflow vs. TFX pipelines. We will cover the details in Chapter 11, “Designing ML Training Pipelines.”\n  * **Understand how to best serve data.** You need to understand the best practices when deploying models. Know when to use batch prediction versus real‐time prediction and how to manage latency with online real‐time prediction.",
        "subsections": {},
        "summary": "* _Design reliable, scalable, and highly available ML solutions using Google Cloud AI/ML services_\n  * Choose an appropriate ML service based on your use case and expertise\n  * Understand data collection and management, including various types of data stores\n  * Implement automation and orchestration with Vertex AI Pipelines or Kubeflow/TFX pipelines\n  * Deploy models efficiently, balancing batch prediction vs. real-time prediction and managing latency"
      },
      "Review Questions": {
        "content": "1. You work for an online travel agency that also sells advertising placements on its website to other companies. You have been asked to predict the most relevant web banner that a user should see next. Security is important to your company. The model latency requirements are 300ms@p99, the inventory is thousands of web banners, and your exploratory analysis has shown that navigation context is a good predictor.\n\nYou want to implement the simplest solution. How should you configure the prediction pipeline?\n\n     1. Embed the client on the website, and then deploy the model on the Vertex AI platform prediction.\n     2. Embed the client on the website, deploy the gateway on App Engine, and then deploy the model on the Vertex AI platform prediction.\n     3. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on the Vertex AI Prediction.\n     4. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Memorystore for writing and for reading the user's navigation context, and then deploy the model on Google Kubernetes Engine (GKE).\n  2. You are training a TensorFlow model on a structured dataset with 100 billion records stored in several CSV files. You need to improve the input/output execution performance. What should you do?\n     1. Load the data into BigQuery and read the data from BigQuery.\n     2. Load the data into Cloud Bigtable, and read the data from Bigtable.\n     3. Convert the CSV files into shards of TFRecords, and store the data in Google Cloud Storage.\n     4. Convert the CSV files into shards of TFRecords, and store the data in the Hadoop Distributed File System (HDFS).\n  3. You are a data engineer who is building an ML model for a product recommendation system in an e‐commerce site that's based on information about logged‐in users. You will use Pub/Sub to handle incoming requests. You want to store the results for analytics and visualizing. How should you configure the pipeline?\n\nPub/Sub ‐> Preprocess(1) ‐> ML training/serving(2) ‐> Storage(3) ‐> Data studio/Looker studio for visualization\n\n     1. 1 = Dataflow, 2 = Vertex Al platform, 3 = Cloud BigQuery\n     2. 1 = Dataproc, 2 = AutoML, 3 = Cloud Memorystore\n     3. 1 = BigQuery, 2 = AutoML, 3 = Cloud Functions\n     4. 1 = BigQuery, 2 = Vertex Al platform, 3 = Google Cloud Storage\n  4. You are developing models to classify customer support emails. You created models with TensorFlow Estimator using small datasets on your on‐premises system, but you now need to train the models using large datasets to ensure high performance. You will port your models to Google Cloud and want to minimize code refactoring and infrastructure overhead for easier migration from on‐prem to cloud. What should you do?\n     1. Use the Vertex AI platform for distributed training.\n     2. Create a cluster on Dataproc for training.\n     3. Create a managed instance group with autoscaling.\n     4. Use Kubeflow Pipelines to train on a Google Kubernetes Engine cluster.\n  5. You are a CTO wanting to implement a scalable solution on Google Cloud to digitize documents such as PDF files and Word DOC files in various silos. You are also looking for storage recommendations for storing the documents in a data lake. Which options have the least infrastructure efforts? (Choose two.)\n     1. Use the Document AI solution.\n     2. Use Vision AI OCR to digitize the documents.\n     3. Use Google Cloud Storage to store documents.\n     4. Use Cloud Bigtable to store documents.\n     5. Use a custom Vertex AI model to build a document processing pipeline.\n  6. You work for a public transportation company and need to build a model to estimate delay for multiple transportation routes. Predictions are served directly to users in an app in real time. Because different seasons and population increases impact the data relevance, you will retrain the model every month. You want to follow Google‐recommended best practices. How should you configure the end‐to‐end architecture of the predictive model?\n     1. Configure Kubeflow Pipelines to schedule your multistep workflow from training to deploying your model.\n     2. Use a model trained and deployed on BigQuery ML and trigger retraining with the scheduled query feature in BigQuery.\n     3. Write a Cloud Functions script that launches a training and deploying job on the Vertex AI platform that is triggered by Cloud Scheduler.\n     4. Use Cloud Composer to programmatically schedule a Dataflow job that executes the workflow from training to deploying your model.\n  7. You need to design a customized deep neural network in Keras that will predict customer purchases based on their purchase history. You want to explore model performance using multiple model architectures, store training data, and be able to compare the evaluation metrics in the same dashboard. What should you do?\n     1. Create multiple models using AutoML Tables.\n     2. Automate multiple training runs using Cloud Composer.\n     3. Run multiple training jobs on the Vertex AI platform with similar job names.\n     4. Create an experiment in Kubeflow Pipelines to organize multiple runs.\n  8. You work with a data engineering team that has developed a pipeline to clean your dataset and save it in a Google Cloud Storage bucket. You have created an ML model and want to use the data to refresh your model as soon as new data is available. As part of your CI/CD workflow, you want to automatically run a Kubeflow Pipelines training job on Google Kubernetes Engine (GKE). How should you architect this workflow?\n     1. Configure your pipeline with Dataflow, which saves the files in Google Cloud Storage. After the file is saved, start the training job on a GKE cluster.\n     2. Use App Engine to create a lightweight Python client that continuously polls Google Cloud Storage for new files. As soon as a file arrives, initiate the training job.\n     3. Configure a Google Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a storage bucket. Use a Pub/Sub–triggered Cloud Function to start the training job on a GKE cluster.\n     4. Use Cloud Scheduler to schedule jobs at regular intervals. For the first step of the job, check the time stamp of objects in your Google Cloud Storage bucket. If there are no new files since the last run, abort the job.\n  9. Your data science team needs to rapidly experiment with various features, model architectures, and hyperparameters. They need to track the accuracy metrics for various experiments and use an API to query the metrics over time. What should they use to track and report their experiments while minimizing manual effort?\n     1. Use Kubeflow Pipelines to execute the experiments. Export the metrics file, and query the results using the Kubeflow Pipelines API.\n     2. Use Vertex AI Platform Training to execute the experiments. Write the accuracy metrics to BigQuery, and query the results using the BigQuery API.\n     3. Use Vertex AI Platform Training to execute the experiments. Write the accuracy metrics to Cloud Monitoring, and query the results using the Monitoring API.\n     4. Use Vertex AI Workbench Notebooks to execute the experiments. Collect the results in a shared Google Sheets file, and query the results using the Google Sheets API.\n  10. As the lead ML Engineer for your company, you are responsible for building ML models to digitize scanned customer forms. You have developed a TensorFlow model that converts the scanned images into text and stores them in Google Cloud Storage. You need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention. What should you do?\n     1. Use the batch prediction functionality of the Vertex AI platform.\n     2. Create a serving pipeline in Compute Engine for prediction.\n     3. Use Cloud Functions for prediction each time a new data point is ingested.\n     4. Deploy the model on the Vertex AI platform and create a version of it for online inference.\n  11. As the lead ML architect, you are using TensorFlow and Keras as the machine learning framework and your data is stored in disk files as block storage. You are migrating to Google Cloud and you need to store the data in BigQuery as tabular storage. Which of the following techniques will you use to store TensorFlow storage data from block storage to BigQuery?\n     1. tf.data.dataset reader for BigQuery\n     2. BigQuery Python Client library\n     3. BigQuery I/O Connector\n     4. tf.data.iterator\n  12. As the CTO of the financial company focusing on building AI models for structured datasets, you decide to store most of the data used for ML models in BigQuery. Your team is currently working on TensorFlow and other frameworks. How would they modify code to access BigQuery data to build their models? (Choose three.)\n     1. tf.data.dataset reader for BigQuery\n     2. BigQuery Python Client library\n     3. BigQuery I/O Connector\n     4. BigQuery Omni\n  13. As the chief data scientist of a retail website, you develop many ML models in PyTorch and TensorFlow for Vertex AI Training. You also use Bigtable and Google Cloud Storage. In most cases, the same data is used for multiple models and projects and also updated. What is the best way to organize the data in Vertex AI?\n     1. Vertex AI–managed datasets\n     2. BigQuery\n     3. Vertex AI Feature Store\n     4. CSV\n  14. You are the data scientist team lead and your team is working for a large consulting firm. You are working on an NLP model to classify customer support requests. You are working on data storage strategy to store the data for NLP models. What type of storage should you avoid in a managed GCP environment in Vertex AI? (Choose two.)\n     1. Block storage\n     2. File storage\n     3. BigQuery\n     4. Google Cloud Storage",
        "subsections": {},
        "summary": "* **Embed client on the website, deploy gateway on App Engine, and then deploy model on the Vertex AI platform prediction**: This is the simplest solution.\n    * _Use vertex ai to process predictions_\n    * _Deploy on app engine for faster predictions_\n    * _Simplify deployment with auto scaling_\n\n* **Convert CSV files into shards of TFRecords, and store the data in Google Cloud Storage or Hadoop Distributed File System (HDFS)**: To improve input/output execution performance.\n    * Store in google cloud storage\n    * Use tfrecords to speed up processing\n\n* **Configure Kubeflow Pipelines to schedule your multistep workflow from training to deploying your model**: For real-time prediction and retraining every month.\n    * Schedule with Cloud Scheduler\n    * _Use Kubeflow pipelines for efficient data engineering_\n    * Run on gke cluster\n\n* **Use Vertex AI Workbench Notebooks to execute the experiments. Collect results in a shared Google Sheets file, and query the results using the Google Sheets API**: For rapid experimentation.\n    * Execute with vertex ai workbench\n    * _Collect results in google sheets for easy analysis_\n    * Use Google Sheets API\n\n* **Use batch prediction functionality of the Vertex AI platform to use ML model on aggregated data collected at the end of each day**: With minimal manual intervention.\n    * Batch prediction on vertex ai\n    * _Automate predictions with scheduled jobs_\n\n* **tf.data.dataset reader for BigQuery**: To store TensorFlow storage data from block storage to BigQuery.\n    * _Use tfdata dataset reader for big query_\n    * Avoid using other connectors\n\n* **BigQuery Python Client library or tf.data.dataset reader for BigQuery**: For code modification to access BigQuery data in machine learning frameworks.\n    * Use python client\n    * Or use tfdata reader \n\n* **Vertex AI–managed datasets, Vertex AI Feature Store, CSV**: To organize the data in Vertex AI for multiple models and projects.\n    * Managed vertex ai datasets\n    * _Use feature store for better management_\n    * Avoid using CSV\n\n* **BigQuery or Cloud Storage**: In a managed GCP environment in Vertex AI to avoid storing large unmanaged data sets.\n    * Avoid block storage\n    * Or file storage"
      }
    },
    "summary": ""
  },
  "Chapter 6Building Secure ML Pipelines": {
    "content": "",
    "subsections": {
      "Building Secure ML Systems": {
        "content": "One of the key tasks of any enterprise is to help ensure the security of its users' and employees' data. Google Cloud provides built‐in security measures to facilitate data security, including encryption of stored data and encryption of data in transit. Let's look at what encryption at rest and encryption in transit are for cloud systems.",
        "subsections": {
          "Encryption at Rest": {
            "content": "For machine learning models, your data will be in either Cloud Storage or BigQuery tables. Google encrypts data stored at rest by default for both Cloud Storage and BigQuery. By default, Google manages the encryption keys used to protect your data. You can also use customer‐managed encryption keys. You can encrypt individual table values in BigQuery using Authenticated Encryption with Associated Data (AEAD) encryption functions. Please refer to `https://cloud.google.com/bigquery/docs/reference/standard-sql/aead-encryption-concepts` to understand AEAD BigQuery encryption functions.\n\nTable 6.1 shows the difference between server‐side encryption and client‐side encryption in terms of cloud storage and BigQuery.\n\n* * *\n\nTo protect data from corruption, Google Cloud Storage supports two types of hashes you can use to check the integrity of your data: CRC32C and MD5.\n\n* * *\n\n**TABLE 6.1** Difference between server‐side and client‐side encryption\n\nServer‐Side Encryption | Client‐Side Encryption\n---|---\nEncryption that occurs after the cloud storage receives your data, but before the data is written to disk and stored. | Encryption that occurs before data is sent to Cloud Storage and BigQuery. Such data arrives at Cloud Storage and BigQuery already encrypted but also undergoes server‐side encryption.\nYou can create and manage your encryption keys using a Google Cloud Key Management Service. |  You are responsible for the client‐side keys and cryptographic operations.",
            "subsections": {},
            "summary": "* **Encryption Overview**\n    * _Google encrypts data at rest by default, with customer-managed options available_\n    * Encryption occurs before data is sent to Cloud Storage and BigQuery (client-side) or after it arrives (server-side)\n\n* **Data Integrity**\n    * Google Cloud Storage supports CRC32C and MD5 hashes for data integrity\n    * Server-side encryption provides protection against corruption\n\n* **Encryption Key Management**\n    * Customer-managed keys can be used with Google Cloud Key Management Service"
          },
          "Encryption in Transit": {
            "content": "To protect your data as it travels over the Internet during read and write operations, Google Cloud uses Transport Layer Security (TLS).",
            "subsections": {},
            "summary": "* **Security Protocol**: Google Cloud uses *Transport Layer Security (TLS)* to protect data in transit.\n* *Data Encryption*: TLS ensures encrypted data exchange between devices and Google Cloud services.\n* *Secure Communication*: TLS protocol provides secure communication channels for sensitive data."
          },
          "Encryption in Use": {
            "content": "Encryption in use protects your data in memory from compromise or data exfiltration by encrypting data while it's being processed. Confidential Computing is an example. `Confidential Computing` protects your data in memory from compromise by encrypting it while it is being processed. You can encrypt your data in use with Confidential VMs and Confidential GKE Nodes. Read this blog for more details on data security concepts: `https://cloud.google.com/blog/topics/developers-practitioners/data-security-google-cloud`.",
            "subsections": {},
            "summary": "* *_Encryption in Use_* \n    * Protects data in memory from compromise or exfiltration\n    * Example: Confidential Computing encrypts data while processing\n    * Available through Confidential VMs and GKE Nodes"
          }
        },
        "summary": "* **Data Security in Google Cloud**: Ensures the security of users' and employees' data through various built-in security measures.\n    * *_Encryption_*: Protects data both while it is stored on servers (at rest) and when it is transmitted over the internet (in transit).\n        + **At Rest Encryption**: Protects data from unauthorized access while it is stored on Google Cloud's servers.\n        + **In Transit Encryption**: Protects data as it is transmitted between devices, applications, and Google Cloud services."
      },
      "Identity and Access Management": {
        "content": "Identity and Access Management is the way to manage access to data and resources in Google Cloud. (For more information about IAM, see `https://cloud.google.com/vertex-ai/docs/general/access-control`.) Vertex AI uses IAM to manage access to resources. You can manage access at the project level or resource level:\n\n  * **Project‐level roles** : To grant access to resources at the project level, assign one or more roles to a principal (user, group, or service account). A service account is an account for an application or compute workload instead of an individual end user. Mostly, service accounts are used for creating Vertex AI Workbench, Vertex AI custom training, and Vertex AI predictions.\n  * **Resource‐level roles** : To grant access to a specific resource, set an IAM policy on that resource; the resource must support resource‐level policies. The policy defines which roles are assigned to which principals.\n\nYou can use these two levels of granularity to customize permissions. For example, you can grant all Vertex AI Feature Store users read permission to all feature stores by setting a project‐level policy. Vertex AI Feature Store stores features in a centralized way. For a subset of users, you grant write permissions to particular feature stores by using a resource‐level policy.\n\n* * *\n\nCurrently, Vertex AI supports resource‐level access control for Vertex AI Feature Store and entity type resources only. Setting a policy at the resource level does not affect project‐level policies.\n\n* * *\n\nThe following are the IAM roles that can be used in Vertex AI:\n\n  * **Predefined roles** allow you to grant a set of related permissions to your Vertex AI resources at the project level. Two of the common predefined roles for Vertex AI are Vertex AI Administrator and Vertex AI User.\n  * **Basic roles** such as Owner, Editor, and Viewer provide access control to your Vertex AI resources at the project level. These roles are common to all Google Cloud services.\n  * **Custom roles** allow you to choose a specific set of permissions, create your own role with those permissions, and grant the role to users in your organization.\n\nNot all Vertex AI predefined roles and resources support resource‐level policies.",
        "subsections": {
          "IAM Permissions for Vertex AI Workbench": {
            "content": "Vertex AI Workbench is a data science service offered by Google Cloud Platform (GCP) that leverages JupyterLab to explore and access data. While setting up a Vertex AI Workbench notebook and resources such as model jobs, training, and deployment, you can choose which Virtual Private Cloud you want to use. Google provides encryption at rest and in transit for Vertex AI Workbench.\n\nThere are two types of Vertex AI notebooks with Vertex AI Workbench:\n\n  * **User‐managed notebook:** User‐managed notebook instances are highly customizable and can be ideal for users who need a lot of control over their environment. Therefore, user‐managed notebook instances can require more time to set up and manage than a managed notebook instance. You can use a tag in the Metadata section of a user‐managed notebook to control the instances; however, this option is not available in managed notebooks. Figure 6.1 shows how to create a user‐managed notebook.\n  * **Managed notebooks:** Managed notebook instances are Google Cloud–managed and therefore less customizable than Vertex AI Workbench user‐managed notebook instances. Some of the advantages of managed notebooks are integration with Cloud Storage and BigQuery in JupyterLab and automatic shutdown of the notebook instances when they're not in use. In Figure 6.2, you see the created managed notebooks on the MANAGED NOTEBOOKS tab. You can also find user‐managed notebooks on the USER‐MANAGED NOTEBOOKS tab.\n\n**FIGURE 6.1** Creating a user‐managed Vertex AI Workbench notebook\n\n**FIGURE 6.2** Managed Vertex AI Workbench notebook\n\nThere are two ways to set up user access modes (permission) for both user‐managed and managed notebooks:\n\n  * **Single User Only:** The Single User Only access mode grants access only to the user that you specify.\n  * **Service Account:** The Service Account access mode grants access to a service account. You can grant access to one or more users through this service account. To use service accounts with the Google Cloud CLI, you need to set up an environment variable where your code runs.\n\nFigure 6.3 shows the permissions for setting up a managed notebook.\n\n**FIGURE 6.3** Permissions for a managed Vertex AI Workbench notebook\n\n* * *\n\nIf you are using Google Colab, you would need to create a service account key with access to the Vertex AI administrator and Cloud Storage owner permission. Then you can provide the location of the JSON key file to the GOOGLE_APPLICATION_CREDENTIALS environment variable to authenticate your Google Colab project to run Vertex AI APIs. Creating a service account key is a security risk that should be avoided if possible. If you must create a service account key, make sure you keep it secure.\n\n* * *\n\nIn this section, we covered IAM (Identity and Access Management) roles and permissions needed to configure for Vertex AI Workbench. We also covered how you can access the JupyterLab notebooks using a service account and single user access for Vertex AI Workbench. Now let’s discuss how we can secure the network with Vertex AI.",
            "subsections": {},
            "summary": "* **Vertex AI Workbench**: Data science service leveraging JupyterLab to explore and access data, with encryption at rest and in transit provided by Google.\n  * Provides two types of notebooks: user-managed and managed\n    + User-managed notebooks are highly customizable but require more setup and management\n      * Can use tags to control instances\n* **Access Modes**: \n  * Single User Only\n  * Service Account, which requires a service account key with proper permissions\n    + Creating a service account key is a security risk, should be avoided if possible"
          },
          "Securing a Network with Vertex AI": {
            "content": "Before we get into securing a network with Vertex AI, you should understand the Google Cloud shared responsibility and shared fate models. Understanding this terminology is important when determining how to best protect your data and workloads on Google Cloud.\n\nShared responsibility model: According to the Google shared responsibility model, the cloud provider must monitor and respond to security threats related to the cloud itself and its underlying infrastructure. Meanwhile, end users, including individuals and companies, are responsible for protecting data and other assets they store in any cloud environment.\n\nShared fate model: This model was started to address the challenges that the shared responsibility model doesn't address. Shared fate focuses on how all parties can better interact to continuously improve security. Shared fate builds on the shared responsibility model because it views the relationship between cloud provider and customer as an ongoing partnership to improve security. There are several components of shared fate:\n\n  * **Help getting started:** Secure blueprints that let you deploy and maintain secure solutions using infrastructure as code (IaC). Blueprints have security recommendations enabled by default, such as the Vertex AI Workbench notebooks blueprint.\n  * **Risk protection program**\n  * **Assured workloads and governance**\n\nTo read more about these concepts, refer to:\n\n`https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate`\n\nFor the exam, you will not be tested on these concepts. However, basic understanding of these concepts helps you to understand how data and access are controlled in Google Cloud.\n\nNow we will cover how you can secure the following:\n\n  * Vertex AI Workbench notebook environment\n  * Vertex AI endpoints (public vs. private endpoints)\n  * Vertex AI training jobs",
            "subsections": {
              "Securing Vertex AI Workbench": {
                "content": "By default, your managed notebooks instance uses a Google‐managed network. (We covered securing the Vertex AI Workbench with IAM in the previous section.) Some of the best practices to secure your workbench are as follows:\n\n  * **Use a private IP address** Vertex AI Workbench by default has public IP addresses assigned, which can increase your attack surface and expose sensitive data. The best practice is to use a private IP address while creating a workbench. To create a private Vertex AI Workbench, you'll need to specify the `‐‐no‐public‐ip` command. The following is the Google CLI command to create a workbench with a private IP address:\n\n[code]\n     gcloud beta notebooks instances create example-instance  \\\n       --vm-image-project=deeplearning-platform-release  \\\n       --vm-image-name=tf2-2-1-cu101-notebooks-20200110  \\\n       --machine-type=n1-standard-4  \\\n       --location=us-central1-b  \\\n       --no-public-ip\n\n[/code]\n\n  * **Connect your instance to a VPC network in the same project** To connect a managed notebooks instance to a VPC network in the same project as your managed notebooks instance, you need to configure private services access. Private services access enables you to reach internal IP addresses hosted in a VPC network. This is useful if you want your VM instances in your VPC network to use internal IP addresses instead of external IP addresses. Refer to this link to learn more:\n\n`https://cloud.google.com/vpc/docs/private-services-access`\n\n  * **Shared VPC network** You can also specify a VPC network located within your project or a shared VPC network that you have access to. If you specify a VPC or shared VPC network, the network requires a private services access connection. Shared VPC allows an organization to connect resources from multiple projects to a common virtual private cloud (VPC) network so that they can communicate with each other securely and efficiently by using internal IP addresses from that network.\n    * **VPC Service Controls** This is a feature that allows you to control the services that are available in your VPC. You can use VPC Service Controls to allow or deny access to specific services or limit the amount of traffic that can be generated by specific services. When you use VPC Service Controls to protect Vertex AI, the following artifacts can't leave your service perimeter:\n      * Training data for an AutoML model or custom model\n      * Models that you created\n      * Requests for online predictions\n      * Results from a batch prediction request",
                "subsections": {},
                "summary": "* **Security Best Practices**\n  * Use a private IP address for your Vertex AI Workbench to reduce attack surface and expose sensitive data.\n    * _Specify the `‐‐no‐public‐ip` command when creating a workbench._\n* Connect your instance to a VPC network in the same project to use internal IP addresses instead of external IP addresses.\n* Use VPC Service Controls to control access to specific services, protect training data and models from leaving your service perimeter."
              },
              "Securing Vertex AI Endpoints": {
                "content": "Vertex AI provides options to host your models by creating an endpoint through Vertex AI prediction, covered in Chapter 10, “Scaling Models in Production.” There are public endpoints and private endpoints.\n\n  * **Public endpoint** This endpoint is publicly accessible to the Internet. By default, this option is available when creating an endpoint with Vertex AI.\n  * **Private endpoints** In Vertex AI Prediction through VPC Peering, you can set up a private connection to talk to your endpoint without your data ever traversing the public Internet, resulting in increased security and lower latency for online predictions. Before you make use of a private endpoint, you'll first need to create connections between your VPC network and Vertex AI.\n\nIt's very easy to set up a private endpoint from the Vertex AI console. Go to Endpoint and select Create Endpoint. You will see an option to select Private. Figure 6.4 shows how you can create a private endpoint in Vertex AI.\n\n**FIGURE 6.4** Creating a private endpoint in the Vertex AI console\n\nTo set up VPC Network Peering, you can configure Vertex AI to peer with a VPC to connect directly with certain resources in Vertex AI, such as custom training jobs that were covered in Chapter 8, “Model Training and Hyperparameter Tuning,” private prediction endpoints, and Vertex AI Matching Engine.\n\n* * *\n\nVertex AI Matching Engine is used to build use cases that match semantically similar items such as a recommendation engine, search engine, etc. Vertex AI Matching Engine is a high‐scale, low‐latency vector database that is referred to as vector similarity‐matching or an approximate nearest neighbor (ANN) service.\n\n* * *",
                "subsections": {},
                "summary": "### Public vs Private Endpoints in Vertex AI\n* _Public Endpoints_: publicly accessible to the Internet\n  * Available by default when creating an endpoint with Vertex AI\n  * Data traverses public Internet, increasing latency and security concerns\n* _Private Endpoints_: secure connection to endpoint without data on public Internet\n  * Set up through VPC Peering for increased security and lower latency\n\n### Setting Up Private Endpoints\n* Create private endpoint in the Vertex AI console\n  * Go to Endpoint > Create Endpoint > Select Private\n* Configure VPC Network Peering for low-latency connection\n\n### Vertex AI Matching Engine\n* High-scale, low-latency vector database\n  * Refers to semantic similarity matching or approximate nearest neighbor (ANN) service"
              },
              "Securing Vertex AI Training Jobs": {
                "content": "Using private IP addresses to connect to your training jobs provides more network security and lower network latency than using public IP addresses. To use private IP addresses, you use VPC to peer your network with any type of Vertex AI custom training job. This allows your training code to access private IP addresses inside your Google Cloud or on‐premises networks. See `https://cloud.google.com/vertex-ai/docs/training/using-private-ip` for more information.\n\n* * *\n\nIt is recommended that you use both VPC Service Controls and IAM for defense in depth. VPC Service Controls prevents service operations such as a `gsutil cp` command copying to a public Cloud Storage bucket or a `bq mk` command copying to a permanent external BigQuery table.\n\n* * *\n\nIn the following sections, we will cover some concepts such as federated learning, differential privacy, and tokenization.",
                "subsections": {},
                "summary": "**Using Private IP Addresses for Vertex AI Training**\n* Connects training jobs to Google Cloud or on-premises networks using VPC peering\n* Provides network security and lower latency compared to public IP addresses\n* Allows access to private IP addresses for training code execution\n\n**Network Security and Defense**\n* Use VPC Service Controls and IAM for defense in depth\n* Prevents service operations from copying data to public buckets or external tables"
              },
              "Federated Learning": {
                "content": "According to the Google AI blog, `https://ai.googleblog.com/2017/04/federated-learning-collaborative.html`, _federated learning_ is a technique that is used to enable mobile phones to collaboratively learn a shared prediction model while keeping all the training data on the device. The device downloads the model and learns from the device data. This updated model is then sent to the cloud with encrypted communication. Since all the training data remains on your device, federated learning allows for smarter models, lower latency, and less power consumption, all while ensuring privacy.\n\nAn example would be a group of hospitals around the world that are participating in the same clinical trial. The data that an individual hospital collects about patients is not shared outside the hospital. As a result, hospitals can't transfer or share patient data with third parties. Federated learning lets affiliated hospitals train shared ML models while still retaining security, privacy, and control of patient data within each hospital by using a centralized model shared by all the hospitals. This model trains local data in the hospital, and only the model update is sent back to the centralized cloud server. The model updates are decrypted, averaged, and integrated into the centralized model. Iteration after iteration, the collaborative training continues until the model is fully trained. This way federated learning decouples the ability to do machine learning from the need to store the data in the cloud. Refer to this link for more information: `https://cloud.google.com/architecture/federated-learning-google-cloud`.",
                "subsections": {},
                "summary": "* **Federated Learning**: enables mobile devices or organizations to collaboratively learn a shared prediction model while keeping training data on-device.\n* *Key Benefits*: \n  * Smarter models\n  * Lower latency\n  * Less power consumption\n  * Ensures privacy and security of training data\n* *Example*: hospitals train shared ML models with patient data remaining on-device, only updating the centralized cloud server with model updates."
              },
              "Differential Privacy": {
                "content": "According to `https://en.wikipedia.org/wiki/Differential_privacy`, _differential privacy (DP)_ is a system for publicly sharing information about a dataset by describing the patterns within groups of individuals within the dataset while withholding information about each individual in the dataset. For example, training a machine learning model for medical diagnosis, we would like to have machine learning algorithms that do not memorize sensitive information about the training set, such as the specific medical histories of individual patients. Differential privacy is a notion that allows quantifying the degree of privacy protection provided by an algorithm for the underlying (sensitive) dataset it operates on. Through differential privacy, we can design machine learning algorithms that responsibly train models on private data.\n\n* * *\n\nYou can use both the techniques together, federated learning with differential privacy, to securely train a model with PII data sitting in distributed silos.\n\n* * *",
                "subsections": {},
                "summary": "* **Differential Privacy**: _system for sharing dataset information while hiding individual identities_\n    * Enables training machine learning models on private data without memorizing sensitive info\n    * Measures the degree of privacy protection provided by an algorithm\n        * Allows responsible training of models on private data using techniques like Federated Learning"
              },
              "Format‐Preserving Encryption and Tokenization": {
                "content": "_Format‐Preserving Encryption_ , or _FPE_ , is an encryption algorithm that preserves the format of information while it is being encrypted. It is the process of encrypting data in such a way that the output (ciphertext) remains in the same format as the input (plain text). See `https://en.wikipedia.org/wiki/Format-preserving_encryption` for more information.\n\nTwo use cases for this are as follows:\n\n  * **Payment Card Verification:** In the retail and e‐commerce sector, payment card data must be collected and stored to make payments. Additionally, employees may need to see and verify the last four digits of a customer's payment card information. FPE makes it easier to expose only the required information to employees while leaving the other 12 digits protected.\n  * **Legacy Databases:** A telecommunications system may have a multitude of legacy systems that require the use of encrypted data for security. However, it might not be an option for the organization to restructure its databases to store encrypted data. With FPE, the structure of the databases can remain unchanged. (For more information, see `www.ubiqsecurity.com/what-is-format-preserving-encryption-fpe-and-its-benefits`.)\n\n_Tokenization_ refers to a process by which a piece of sensitive data, such as a credit card number, is replaced by a surrogate value known as a token. The sensitive data still generally needs to be stored securely at one centralized location for subsequent reference and requires strong protections around it. The security of a tokenization approach depends on the security of the sensitive values and the algorithm and process used to create the surrogate value and map it back to the original value.\n\nFPE obfuscates sensitive information, while tokenization removes it entirely (to another location).",
                "subsections": {},
                "summary": "**Format-Preserving Encryption (FPE)**\n\n* _Preserves data format during encryption_\n* Used for secure storage and transmission of sensitive data\n    * Examples: payment card verification, legacy database systems\n    * _Key difference from tokenization_: obfuscates sensitive information, while tokenization removes it entirely"
              }
            },
            "summary": "* **Google Cloud Shared Responsibility Model**: \n    * The cloud provider monitors security threats, while end users protect their data and assets.\n* **Shared Fate Model**:\n    * An ongoing partnership between the cloud provider and customer to improve security.\n    * Includes components such as secure blueprints, risk protection programs, and assured workloads.\n\n* _Key Takeaways_\n  * Secure your data with Google Cloud's shared responsibility model and shared fate model.\n  * Learn how to secure Vertex AI Workbench notebook environment, endpoints, and training jobs."
          }
        },
        "summary": "* _Identity and Access Management (IAM) in Google Cloud is used to manage access to data and resources, including Vertex AI._\n  * *_Project-level roles_* : Assign one or more roles to a principal (user, group, or service account) for access to project resources.\n  * *_Resource-level roles_* : Set an IAM policy on a specific resource for granular permissions control.\n\n* Note: Only resource-level policies are supported for Vertex AI Feature Store and entity type resources. Project-level policies take precedence over resource-level policies."
      },
      "Privacy Implications of Data Usage and Collection": {
        "content": "In the following sections, we will cover _personally identifiable information (PII)_ and _protected health information (PHI)_ in the data and how Google Cloud recommends strategies to deal with sensitive data.\n\nPII is a type of data that allows for an individual to be identified. It includes any information relating to a specific individual, such as name, address, Social Security number (SSN), date of birth, financial information, passport number, telephone numbers, and email addresses.\n\nThe Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule provides federal protections for PHI held by covered entities and gives patients an array of rights with respect to that information. At the same time, the Privacy Rule is balanced so that it permits the disclosure of personal health information needed for patient care and other important purposes.",
        "subsections": {
          "Google Cloud Data Loss Prevention": {
            "content": "The Google Cloud Data Loss Prevention (DLP) API can de‐identify sensitive data in text content, including text stored in container structures such as tables. _De‐identification_ is the process of removing identifying information from data. The API detects sensitive data such as PII and then uses a de‐identification transformation to mask, delete, or otherwise obscure the data.\n\nThe de‐identification techniques used are as follows:\n\n  * Masking sensitive data by partially or fully replacing characters with a symbol, such as an asterisk (*) or hash (#)\n  * Replacing each instance of sensitive data with a token using a technique such as cryptographic hashing\n  * Encrypting and replacing sensitive data using a randomly generated or predetermined key\n  * Using bucketing, which replaces a more identifiable value with a less distinguishing value\n\nThe following are some of the key concepts associated with DLP:\n\n  * **Data profiles:** The data profiler lets you protect data across your organization by identifying where sensitive and high‐risk data reside. When you turn on data profiling, Cloud DLP automatically scans all BigQuery tables and columns across the entire organization, individual folders, and projects. It then creates data profiles at the table, column, and project levels.\n  * **Risk analysis:** You can use risk analysis methods before de‐identification to help determine an effective de‐identification strategy or after de‐identification to monitor for any changes or outliers. Cloud DLP can compute four re‐identification risk metrics: _k_ ‐anonymity, _l_ ‐diversity, _k_ ‐map, and _δ_ ‐presence. (For more information, see `https://cloud.google.com/dlp/docs/concepts-risk-analysis`.)\n  * **Inspection (jobs and triggers):** A _job_ is an action that Cloud Data Loss Prevention runs to either scan content for sensitive data or calculate the risk of re‐identification. Cloud DLP creates and runs a job resource whenever you tell it to inspect your data. You can schedule when Cloud DLP runs jobs by creating job triggers. A _job trigger_ is an event that automates the creation of DLP jobs to scan Google Cloud Storage repositories, including Cloud Storage buckets, BigQuery tables, and Datastore. You can also trigger a DLP scan job by using Cloud Functions every time a file is uploaded to Cloud Storage.\n\nDLP provides templates that are configurations to help you set up DLP jobs; see Figure 6.5.\n\n**FIGURE 6.5** Architecture for de‐identification of PII on large datasets using DLP\n\nThe architecture consists of the following:\n\n  * **Data de‐identification streaming pipeline** : De‐identifies sensitive data in text using Dataflow. If you have streaming data coming, you can use Dataflow to trigger a DLP job to de‐identify data and store it in Google Cloud Storage or populate the de‐identified data in a BigQuery table. You can also run a DLP job on batch data stored in Cloud Storage.\n  * **Configuration (DLP template and key) management** : You can manage the templates and configuration for your DLP jobs with a small group of people as security admins and use Cloud KMS (key management service) to avoid exposing de‐identification methods and encryption keys.\n  * **Data validation and re‐identification pipeline** : You can have batch and streaming data de‐identified and stored in BigQuery as a DLP job output. You can also choose to store this in other types of storage. You can validate copies of the de‐identified data and use a Dataflow pipeline to re‐identify data at a large scale.",
            "subsections": {},
            "summary": "* **De-identification with Google Cloud DLP**: removes identifying information from text content, including PII\n    * _Masking_: replaces characters with symbols or tokens\n    * _Replacing_: replaces sensitive data with a token using cryptographic hashing\n    * _Encrypting_: encrypts and replaces sensitive data with a randomly generated key\n* **Key Concepts**:\n  * *_Data Profiling_*: identifies sensitive data across the organization\n  * *_Risk Analysis_*: determines an effective de-identification strategy or monitors for changes/outliers\n  * *_Inspection (Jobs and Triggers)_*: scans content for sensitive data, schedules jobs to run"
          },
          "Google Cloud Healthcare API for PHI Identification": {
            "content": "Under the U.S. Health Insurance Portability and Accountability Act (HIPAA), PHI that is linked based on the list of 18 identifiers such as name, medical record number, Social Security number, IP address, and so on must be treated with special care. For the complete list of identifiers, refer to this Wikipedia page: `https://en.wikipedia.org/wiki/Protected_health_information`.\n\nThe Google Cloud Healthcare API has the de‐identify operation, which removes PHI or otherwise sensitive information from healthcare data. The healthcare API's de‐identification is highly configurable and redacts PHI from text, images, Fast Healthcare Interoperability Resources (FHIR), and Digital Imaging and Communications in Medicine (DICOM) data. Source `https://cloud.google.com/healthcare-api/docs/concepts/de-identification` The Cloud Healthcare API also detects sensitive data in DICOM instances and FHIR data, such as protected PHI, and then uses a de‐identification transformation to mask, delete, or otherwise obscure the data.\n\nThe PHI targeted by the de‐identify command includes the 18 identifiers described in the HIPAA Privacy Rule de‐identification standard. The HIPAA Privacy Rule does not restrict the use or disclosure of de‐identified health information, as it is no longer considered protected health information.\n\nFor CSVs, BigQuery tables, and text strings, the open source DLP API Dataflow pipeline (see the GitHub repo `https://github.com/GoogleCloudPlatform/healthcare-deid`) eases the process of configuring and running the DLP API on healthcare data.",
            "subsections": {},
            "summary": "* _HIPAA_ requires special care for PHI linked to 18 identifiers, such as name or SSN.\n* Google Cloud Healthcare API's de-identification removes PHI from various data types.\n    * _Configurable_ and covers text, images, FHIR, and DICOM data.\n* De-identified health information is not protected under HIPAA Privacy Rule."
          },
          "Best Practices for Removing Sensitive Data": {
            "content": "Depending on the structure of the dataset, removing sensitive data requires different approaches, as shown in Table 6.2.\n\n**TABLE 6.2** Strategies for handling sensitive data\n\nType of Data | Strategy Used\n---|---\nData is restricted to specific columns in structured datasets. | You can create a view that doesn't provide access to the columns in question. The data engineers cannot view the data, but at the same time the data is live and doesn't require human intervention to de‐identify it for continuous training.\nSensitive data is part of unstructured content, but it's identifiable using known patterns or regex. | You can use Cloud DLP to address this type of data.\nSensitive data exists within images, videos, audio, or unstructured free‐form data. | Use NLP API, Cloud Speech API, and Vision AI and Video Intelligence API to identify the sensitive data such as email and location out of box and then mask or remove it.\n\nRefer to `https://cloud.google.com/architecture/sensitive-data-and-ml-datasets` for more details.\n\nOne of the methods of protecting data with multiple columns is to use an ML algorithm such as Principal Component Analysis (PCA) or other dimension‐reducing techniques to combine several features and then carry out ML training only on the resulting PCA vectors, according to `https://medium.com/lizuna/beacon-the-use-of-principal-components-analysis-to-mask-sensitive-data-in-machine-learning-7904b01445d0`. For example, given three different fields of age, smoker (represented as 1 or 0), and body weight, the data might get condensed into a single PCA column that uses the following equation:\n\n  * 1.5age + 30smoker + 0.2 * body‐weight\n\nSomebody who is 20 years old, smokes, and weighs 140 pounds generates a value of 88. This is the same value generated by someone who is 30 years old, doesn't smoke, and weighs 215 pounds.\n\nThis method can be quite robust because even if one identifies individuals who are unique in some way, it is hard to determine without an explanation of the PCA vector formula what makes them unique.\n\nCoarsening is another technique used to decrease the granularity of data in order to make it more difficult to identify sensitive data within the dataset while still giving comparable benefits versus a training model with the pre‐coarsened data. The fields in Table 6.3 are particularly well‐suited to this approach.\n\n**TABLE 6.3** Techniques to handle sensitive fields in data\n\nField | Description\n---|---\nIP addresses | Zero out the last octet of IPv4 addresses (the last 80 bits if using IPv6).\nNumeric quantities | Numbers can be binned to make them less likely to identify an individual; for example, age and birthdays can be changed into ranges.\nZip codes | Can be coarsened to include just the first three digits.\nLocation | Use location identifiers such as city, state, or zip code, or use a large range to obfuscate the unique characteristics of one row.",
            "subsections": {},
            "summary": "* **Sensitive Data Handling**: Removing sensitive data requires different approaches depending on its structure and type.\n\n  * *Structured datasets*: Create a view that restricts access to columns in question.\n  * *Unstructured content*: Use Cloud DLP or NLP API with regex to identify patterns.\n  * *Images, videos, audio, etc.*: Use APIs like Cloud Speech API and Vision AI to detect sensitive data.\n\n* **Data Coarsening Techniques**: \n  * *IP addresses*: Zero out the last octet of IPv4 addresses.\n  * *Numeric quantities*: Bin numbers to reduce individual identification.\n  * *Zip codes*: Coarsen to include just the first three digits."
          }
        },
        "summary": "* **Data Sensitive Topics**\n\t* _personally identifiable information (PII)_: data allowing an individual to be identified\n\t* _protected health information (PHI)_: sensitive medical data with federal protections under HIPAA"
      },
      "Summary": {
        "content": "In this chapter, we discussed some of the security best practices used to manage data for machine learning in Google Cloud, such as encryption at rest and encryption in transit.\n\nWe also covered IAM briefly and how to use IAM to provide and manage access to Vertex AI Workbench for your data science team. We covered some secure ML development techniques such as federated learning and differential privacy.\n\nLast, we covered how you can manage PII and PHI data using the Cloud DLP and Cloud Healthcare APIs. We also covered an architecture pattern on how you can scale the PII identification and de‐identification on a large dataset.",
        "subsections": {},
        "summary": "* *_Security Best Practices for Machine Learning_*: encryption at rest, encryption in transit, IAM access management\n* *_Secure ML Development Techniques_*: federated learning, differential privacy\n* *_PII and PHI Data Management_*: Cloud DLP, Cloud Healthcare APIs, scalable architecture pattern"
      },
      "Exam Essentials": {
        "content": "* **Build secure ML systems.** Understand encryption at rest and encryption in transit for Google Cloud. Know how encryption at rest and in transit works for storing data for machine learning in Cloud Storage and BigQuery. Know how you can set up IAM roles to manage your Vertex AI Workbench and how to set up network security for your Vertex AI Workbench. Last, understand some concepts such as differential privacy, federated learning, and tokenization.\n  * **Understand the privacy implications of data usage and collection.** Understand the Google Cloud Data Loss Prevention (DLP) API and how it helps identify and mask PII type data. Also, understand the Google Cloud Healthcare API to identify and mask PHI type data. Finally, understand some of the best practices for removing sensitive data.",
        "subsections": {},
        "summary": "* **Secure ML Systems**\n  * Understand encryption at rest and in transit for Google Cloud Storage and BigQuery\n  * Set up IAM roles for Vertex AI Workbench management and network security\n  * Learn about differential privacy, federated learning, and tokenization concepts\n* _Understanding Data Privacy Implications_\n  * Identify and mask PII type data with the Google Cloud DLP API\n  * Mask PHI type data using the Google Cloud Healthcare API\n  * Apply best practices for removing sensitive data"
      },
      "Review Questions": {
        "content": "1. You are an ML security expert at a bank that has a mobile application. You have been asked to build an ML‐based fingerprint authentication system for the app that verifies a customer's identity based on their fingerprint. Fingerprints cannot be downloaded into and stored in the bank databases. Which learning strategy should you recommend to train and deploy this ML model and make sure the fingerprints are secure and protected?\n     1. Differential privacy\n     2. Federated learning\n     3. Tokenization\n     4. Data Loss Prevention API\n  2. You work on a growing team of more than 50 data scientists who all use Vertex AI Workbench. You are designing a strategy to organize your jobs, models, and versions in a clean and scalable way. Which strategy is the most managed and requires the least effort?\n     1. Set up restrictive IAM permissions on the Vertex AI platform notebooks so that only a single user or group can access a given instance.\n     2. Separate each data scientist's work into a different project to ensure that the jobs, models, and versions created by each data scientist are accessible only to that user.\n     3. Use labels to organize resources into descriptive categories. Apply a label to each created resource so that users can filter the results by label when viewing or monitoring the resources.\n     4. Set up a BigQuery sink for Cloud Logging logs that is appropriately filtered to capture information about AI Platform resource usage. In BigQuery, create a SQL view that maps users to the resources they are using.\n  3. You are an ML engineer of a Fintech company working on a project to create a model for document classification. You have a big dataset with a lot of PII that cannot be distributed or disclosed. You are asked to replace the sensitive data with specific surrogate characters. Which of the following techniques is best to use?\n     1. Format‐preserving encryption or tokenization\n     2. K‐anonymity\n     3. Replacement\n     4. Masking\n  4. You are a data scientist of an EdTech company, and your team needs to build a model on the Vertex AI platform. You need to set up access to a Vertex AI Python library on Google Colab Jupyter Notebook. What choices do you have? (Choose three.)\n     1. Create a service account key.\n     2. Set the environment variable named GOOGLE_APPLICATION_CREDENTIALS.\n     3. Give your service account the Vertex AI user role.\n     4. Use console keys.\n     5. Create a private account key.\n  5. You are a data scientist training a deep neural network. The data you are training contains PII. You have two challenges: first you need to transform the data to hide PII, and you also need to manage who has access to this data in various groups in the GCP environment. What are the choices provided by Google that you can use? (Choose two.)\n     1. Network firewall\n     2. Cloud DLP\n     3. VPC security control\n     4. Service keys\n     5. Differential privacy\n  6. You are a data science manager and recently your company moved to GCP. You have to set up a JupyterLab environment for 20 data scientists on your team. You are looking for a least‐managed and cost‐effective way to manage the Vertex AI Workbench so that your instances are only running when the data scientists are using the notebook. How would you architect this on GCP?\n     1. Use Vertex AI–managed notebooks.\n     2. Use Vertex AI user‐managed notebooks.\n     3. Use Vertex AI user‐managed notebooks with a script to stop the instances when not in use.\n     4. Use a Vertex AI pipeline.\n  7. You have Fast Healthcare Interoperability Resources (FHIR) data and you are building a text classification model to detect patient notes. You need to remove the PHI from the data. Which service you would use?\n     1. Cloud DLP\n     2. Cloud Healthcare API\n     3. Cloud NLP API\n     4. Cloud Vision AI\n  8. You are an ML engineer of a Fintech company building a real‐time prediction engine that streams files that may contain personally identifiable information (PII) to GCP. You want to use the Cloud Data Loss Prevention (DLP) API to scan the files. How should you ensure that the PII is not accessible by unauthorized individuals?\n     1. Stream all files to Google Cloud, and then write the data to BigQuery. Periodically conduct a bulk scan of the table using the DLP API.\n     2. Stream all files to Google Cloud, and write batches of the data to BigQuery. While the data is being written to BigQuery, conduct a bulk scan of the data using the DLP API.\n     3. Create two buckets of data: sensitive and nonsensitive. Write all data to the Nonsensitive bucket. Periodically conduct a bulk scan of that bucket using the DLP API, and move the sensitive data to the Sensitive bucket.\n     4. Periodically conduct a bulk scan of the Google Cloud Storage bucket using the DLP API, and move the data to either the Sensitive or Nonsensitive bucket.",
        "subsections": {},
        "summary": "* **Secure ML Model Deployment**\n+ *Use Federated Learning to train the model locally on each user's device and then aggregate the results without sharing the sensitive data.*\n  + This approach ensures that fingerprints are never downloaded into the bank databases, maintaining security and compliance.\n  + *This is a recommended approach for protecting sensitive biometric data.*\n* **Managing Vertex AI Resources**\n+ *Set up restrictive IAM permissions on the Vertex AI platform notebooks so that only a single user or group can access a given instance.*\n  + This ensures minimal management effort while still controlling access to resources, especially when working with large teams.\n  + *This approach provides an efficient way to manage jobs, models, and versions within Vertex AI Workbench.*\n* **Data Protection for Fintech**\n+ *Use Format-Preserving Encryption or tokenization to replace sensitive data with surrogate characters.*\n  + This technique ensures that sensitive information remains protected while allowing the model to function without access to PII.\n  + *This approach provides an effective method for protecting sensitive data in machine learning models.*\n* **Setting up Vertex AI Access**\n+ *Create a service account key and set environment variables or give the service account the necessary permissions.*\n  + These options enable you to securely access the Vertex AI Python library on Google Colab Jupyter Notebook.\n  + *These choices provide efficient ways to manage access to Vertex AI resources.*\n* **Data Protection for Deep Neural Networks**\n+ *Use Cloud DLP to scan the data and manage who has access to it in various GCP groups.*\n  + This service helps protect sensitive PII within machine learning models while ensuring secure access controls.\n  + *Cloud DLP is a recommended choice for protecting sensitive data in deep neural networks.*\n* **Managing Vertex AI Instances**\n+ *Use Vertex AI user-managed notebooks with a script to stop instances when not in use.*\n  + This approach ensures that only necessary resources are running, reducing costs and improving efficiency.\n  + *This method provides an effective way to manage instance usage for JupyterLab environments.*"
      }
    },
    "summary": ""
  },
  "Chapter 7Model Building": {
    "content": "",
    "subsections": {
      "Choice of Framework and Model Parallelism": {
        "content": "The number of parameters in modern deep learning models is becoming larger and larger, and the size of the dataset is also increasing dramatically. To train a sophisticated modern deep learning model on a large dataset, you have to use multinode training; otherwise, it just takes forever. You may always see data parallelism and model parallelism in distributed deep learning training.",
        "subsections": {
          "Data Parallelism": {
            "content": "_Data parallelism_ is when the dataset is split into parts and then assigned to parallel computational machines or graphics processing units (GPUs). For every GPU or node, the same parameters are used for the forward propagation. A small batch of data is sent to every node, and the gradient is computed normally and sent back to the main node. There are two strategies when distributed training is practiced, _synchronous_ and _asynchronous_. For data parallelism, we have to reduce the learning rate to keep a smooth training process if there are too many computational nodes. Refer to `https://analyticsindiamag.com/data-parallelism-vs-model-parallelism-how-do-they-differ-in-distributed-training` for more details.",
            "subsections": {
              "Synchronous Training": {
                "content": "In synchronous training, the model sends different parts of the data into each accelerator or GPU. Every GPU has a complete copy of the model and is trained solely on a part of the data. Every single part starts a forward pass simultaneously and computes a different output and gradient. Synchronous training uses an all‐reduce algorithm, which collects all the trainable parameters from various workers and accelerators.",
                "subsections": {},
                "summary": "* **Synchronous Training**: Model sends parts of data to each GPU or accelerator, where each GPU trains on a separate part of the data.\n* _**All-Reduce Algorithm Used**:_ Each GPU computes output and gradient simultaneously, then collects trainable parameters from all GPUs via an all-reduce algorithm."
              },
              "Asynchronous Training": {
                "content": "Synchronous training can be harder to scale and can result in workers staying idle at times. In asynchronous training, workers don't have to wait for each other during downtime in maintenance, and all workers are independently training over the input data and updating variables asynchronously. An example is the parameter server strategy for TensorFlow distributed training. See Figure 7.1 to understand data parallelism with a parameter server.\n\n**FIGURE 7.1** Asynchronous data parallelism\n\n* * *\n\nThe “all‐reduce sync” strategy is great for Tensor Processing Unit (TPU) and one‐machine multi‐GPUs.\n\n* * *",
                "subsections": {},
                "summary": "* _Asynchronous Training_: allows workers to train independently, reducing idle time\n    * enables independent training over input data and updating variables asynchronously\n    * improves scalability compared to synchronous training\n* **All-Reduce Sync**: suitable for Tensor Processing Unit (TPU) and one-machine multi-GPUs"
              }
            },
            "summary": "* _Data parallelism_ involves splitting data across multiple GPUs or nodes, allowing for concurrent processing.\n* The same parameters are used for forward propagation on every node, and gradients are computed and sent back to the main node.\n* **Two key strategies**:\n  * *_Synchronous_*: all nodes wait for each other's updates\n  * *_Asynchronous_*: nodes update independently"
          },
          "Model Parallelism": {
            "content": "In _model parallelism_ , every model is partitioned into parts, just as with data parallelism. Each model is then placed on an individual GPU.\n\nModel parallelism has some obvious benefits. It can be used to train a model such that it does not fit into just a single GPU. For example, say we have 10 GPUs and we want to train a simple ResNet50 model. We could assign the first five layers to GPU 1, the second five layers to GPU 2, and so on, and the last five layers to GPU 10. During the training, in each iteration, the forward propagation has to be done in GPU 1 first and GPU 2 is waiting for the output from GPU 1. Once the forward propagation is done, we calculate the gradients for the last layers that reside in GPU 10 and update the model parameters for those layers in GPU 10. Then the gradients back propagate to the previous layers in GPU 9. Each GPU/node is like a compartment in the factory production line; it waits for the products from its previous compartment and sends its own products to the next compartment. See Figure 7.2 where the model is split into various GPUs.\n\n**FIGURE 7.2** Model parallelism\n\nIncreasing the size of deep learning models (layers and parameters) yields better accuracy for complex vision models. However, there is a limit to the maximum model size you can fit in the memory of a single GPU. When you're training large deep learning models, GPU memory limitations can be a bottleneck. Therefore, model parallelism can be used to overcome the limitations associated with training a model on a single GPU. You can split the model (layers) on multiple GPUs.\n\nYou may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs. tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. (See `www.tensorflow.org/guide/distributed_training`.) Using this API, you can distribute your existing models and training code with minimal code changes. Table 7.1 explains which of these are supported in which scenarios in TensorFlow.\n\n**TABLE 7.1** Distributed training strategies using TensorFlow\n\nStrategy | Description\n---|---\nMirroredStrategy | Synchronous distributed training on multiple GPUs on one machine.\nCentralStorageStrategy | Synchronous training but no mirroring.\nMultiWorkerMirroredStrategy | Synchronous distributed training across multiple workers, each with potentially multiple GPUs or multiple machines.\nTPUStrategy | Synchronous distributed training on multiple TPU cores.\nParameterServerStrategy | Some machines are designated as workers and some as parameter servers.\n\n* * *\n\nTypically sync training is supported via all‐reduce and async through parameter server architecture for TensorFlow.\n\nAfter you have trained your TF model using the appropriate distribution strategy, you can deploy the model using either tf.serving or TFLite on mobile devices and TensorFlow.js for browsers. Figure 7.3 shows training and serving with TensorFlow.\n\n* * *\n\n**FIGURE 7.3** Training strategy with TensorFlow",
            "subsections": {},
            "summary": "* **Model Parallelism**: Models are partitioned and placed on individual GPUs, allowing training of large models that don't fit in a single GPU.\n    * Splitting the model across multiple GPUs can overcome memory limitations and improve accuracy.\n    * _tf.distribute.Strategy_ API is used to distribute training across multiple GPUs or machines."
          }
        },
        "summary": "* **Distributed Training**: Large deep learning models require massive datasets and rapid training times\n* **Multinode Training**: Using multiple nodes for training is necessary to achieve fast training speeds\n    * _Data Parallelism_ and _Model Parallelism_ are used to distribute the workload across nodes"
      },
      "Modeling Techniques": {
        "content": "Let's go over some basic terminology in neural networks that you might see in exam questions.",
        "subsections": {
          "Artificial Neural Network": {
            "content": "Artificial neural networks (ANNs) are the simplest type of neural network; they have one hidden layer. A feedforward neural network is a classic example of an ANN. They are mainly used for supervised learning where the data is mostly numerical and structured, such as, for example, regression problems. See Figure 7.4.\n\n**FIGURE 7.4** Artificial or feedforward neural network",
            "subsections": {},
            "summary": "* **Feedforward Neural Networks**: One-hidden layer type of ANN, primarily used for *_supervised learning_* in numerical and structured data, like *_regression problems_*."
          },
          "Deep Neural Network (DNN)": {
            "content": "Deep neural networks (DNNs) can be defined as ANNs with additional depth—that is, an increased number of hidden layers between the input and the output layers. A neural network with usually at least two layers qualifies as a DNN, or _deep net_ for short. See Figure 7.5 to understand DNN.\n\n**FIGURE 7.5** Deep neural network",
            "subsections": {},
            "summary": "* **Definition**: *Deep neural networks (DNNs) are ANNs with multiple hidden layers between input and output layers.* \n* **Qualification**: A DNN typically has at least two hidden layers, making it a _deep net_."
          },
          "Convolutional Neural Network": {
            "content": "Convolutional neural networks (CNNs) are a type of DNN network designed for image input. CNNs are most well‐suited to image classification tasks, although they can be used on a wide array of tasks that take images as input.",
            "subsections": {},
            "summary": "* **Convolutional Neural Networks (CNNs)**: Designed for image input and best suited for *_image classification tasks_*.\n* *Can also be applied to:* Various other tasks that involve image processing and analysis.\n* *Not limited to:* Classification tasks only, but are a key component in many computer vision applications."
          },
          "Recurrent Neural Network": {
            "content": "Recurrent neural networks (RNNs) are designed to operate upon sequences of data. They have proven to be very effective for natural language processing problems where sequences of text are provided as input to the model. RNNs have also seen some modest success for time‐series forecasting and speech recognition. The most popular type of RNN is the long short‐term memory (LSTM) network. LSTMs can be used in a model to accept a sequence of input data and make a prediction, such as to assign a class label or predict a numerical value like the next value or values in the sequence.\n\nNeural networks are trained using stochastic gradient descent and require that you choose a loss function when designing and configuring your model. A loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. The loss function is one of the important components of neural networks. Loss is nothing but a prediction error of neural net. And the method to calculate the loss is called loss function. In simple words, the loss is used to calculate the gradients. And gradients are used to update the weights of the neural net.",
            "subsections": {},
            "summary": "* **Recurrent Neural Networks (RNNs)**: designed for sequences of data, effective for natural language processing and time-series forecasting\n    * Use long short-term memory (LSTM) networks for predictions, such as class labels or numerical values\n    * _Key application areas_: text analysis, speech recognition, and time-series forecasting\n* **Training Neural Networks**: stochastic gradient descent with a chosen loss function\n    * _Goal of training_: find weights and biases with low average loss across all examples\n    * Loss is prediction error, calculated using a loss function to update weights and biases"
          },
          "What Loss Function to Use": {
            "content": "Importantly, the choice of loss function is directly related to the activation function used in the output layer of your neural network. These two design elements are connected. Table 7.2 summarizes the loss functions based on ML problems.\n\n**TABLE 7.2** Summary of loss functions based on ML problems\n\n****|  Output | Output Layer Configuration or Activation Function | Loss Functions\n---|---|---|---\nRegression problem | Numerical output | One node with a linear activation unit | Mean squared error (MSE)\nBinary classification problem | Binary outcome | Sigmoid activation unit | Binary cross‐entropy, categorical hinge loss, and squared hinge loss (Keras)\nMulticlass classification problem | Single label multiclass | Softmax activation function | Categorical cross‐entropy (on one‐hot encoded data) and sparse categorical cross‐entropy (apply on integers)\n\n* * *\n\nUse sparse categorical cross‐entropy when your classes are mutually exclusive (when each sample belongs exactly to one class) and categorical cross‐entropy when one sample can have multiple classes or labels. Look at the list of loss functions supported by tf.keras: `www.tensorflow.org/api_docs/python/tf/keras/losses`.\n\n* * *\n\nThe following is an example code snippet of TensorFlow multiclass classification. The model must have one node for each class in the output layer and use the softmax activation function. The loss function `sparse_categorical_crossentropy` is appropriate for integer encoded class labels (e.g., 0 for one class, 1 for the next class, etc.).\n[code]\n     # define model\n     model = Sequential()\n     model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n     model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n     **model.add(Dense(3, activation='softmax'))**\n\n     # compile the model\n     **model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])**\n\n[/code]",
            "subsections": {},
            "summary": "* The choice of loss function is directly related to the activation function used in the output layer.\n* Different ML problems require different loss functions:\n  * Regression: Mean Squared Error (MSE)\n  * Binary classification: Binary cross-entropy\n    * Categorical hinge loss and squared hinge loss (Keras)\n  * Multiclass classification:\n    * Softmax activation, categorical cross-entropy on one-hot encoded data\n    * Sparse categorical cross-entropy on integer encoded data"
          },
          "Gradient Descent": {
            "content": "The gradient descent algorithm calculates the gradient of the loss curve at the starting point. The gradient of the loss is equal to the derivative (slope) of the curve. The gradient has both magnitude and direction (vector) and always points in the direction of the steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.",
            "subsections": {},
            "summary": "* **Gradient Descent Algorithm**: calculates the gradient of the loss curve\n    * _Gradients point in the direction of the steepest increase_\n    * **Steepness is minimized by taking a step in the negative gradient**"
          },
          "Learning Rate": {
            "content": "As we know, the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called _step size_) to determine the next point. For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.",
            "subsections": {},
            "summary": "* **Gradient Descent Update**: \n    * The gradient vector guides the update direction.\n    * Learning rate (step size) scales the gradient magnitude to determine the step size.\n        * Example: `learning_rate * gradient_magnitude`"
          },
          "Batch": {
            "content": "In gradient descent, a batch is the total number of examples you use to calculate the gradient in a single iteration. So far, we've assumed that the batch has been the entire dataset. A very large batch may cause even a single iteration to take a very long time to compute.",
            "subsections": {},
            "summary": "* *Batch size* refers to the number of examples used for one iteration\n    * Using a larger batch can lead to longer computation times\n        * In practice, using the **entire dataset** as a batch is often impractical and unnecessary"
          },
          "Batch Size": {
            "content": "Batch size is the number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini‐batch is usually between 10 and 1,000. Batch size is usually fixed during training and inference; however, TensorFlow does permit dynamic batch sizes.",
            "subsections": {},
            "summary": "* _Batch Size_ refers to the number of examples in a batch\n* Typical batch sizes range from 10 to 1,000 for mini-batches\n* Batch size remains constant during **training** and **inference**, but TensorFlow allows **dynamic batch sizes**."
          },
          "Epoch": {
            "content": "An epoch means an iteration for training the neural network with all the training data. In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass. An epoch is made up of one or more batches.",
            "subsections": {},
            "summary": "* **Epoch**: \n  * Iteration for training neural networks\n  * Use all training data exactly once\n  * Forward pass + Backward pass count as one pass\n* \n  * Made up of one or more **_batches_**"
          },
          "Hyperparameters": {
            "content": "We covered loss, learning rate, batch size, and epoch. These are the hyperparameters that you can change while training your ML model. Most machine learning programmers spend a fair amount of time tuning the learning rate.\n\n  * If you pick a learning rate that is too small, learning will take too long.\n  * Conversely, if you specify a large batch size, the model might take more time to compute.",
            "subsections": {
              "Tuning Batch Size": {
                "content": "The following are the best practices for tuning batch size:\n\n  * A smaller mini‐batch size (not too small) usually leads not only to a smaller number of iterations of a training algorithm than a large batch size but also to a higher accuracy overall; that is, a neural network that performs better, in the same amount of training time. Our parallel coordinate plot also makes a key trade‐off very evident: larger batch sizes take less time to train but are less accurate.\n  * If batch size is too small, training will bounce around; if it's too large, training will take a very long time.\n  * However, using a smaller batch size lets your gradient update more often per epoch, which can result in a larger decrease in loss per epoch. Furthermore, models trained using smaller batches generalize better.\n\n* * *\n\nLarge batch size can lead to out of memory error while training neural networks.\n\n* * *",
                "subsections": {},
                "summary": "* **Batch Size Tuning**: \n  * Smaller mini-batch sizes generally improve accuracy and stability.\n  * Optimal batch size balances training speed and accuracy, avoiding overfitting or underfitting.\n    * * Too small: unstable training; too large: slow training with high risk of memory errors."
              },
              "Tuning Learning Rate": {
                "content": "It is important to achieve a desirable learning rate for the following reasons:\n\n  * Both low and high learning rates result in wasted time and resources.\n  * A lower learning rate means more training time.\n  * More time results in increased cloud GPU costs.\n  * A higher rate could result in a model that might not be able to predict anything accurately.\n  * If the learning rate is too small, training will take ages; if it's too large, training will bounce around and ultimately diverge.",
                "subsections": {},
                "summary": "* _A desirable learning rate minimizes wasted time and resources_\n* A balanced learning rate avoids overfitting or underfitting\n* _Incorrect rates lead to increased cloud GPU costs and reduced model accuracy_"
              }
            },
            "summary": "* **Hyperparameters in ML Model Training**\n    * Adjusting hyperparameters like loss, learning rate, and batch size can significantly impact training time\n    * Choosing the right learning rate is crucial, as it affects how quickly the model learns *_i.e., convergence speed_*\n        + Too small: slow convergence\n        + Too large: potential for overshooting or unstable learning"
          }
        },
        "summary": "Let's go over some basic terminology in neural networks that you might see in exam questions."
      },
      "Transfer Learning": {
        "content": "According to a Wiki definition, _transfer learning_ (https://en.wikipedia.org/wiki/Transfer_learning) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. In deep learning, transfer learning is a technique whereby a neural network model is first trained on a problem similar to the problem that is being solved. One or more layers from the trained model are then used in a new model trained on the problem of interest.\n\nTransfer learning is an optimization to save time or get better performance.\n\n  * You can use an available pretrained model, which can be used as a starting point for training your own model.\n  * Transfer learning can enable you to develop models even for problems where you may not have very much data.",
        "subsections": {},
        "summary": "* **Transfer Learning**: Saving knowledge from one problem to apply to another related problem\n*   _**Technique:**_ Using a pre-trained neural network as a starting point, or transferring layers from a trained model to a new model.\n*   *Enables faster training and better performance, even with limited data.*"
      },
      "Semi‐supervised Learning": {
        "content": "Semi‐supervised learning (SSL) is a third type of learning. It is a machine learning problem that involves a small number of labeled examples and a large number of unlabeled examples. Semi‐supervised learning is an approach to `machine learning` that combines a small amount of `labeled data` with a large amount of unlabeled data during training. It falls between `unsupervised learning` (with no labeled training data) and `supervised learning` (with only labeled training data).",
        "subsections": {
          "When You Need Semi‐supervised Learning": {
            "content": "When you don't have enough labeled data to produce an accurate model and you don't have the resources to get more data, you can use semi‐supervised techniques to increase the size of your training data.\n\nFor example, imagine you are developing a model intended to detect fraud for a large bank. Some fraud you know about, but other instances of fraud are slipping by without your knowledge. You can label the dataset with the fraud instances you're aware of, but the rest of your data will remain unlabeled. You can use a semi‐supervised learning algorithm to label the data and retrain the model with the newly labeled dataset. Then, you apply the retrained model to new data, more accurately identifying fraud using supervised machine learning techniques. However, there is no way to verify that the algorithm has produced labels that are 100 percent accurate, resulting in less trustworthy outcomes than traditional supervised techniques.\n\nThe following are some use cases:\n\n  * Fraud detection or anomaly detection\n  * Clustering\n  * Speech recognition\n  * Web content classification\n  * Text document classification",
            "subsections": {},
            "summary": "* **Semi-Supervised Learning**: Increases model accuracy by adding labeled data when enough labeled data is not available.\n    * _Useful for situations with limited labeled data_, such as fraud detection or anomaly detection, where some instances are known but others are not.\n        * _Can be applied to various domains_ such as speech recognition and web content classification."
          },
          "Limitations of SSL": {
            "content": "With a minimal amount of labeled data and plenty of unlabeled data, semi‐supervised learning shows promising results in classification tasks. But it doesn't mean that semi‐supervised learning is applicable to all tasks. If the portion of labeled data isn't representative of the entire distribution, the approach may fall short.",
            "subsections": {},
            "summary": "* _Semi-supervised learning_ uses a minimal amount of labeled and plenty of unlabeled data for classification tasks\n* Its success depends on the representativeness of the labeled data to the entire distribution\n* Inaccurate results can occur if the labeled data is not representative"
          }
        },
        "summary": "* **Semi-supervised Learning**: Combines small amounts of labeled data with large amounts of unlabeled data for machine learning\n    * Involves a mix of labeled and unlabeled examples during training\n    * Falls between supervised and unsupervised learning approaches"
      },
      "Data Augmentation": {
        "content": "Neural networks typically have a lot of parameters. You would need to show your machine learning model a proportional number of examples to get good performance. Also, the number of parameters you need is proportional to the complexity of the task your model has to perform.\n\nYou need a large amount of data examples to train neural networks. In most of the use cases, it's difficult to find a relevant dataset with a large number of examples. So, to get more data or examples to train the neural networks, you need to make minor alterations to your existing dataset—minor changes such as flips or translations or rotations. Our neural network would think these are distinct images. This is _data augmentation_ , where we train our neural network with synthetically modified data (orientation, flips, or rotation) in the case of limited data.\n\nEven if you have a large amount of data, it can help to increase the amount of relevant data in your dataset. There are two ways you can apply augmentation in your ML pipeline: offline augmentation and online augmentation.",
        "subsections": {
          "Offline Augmentation": {
            "content": "In offline augmentation, you perform all the necessary transformations beforehand, essentially increasing the size of your dataset. This method is preferred for relatively smaller datasets because you would end up increasing the size of the dataset by a factor equal to the number of transformations you perform. For example, by rotating all images, you can increase the size of the dataset by a factor of 2.",
            "subsections": {},
            "summary": "* *_Offline Augmentation_*: transforms data before training\n    * increases dataset size by factor equal to number of transformations\n        * e.g., rotation increases dataset size by 2x"
          },
          "Online Augmentation": {
            "content": "In online augmentation, you perform data augmentation transformations on a mini‐batch, just before feeding it to your machine learning model. This is also known as _augmentation on the fly_. This method is preferred for large datasets as mini‐batches you would feed into the model.\n\nThe following list includes some of the data augmentation techniques for images:\n\n  * Flip\n  * Rotate\n  * Crop\n  * Scale\n  * Gaussian noise (adding just the right amount of noise to enhance the learning capability)\n  * Translate\n  * Conditional generative adversarial networks (GANs) to transform an image from one domain to an image in another domain\n  * `Transfer learning` to give the models a better chance with the scarce amount of data",
            "subsections": {},
            "summary": "* **Online Augmentation**: performing data augmentation transformations on mini-batches before feeding them to machine learning models.\n* Data augmentation techniques for images:\n    * *_Flip_*\n    * *_Rotate_*\n    * *_Scale_*\n    * *_Gaussian Noise_*\n    * *_Translate_*"
          }
        },
        "summary": "* **Data Augmentation**: * _Neural networks need large amounts of data examples for good performance_*\n* **Data Generation Techniques**:\n  * Offline augmentation: modifying existing data with techniques like flips, translations, or rotations\n  * Online augmentation: applying data augmentation during training to increase relevant data"
      },
      "Model Generalization and Strategies to Handle Overfitting and Underfitting": {
        "content": "While training neural network models, there are two important things that exist:\n\n  * **Bias:** Bias is the difference between the average prediction of our model and the correct value we are trying to predict. It is actually the error rate of the training data. When the error rate has a high value, bias is high, and when error is low, its bias is low. A model with high bias pays very little attention to the training data and oversimplifies the model.\n  * **Variance:** The error rate of the testing data is called variance. When the error rate has a high value, we call it high variance, and when the error rate has a low value, we call it low variance. A model with high variance pays a lot of attention to training data and does not generalize on the data it hasn't seen before. As a result, such models perform very well on training data but have high error rates on test data.\n\nTraining a deep neural network that can generalize well to new data is a challenging problem. A model with too little capacity cannot learn the problem, whereas a model with too much capacity can learn it too well and overfit the training dataset. Both cases result in a model that does not generalize well.",
        "subsections": {
          "Bias Variance Trade‐Off": {
            "content": "You need to find the right balance without overfitting or underfitting the data. If your model is too simple and has very few parameters, then it may have high bias and low variance. If our model has a large number of parameters, then it's going to have high variance and low bias. This trade‐off in complexity is why there is a trade‐off between bias and variance.\n\nUnderfitting can easily be addressed by increasing the capacity (weights) of the network, but overfitting requires the use of specialized techniques. Refer to `www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning` to learn more.",
            "subsections": {},
            "summary": "* _Key trade-off in complexity_: high bias vs low variance\n    * **Model simplicity**: few parameters (high bias, low variance)\n    * **Overfitting and underfitting_*\n        *_Underfitting_*: increase model capacity (weights), \n        *_Overfitting_*: use specialized techniques"
          },
          "Underfitting": {
            "content": "An underfit model fails to sufficiently learn the problem and performs poorly on a training dataset and does not perform well on a test or validation dataset. In the context of the bias variance trade‐off, an underfit model has high bias and low variance. Regardless of the specific samples in the training data, it cannot learn the problem. There are a couple of reasons for model underfitting:\n\n  * Data used for training is not cleaned.\n  * The model has a high bias.\n\nThe overfit model performance varies widely with unseen examples in the training dataset. Here are some of the ways to reduce underfitting:\n\n  * Increase model complexity.\n  * Increase the number of features by performing feature engineering.\n  * Remove noise from the data.\n  * Increase the number of epochs or increase the duration of training to get better results.",
            "subsections": {},
            "summary": "* **Underfit Model**: _Fails to learn problem, poor performance on datasets_\n    * High bias, low variance\n    * Reasons:\n        * Poorly cleaned training data\n        * High bias in model\n    * Solutions:\n        * Increase complexity\n        * Remove noise from data\n        * Increase epochs or duration of training"
          },
          "Overfitting": {
            "content": "The model learns the training data too well and performance varies widely with new unseen examples or even statistical noise added to examples in the training dataset. An overfit model has low bias and high variance. There are two ways to approach an overfit model:\n\n  * Reduce overfitting by training the network on more examples.\n  * Reduce overfitting by changing the complexity of network structure and parameters.\n\nHere are some of the ways to avoid overfitting:\n\n  * Regularization technique: explained in next section.\n  * `Dropout`: Probabilistically remove inputs during training.\n  * `Noise`: Add statistical noise to inputs during training.\n  * `Early stopping`: Monitor model performance on a validation set and stop training when performance degrades.\n  * Data augmentation.\n  * Cross‐validation.\n\n* * *\n\nBigQuery ML supports two methods for preventing overfitting: early stopping and regularization. Refer to `https://cloud.google.com/bigquery-ml/docs/preventing-overfitting`.\n\n* * *\n\nRefer to this link to learn 10 ways to avoid overfitting: `www.v7labs.com/blog/overfitting`.",
            "subsections": {},
            "summary": "**Overfitting**\n* _Definition:_ Model learns training data too well, performing poorly on new examples or with added noise.\n* Two approaches to address:\n\t+ Increase training examples\n\t+ Change network complexity and parameters\n* **Ways to avoid overfitting:**\n  * Regularization techniques\n  * Dropout\n  * Noise addition\n  * Early stopping\n  * Data augmentation\n  * Cross-validation"
          },
          "Regularization": {
            "content": "Regularization comes into play and shrinks the learned estimates toward 0. In other words, it tunes the loss function by adding a penalty term that prevents excessive fluctuation of the coefficients, thereby reducing the chances of overfitting.\n\nL1 and L2 are two common regularization methods. You will use L1 when you are trying to reduce features and L2 when you are looking for a stable model. Table 7.3 summarizes the difference between the two techniques.\n\n**TABLE 7.3** Differences between L1 and L2 regularization\n\nL1 Regularization | L2 Regularization\n---|---\nL1 regularization, also known as L1 norm or lasso (in regression problems), combats overfitting by shrinking the parameters toward 0. This makes some features obsolete. So, this works well for feature selection in case you have a huge number of features. | L2 regularization, or the L2 norm or ridge (in regression problems), combats overfitting by forcing weights to be small but not making them exactly 0.\nL1 regularization penalizes the sum of absolute values of the weights. | L2 regularization penalizes the sum of squares of the weights.\nL1 regularization has built‐in feature selection. | L2 regularization doesn't perform feature selection.\nL1 regularization is robust to outliers. | L2 regularization is not robust to outliers.\nL1 regularization helps with feature selection and reducing model size or leading to smaller models. | L2 regularization always improves generalization in linear models.\n\nWe covered regularization in this section. Now we will cover some of the common ways backpropagation can go wrong while training neural networks and ways to regularize:\n\n  * **Exploding gradients:** If the weights in a network are very large, then the gradients for the lower layers involve products of many large terms leading to gradients that get too large to converge. Batch normalization and lower learning rate can help prevent exploding gradients.\n  * **Dead ReLU units:** Rectified linear activation function, or ReLU for short, is a linear function that will output the input directly if it is positive; otherwise, it will output 0. Networks that use the rectifier function for the hidden layers are referred to as rectified networks. Once the weighted sum for a ReLU unit falls below 0, the ReLU unit can get stuck. It outputs 0 activation, contributing nothing to the network's output, and gradients can no longer flow through it during backpropagation. With a source of gradients cut off, the input to the ReLU may not ever change enough to bring the weighted sum back above 0. Lowering the learning rate can help keep ReLU units from dying.\n  * **Vanishing gradients:** The gradients for the lower layers (closer to the input) can become very small. When the gradients vanish toward 0 for the lower layers, these layers train very slowly or they do not train at all. The ReLU activation function can help prevent vanishing gradients.\n  * **Dropout regularization:** This type of regularization is useful for neural networks. It works by randomly “dropping out” unit activations in a network for a single gradient step. The more you drop out, the stronger the regularization:\n    * 0.0 = No dropout regularization.\n    * 1.0 = Drop out everything. The model learns nothing.\n    * Values between 0.0 and 1.0 = More useful.\n\n* * *\n\nFor vanishing gradients, using ReLU instead of sigmoid can help. For exploding gradients, batch normalization, grading, and clipping can help. Lowering your learning rate can help with both exploding gradients and dead ReLU units.\n\n* * *\n\nLosses are good now, but in case you want to reduce your training loss further, you can try the following techniques:\n\n  * Increase the depth and width of your neural network.\n  * If the features don't add information relative to existing features, try a different feature.\n  * Decrease the learning rate.\n  * Increase the depth and width of your layers (to increase predictive power).\n  * If you have lots of data, use held‐out test data.\n  * If you have little data, use cross‐validation or bootstrapping.\n\nThe model you have trained is not converging and it is bouncing around. This can be due to the following:\n\n  * Features might not have predictive power.\n  * Raw data might not comply with the defined schema.\n  * Learning rate seems high, and you need to decrease it.\n  * Reduce your training set to few examples to obtain a very low loss.\n  * Start with one or two features (and a simple model) that you know have predictive power and see if the model overperforms your baseline.",
            "subsections": {},
            "summary": "* **Regularization** is used to prevent overfitting by reducing fluctuation of coefficients\n* L1 and L2 regularization methods are used, with L1 for feature selection and L2 for stable models\n    * L1 regularization reduces features, while L2 regularization keeps weights small but not zero\n    * L1 is robust to outliers, but L2 is not\n\n* **Common issues in neural networks**:\n  * _Exploding gradients_: large weights cause gradients to become too large; batch normalization and lower learning rate can help\n  * _Dead ReLU units_: once a unit outputs 0, it can get stuck and gradients cannot flow through; lowering learning rate can help\n  * _Vanishing gradients_: gradients for lower layers become very small; using ReLU activation function can help\n\n* **Techniques to reduce training loss**:\n  * Increase depth and width of neural network\n  * Decrease learning rate\n    * Using dropout regularization: randomly dropping out unit activations"
          }
        },
        "summary": "* *_Understanding Model Performance_*\n  * **Bias:** High bias means high error rate in training data, paying little attention to data.\n  * *_High Variance:_* High variance means low performance on test data, overfitting to training data.\n\n*_The Challenge of Generalization_*\n\n A model with too little capacity cannot learn, and a model with too much capacity can overfit."
      },
      "Summary": {
        "content": "In this chapter, we discussed model parallelism and data parallelism and some strategies to use while training a TensorFlow model with a model and data parallel approach.\n\nYou learned about modeling techniques such as what loss function to choose while training a neural network. We covered important concepts related to training neural networks such as gradient descent, learning rate, batch size, epoch, and hyperparameters.\n\nWe also covered the importance of these hyperparameters when training a neural network—for example, what happens when we decrease learning rate or increase the epoch while training the network.\n\nWe discussed transfer learning and the advantages of using it. We also covered semi‐supervised learning: when you need semi‐supervised learning along with its limitations.\n\nWe discussed data augmentation techniques. You use online augmentation when you have a large dataset and offline augmentation when you have a small dataset. We also covered techniques such as rotation and flipping to augment your existing dataset.\n\nFinally, we discussed model underfitting, model overfitting, and regularization concepts.",
        "subsections": {},
        "summary": "* **Training Neural Networks**: Overview of key concepts including loss function, gradient descent, learning rate, batch size, epoch, and hyperparameters.\n    * _Hyperparameter Tuning_: Importance of adjusting hyperparameters such as learning rate and epoch to optimize network performance.\n    * _Transfer Learning and Semi-Supervised Learning_: Strategies for using transfer learning and understanding the limitations of semi-supervised learning."
      },
      "Exam Essentials": {
        "content": "* **Choose either framework or model parallelism.** Understand multinode training strategies to train a large neural network model. The strategy can be data parallel or model parallel. Also, know what strategies can be used for distributed training of TensorFlow models.\n  * **Understand modeling techniques.** Understand when to use which loss function (sparse cross‐entropy versus categorical cross‐entropy). Understand important concepts such as gradient descent, learning rate, batch size, and epoch. Also understand that these are hyperparameters and know some strategies to tune these hyperparameters to minimize loss or error rate while training your model.\n  * **Understand transfer learning.** Understand what transfer learning is and how it can help with training neural networks with limited data as these are pretrained models trained on large datasets.\n  * **Use semi‐supervised learning (SSL).** Understand semi‐supervised learning and when you need to use this method. Also know the limitations of SSL.\n  * **Use data augmentation.** You need to understand data augmentation and how you can apply it in your ML pipeline (online versus offline). You also need to learn some key data augmentation techniques such as flipping, rotation, GANs, and transfer learning.\n  * **Understand model generalization and strategies to handle overfitting and underfitting.** You need to understand bias variance trade‐off while training a neural network. Know the strategies to handle underfitting as well as strategies to handle overfitting, such as regularization. You need to understand the difference between L1 and L2 regularization and when to apply which approach.",
        "subsections": {},
        "summary": "* **Training Strategies**\n    * _Model parallelism_: Split large neural networks across multiple nodes for efficient computation\n    * Data parallelism: Train on multiple GPUs or machines in parallel using data replication\n    * Distributed training of TensorFlow models: Use TensorFlow's distributed training features\n\n* **Hyperparameter Tuning and Optimization**\n    * _Loss functions_: Choose between sparse cross-entropy and categorical cross-entropy depending on task type\n    * Gradient descent, learning rate, batch size, and epoch: Understand the role of each hyperparameter in model performance\n    * Regularization techniques (L1, L2): Apply to prevent overfitting\n\n* **Transfer Learning**\n    * _Pretrained models_: Leverage pre-trained models for limited data scenarios with high accuracy\n\n* **Additional Techniques**\n    * Data augmentation: Apply techniques like flipping, rotation, and GANs to increase dataset size\n    * Semi-supervised learning (SSL): Use when labeled data is scarce, but unlabeled data is abundant"
      },
      "Review Questions": {
        "content": "1. Your data science team trained and tested a deep neural net regression model with good results in development. In production, six months after deployment, the model is performing poorly due to a change in the distribution of the input data. How should you address the input differences in production?\n     1. Perform feature selection on the model using L1 regularization and retrain the model with fewer features.\n     2. Retrain the model, and select an L2 regularization parameter with a hyperparameter tuning service.\n     3. Create alerts to monitor for skew, and retrain the model.\n     4. Retrain the model on a monthly basis with fewer features.\n  2. You are an ML engineer of a start‐up and have trained a deep neural network model on Google Cloud. The model has low loss on the training data but is performing worse on the validation data. You want the model to be resilient to overfitting. Which strategy should you use when retraining the model?\n     1. Optimize for the L1 regularization and dropout parameters.\n     2. Apply an L2 regularization parameter of 0.4, and decrease the learning rate by a factor of 10.\n     3. Apply a dropout parameter of 0.2.\n     4. Optimize for the learning rate, and increase the number of neurons by a factor of 2.\n  3. You are a data scientist of a Fintech company training a computer vision model that predicts the type of government ID present in a given image using a GPU‐powered virtual machine on Compute Engine. You use the following parameters: Optimizer: SGD, Image shape = 224x224, Batch size = 64, Epochs = 10, and Verbose = 2.\n\nDuring training you encounter the following error: “ResourceExhaustedError: out of Memory (oom) when allocating tensor.” What should you do?\n\n     1. Change the optimizer.\n     2. Reduce the batch size.\n     3. Change the learning rate.\n     4. Reduce the image shape.\n  4. You are a data science manager of an EdTech company and your team needs to build a model that predicts whether images contain a driver's license, passport, or credit card. The data engineering team already built the pipeline and generated a dataset composed of 20,000 images with driver's licenses, 2,000 images with passports, and 2,000 images with credit cards. You now have to train a model with the following label map: ['drivers_license', 'passport', 'credit_card']. Which loss function should you use?\n     1. Categorical hinge\n     2. Binary cross‐entropy\n     3. Categorical cross‐entropy\n     4. Sparse categorical cross‐entropy\n  5. You are a data scientist training a deep neural network. During batch training of the neural network, you notice that there is an oscillation in the loss. How should you adjust your model to ensure that it converges?\n     1. Increase the size of the training batch.\n     2. Decrease the size of the training batch.\n     3. Increase the learning rate hyperparameter.\n     4. Decrease the learning rate hyperparameter.\n  6. You have deployed multiple versions of an image classification model on the Vertex AI platform. You want to monitor the performance of the model versions over time. How should you perform this comparison?\n     1. Compare the loss performance for each model on a held‐out dataset.\n     2. Compare the loss performance for each model on the validation data.\n     3. Compare the mean average precision across the models using the Continuous Evaluation feature.\n     4. Compare the ROC curve for each model.\n  7. You are training an LSTM‐based model to summarize text using the following hyperparameters: epoch = 20, batch size =32, and learning rate = 0.001. You want to ensure that training time is minimized without significantly compromising the accuracy of your model. What should you do?\n     1. Modify the epochs parameter.\n     2. Modify the batch size parameter.\n     3. Modify the learning rate parameter.\n     4. Increase the number of epochs.\n  8. Your team needs to build a model that predicts whether images contain a driver's license or passport. The data engineering team already built the pipeline and generated a dataset composed of 20,000 images with driver's licenses and 5,000 images with passports. You have transformed the features into one‐hot encoded value for training. You now have to train a model to classify these two classes; which loss function should you use?\n     1. Sparse categorical cross‐entropy\n     2. Categorical cross‐entropy\n     3. Categorical hinge\n     4. Binary cross‐entropy\n  9. You have developed your own DNN model with TensorFlow to identify products for an industry. During training, your custom model converges but the tests are giving unsatisfactory results. What do you think is the problem and how can you fix it? (Choose two.)\n     1. You have to change the algorithm to XGBoost.\n     2. You have an overfitting problem.\n     3. You need to increase your learning rate hyperparameter.\n     4. The model is complex and you need to regularize the model using L2.\n     5. Reduce the batch size.\n  10. As the lead ML engineer for your company, you are building a deep neural network TensorFlow model to optimize customer satisfaction. Your focus is to minimize bias and increase accuracy for the model. Which other parameter do you need to consider so that your model converges while training and doesn't lead to underfit or overfit problems?\n     1. Learning rate\n     2. Batch size\n     3. Variance\n     4. Bagging\n  11. As a data scientist, you are working on building a DNN model for text classification using Keras TensorFlow. Which of the following techniques should not be used? (Choose two.)\n     1. Softmax function\n     2. Categorical cross‐entropy\n     3. Dropout layer\n     4. L1 regularization\n     5. K‐means\n  12. As the ML developer for a gaming company, you are asked to create a game in which the characters look like human players. You have been asked to generate the avatars for the game. However, you have very limited data. Which technique would you use?\n     1. Feedforward neural network\n     2. Data augmentation\n     3. Recurrent neural network\n     4. Transformers\n  13. You are working on building a TensorFlow model for binary classification with a lot of categorical features. You have to encode them with a limited set of numbers. Which activation function will you use for the task?\n     1. One‐hot encoding\n     2. Sigmoid\n     3. Embeddings\n     4. Feature cross\n  14. You are the data scientist working on building a TensorFlow model to optimize the level of customer satisfaction for after‐sales service. You are struggling with learning rate, batch size, and epoch to optimize and converge your model. What is your problem in ML?\n     1. Regularization\n     2. Hyperparameter tuning\n     3. Transformer\n     4. Semi‐supervised learning\n  15. You are a data scientist working for a start‐up on several projects with TensorFlow. You need to increase the performance of the training and you are already using caching and prefetching. You want to use GPU for training but you have to use only one machine to be cost‐effective. Which of the following tf distribution strategies should you use?\n     1. MirroredStrategy\n     2. MultiWorkerMirroredStrategy\n     3. TPUStrategy\n     4. ParameterServerStrategy",
        "subsections": {},
        "summary": "* **Resource Exhaustion Error**: Reduce the batch size to decrease memory allocation.\n    * Decrease the learning rate hyperparameter or increase it, whichever is less computationally intensive.\n    * Increase the image shape.\n\n* **Loss Function for Multiclass Classification**: Categorical cross-entropy\n    * Use a larger model with more parameters.\n    * Regularize the model using L2 to reduce overfitting.\n\n* **Convergence Issues in Batch Training**:\n  * Decrease the size of the training batch.\n  * Increase the learning rate hyperparameter.\n  * Reduce the number of epochs.\n\n* **Monitoring Model Performance**: Compare the mean average precision across models using Continuous Evaluation feature.\n    * Compare the ROC curve for each model.\n    * Use a held-out dataset for comparison.\n\n* **Optimizing Training Time without Compromising Accuracy**:\n  * Modify the batch size parameter.\n  * Increase the number of epochs.\n  * Modify the learning rate parameter.\n\n* **Binary Classification with Categorical Features**: One-hot encoding\n    * Sigmoid activation function.\n    * Feature cross-activation.\n\n* **Hyperparameter Tuning and Learning Rate Issues**: Hyperparameter tuning\n    * Regularization issues.\n    * Transformer model limitations."
      }
    },
    "summary": ""
  },
  "Chapter 8Model Training and Hyperparameter Tuning": {
    "content": "",
    "subsections": {
      "Ingestion of Various File Types into Training": {
        "content": "Data for training can be in various types, such as, for example, the following:\n\n  * Structured data such as tables from an on‐premise database or CSV files\n  * Semi‐structured data such as PDFs or JSON files\n  * Unstructured data such as chats, emails, audio, images, or videos\n\nAlso, this data can be either batch data or real‐time streaming data—for example, data streamed from Internet of Things (IoT) sensors. Moreover, the data can be small, such as a few megabytes, or it can be petabyte scale. As you learned in previous chapters, before training the data, it's important to clean and transform it so that you can apply ML training on it. The Google Cloud analytics portfolio provides tools to collect, store, process, and analyze this data, as shown in Figure 8.1.\n\n**FIGURE 8.1** Google Cloud data and analytics overview\n\nLet's look at the various steps.",
        "subsections": {
          "Collect": {
            "content": "If you need to collect batch or streaming data from various sources such as IoT devices, e‐commerce websites, or any third‐party applications, this can be done by Google Cloud services.\n\n  * **Pub/Sub and Pub/Sub Lite for real‐time streaming** : Pub/Sub is a serverless scalable service (1 KB to 100 GB with consistent performance) for messaging and real‐time analytics. Pub/Sub can both publish and subscribe across the globe, regardless of where your ingestion or processing applications live. It has deep integration with processing services (Dataflow) and analytics services (BigQuery). You can directly stream data from a third party to BigQuery using Pub/Sub. Pub/Sub Lite is also a serverless offering that optimizes for cost over reliability. Pub/Sub Lite is good for workloads with more predictable and consistent load.\n  * **Datastream for moving on premise Oracle and MySQL databases to Google Cloud data storage** : Datastream is a serverless and easy‐to‐use change data capture (CDC) and replication service. It allows you to synchronize data across heterogeneous databases and applications reliably and with minimal latency and downtime. Datastream supports streaming from Oracle and MySQL databases into Cloud Storage. Datastream is integrated with Dataflow, and it leverages Dataflow templates to load data into BigQuery, Cloud Spanner, and Cloud SQL.\n  * **BigQuery Data Transfer Service:** You can load data from the following sources to BigQuery using the BigQuery Data Transfer Service:\n    * Data warehouses such as Teradata and Amazon Redshift\n    * External cloud storage provider Amazon S3\n    * Google software as a service (SaaS) apps such as Cloud Storage, Google Ads, etc.\n\nAfter you configure a data transfer, the BigQuery Data Transfer Service automatically loads data into BigQuery on a regular basis.",
            "subsections": {},
            "summary": "* **Google Cloud Services for Batch and Streaming Data**\n  * Pub/Sub and Pub/Sub Lite for real-time streaming\n    + Serverless scalable messaging service with global reach\n    + Integrates with Dataflow, BigQuery, and analytics services\n  * Datastream for migrating on-premises databases to Google Cloud\n    + Change data capture and replication service for heterogeneous databases\n    + Supports Oracle and MySQL databases into Cloud Storage\n  * BigQuery Data Transfer Service for loading external data\n    + Loads from Teradata, Amazon Redshift, and cloud storage providers"
          },
          "Process": {
            "content": "Once you have collected the data from various sources, you need tools to process or transform the data before it is ready for ML training. The following sections cover some of the tools that can help.",
            "subsections": {
              "Cloud Dataflow": {
                "content": "Cloud Dataflow is a serverless, fully managed data processing or ETL service to process streaming and batch data. Dataflow used Apache Beam before open‐sourcing its SDK. Apache Beam offers exactly‐once streaming semantics, which means it has mechanisms in place to process each message not only at least once, but exactly one time. This simplifies your business logic because you don't have to worry about handling duplicates or errors.\n\nData flows are processing pipelines that perform a set of actions, and this allows you to build pipelines, monitor their execution, and transform and analyze data. It aims to address the performance issues of MapReduce when building pipelines. Many Hadoop workloads can be done easily and be more maintainable with Dataflow. Cloud Dataflow allows you to process and read data from source Google Cloud data services to sinks as shown in Figure 8.2.\n\n**FIGURE 8.2** Cloud Dataflow source and sink\n\nFor example, you can process and transform data from Cloud Pub/Sub (Source) to BigQuery (Sink) using Cloud Dataflow.",
                "subsections": {},
                "summary": "* **Cloud Dataflow**: A serverless, fully managed data processing service for streaming and batch data\n    * _Applies exactly-once streaming semantics_, ensuring each message is processed exactly once.\n    * Enables building and monitoring data processing pipelines with improved performance over MapReduce."
              },
              "Cloud Data Fusion": {
                "content": "We covered Cloud Data Fusion in Chapter 3, “Feature Engineering.” Cloud Data Fusion is a UI‐based ETL tool with no code implementation.",
                "subsections": {},
                "summary": "* _Cloud Data Fusion_ \n    * A UI-based ETL (Extract, Transform, Load) tool\n    * No code implementation required"
              },
              "Cloud Dataproc": {
                "content": "Dataproc is a fully managed and highly scalable service for running Apache Spark, Apache Flink, Presto, and 30+ open‐source tools and frameworks. Dataproc lets you take advantage of open‐source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning them off when you do not need them.\n\nDataproc has built‐in integration with other Google Cloud Platform services such as BigQuery, Cloud Storage, Cloud Bigtable, Cloud Logging, and Cloud Monitoring, which provides a complete data platform. For example, you can use Dataproc to effortlessly ETL (Extract Transform Load) terabytes of raw log data directly into BigQuery for business reporting. Dataproc uses the Hadoop Distributed File System (HDFS) for storage. Additionally, Dataproc automatically installs the HDFS‐compatible Cloud Storage connector, which enables the use of Cloud Storage in parallel with HDFS. Data can be moved in and out of a cluster through upload/download to HDFS or Cloud Storage. Table 8.1 summarizes connectors with Dataproc.\n\n**TABLE 8.1** Dataproc connectors\n\nConnector | Description\n---|---\nCloud Storage connector | This is by default available on Dataproc and this connector helps run Apache Hadoop or Apache Spark jobs directly on data in Cloud Storage. Store your data in Cloud Storage and access it directly with Cloud Storage connector. You do not need to transfer it into HDFS first.\nBigQuery connector | You can use BigQuery connector to enable programmatic read/write access to BigQuery. This is an ideal way to process data that is stored in BigQuery as command‐line access is not exposed. The BigQuery connector is a library that enables Spark and Hadoop applications to process data from BigQuery and write data to BigQuery. BigQuery Spark connector is used for Spark and BigQuery Hadoop connector is used for Hadoop.\nBigQuery Spark connector | Apache Spark SQL connector for Google BigQuery. The connector supports reading Google BigQuery tables into Spark's DataFrames, and writing DataFrames back into BigQuery. This is done by using the Spark SQL Data Source API to communicate with BigQuery.\nCloud Bigtable with Dataproc | Bigtable is an excellent option for any Apache Spark or Hadoop uses that require Apache HBase. Bigtable supports the Apache HBase APIs so it is easy to use Bigtable with Dataproc.\nPub/Sub Lite Spark connector | The Pub/Sub Lite Spark connector supports Pub/Sub Lite as an input source to Apache Spark Structured Streaming in the default micro‐batch processing and experimental continuous processing modes.\n\nAll Cloud Dataproc clusters come with the BigQuery connector for Hadoop built in so that you can easily and quickly read and write BigQuery data to and from Cloud Dataproc.",
                "subsections": {},
                "summary": "**Dataproc Overview**\n\n* A fully managed service for running Apache Spark, Flink, Presto, and 30+ open-source tools\n* Allows use of open-source data tools for batch processing, querying, streaming, and machine learning\n* Provides automated cluster creation, management, and cost savings\n\n**Integration with Google Cloud Platform Services**\n\n* Integrates with BigQuery, Cloud Storage, Cloud Bigtable, Cloud Logging, and Cloud Monitoring\n* Enables effortless ETL of raw log data into BigQuery for business reporting"
              },
              "Cloud Composer": {
                "content": "There are multiple ways of creating, running, and managing workflows such as running Cron tasks, using Cron jobs, and scripting and creating custom applications. Each approach has pros and cons. More importantly, there is management overhead in all the approaches here. That is why we have Cloud Composer, which is a fully managed data workflow orchestration service that allows you to author, schedule, and monitor pipelines. Cloud Composer is built on Apache Airflow, and pipelines are configured as directed acyclic graphs (DAGs) using Python.\n\nIt supports hybrid and multicloud architecture to manage your workflow pipelines whether it's on‐premises, in multiple clouds, or fully within Google Cloud.\n\nCloud Composer provides end‐to‐end integration with Google Cloud products including BigQuery, Dataflow, Dataproc, Datastore, Cloud Storage, Pub/Sub, and Vertex AI Platform, which gives users the freedom to fully orchestrate their pipeline.",
                "subsections": {},
                "summary": "* *_Cloud Composer_* is a managed workflow orchestration service that simplifies creating, running, and managing workflows with minimal management overhead\n    * _Supports hybrid and multicloud architecture_ for on-premises, multiple clouds, or Google Cloud-only deployments\n    * _Provides end-to-end integration_ with Google Cloud products like BigQuery and Dataflow"
              },
              "Cloud Dataprep": {
                "content": "We covered Cloud Dataprep in Chapter 3. Cloud Dataprep is a UI‐based ETL tool to visually explore, clean, and prepare structured and unstructured data for analysis, reporting, and machine learning at any scale.",
                "subsections": {},
                "summary": "* **Cloud Dataprep**: _UI-based ETL tool for visual exploration, cleaning, and preparing data for analysis and machine learning_\n    * Prepares data for any scale\n    * Covers structured and unstructured data"
              },
              "Summary of Processing Tools": {
                "content": "A summary of all the tools we have covered so far is shown in Figure 8.3.\n\n**FIGURE 8.3** Summary of processing tools on GCP",
                "subsections": {},
                "summary": "* *_GCP Processing Tools_* \n    * **Data Ingestion**: Cloud Pub/Sub, Cloud Storage\n    * **Data Transformation**: Cloud Data Fusion, Cloud Transformations\n    * **Data Analysis**: BigQuery, Cloud Dataproc\n    * **Machine Learning**: AutoML, AI Platform"
              }
            },
            "summary": "* *_Data Preprocessing Tools_* \n  * **Data cleaning and handling**\n  * **Feature engineering**\n  * **Data transformation**"
          },
          "Store and Analyze": {
            "content": "After you use tools to collect and process data, you need scalable storage to store various types of data. Table 8.2 shows the Google Cloud Storage options for various types of data for machine learning. Refer to Table 8.2 for data storage guidance on GCP for machine learning.\n\n**TABLE 8.2** Data storage guidance on GCP for machine learning\n\nType of Data | Product\n---|---\nTabular data | BigQuery, BigQuery ML\nImage, video, audio, and unstructured data | Google Cloud Storage\nUnstructured data | Vertex Data Labeling\nStructured data |  Vertex AI Feature Store\nFor AutoML image, video, text | Vertex AI Managed Datasets\n\n* * *\n\nAvoid storing data in block storage like Network File System (NFS) and VMs. Similarly, avoid reading data directly from databases like Cloud SQL.\n\n* * *\n\nStore image, video, audio, and unstructured data in large container formats on Cloud Storage. Also, combine many individual images, videos, and audio clips into large files because this will improve your read and write throughput to Google Cloud Storage. Aim for files of at least 100 MB, and between 100 and 10,000 shard.\n\n* * *\n\nFor TensorFlow workloads, store data as sharded TFRecord files, and for any other framework, store as Avro files in Google Cloud Storage. You can also use TF I/O to manage data in Parquet format for TensorFlow training. TensorFlow I/O is a collection of file systems and file formats that are not available in TensorFlow's built‐in support. It provides useful extra dataset, streaming, and file system extensions.\n\n* * *",
            "subsections": {},
            "summary": "* **Data Storage Guidance**\n  - *_Table Type_* | *_Google Cloud Product_*\n  - Tabular data | BigQuery, BigQuery ML\n  - Image/video/audio & unstructured data | Google Cloud Storage\n  - Unstructured data | Vertex Data Labeling\n  - Structured data | Vertex AI Feature Store"
          }
        },
        "summary": "* **Data Types**: Structured (e.g., databases), semi-structured (e.g., PDFs), and unstructured (e.g., chats, emails)\n* **Data Sources**: Batch data, real-time streaming data (e.g., IoT sensors)\n* **Data Volumes**: Small (< few megabytes) to petabyte scale"
      },
      "Developing Models in Vertex AI Workbench by Using Common Frameworks": {
        "content": "In this section, we will cover how you can create a Jupyter Notebook environment to train, tune, and deploy your model using Vertex AI Workbench.\n\nVertex AI Workbench is a Jupyter Notebook–based development environment for the entire data science workflow. You can interact with Vertex AI and other Google Cloud services from within a Vertex AI Workbench instance's Jupyter Notebook. We introduced Vertex AI Workbench in Chapter 6, “Building Secure ML Pipelines,” when we covered the security aspect of creating an instance and the two types of notebooks:\n\n  * **User‐managed notebook** The oldest version of Jupyter Notebooks within the Vertex AI platform (previously the AI platform). They have more control and fewer features compared to managed notebooks. See Table 8.3 for the feature differences.\n  * **Managed notebook** The latest offering. These come with features such as automatic shutdown, integration with data storage, and the ability to schedule the notebook for an execution.\n\n**TABLE 8.3** Differences between managed and user‐managed notebooks\n\nManaged notebook | User‐managed notebook\n---|---\n**Automated shutdown for idle instances:** Choosing a managed notebook will shut down your Jupyter Notebooks when not in use. This feature helps save costs because the instances will shut down when not in use automatically. | **Automated shutdown for idle instances:** This feature is not supported out of the box. However, you can create a monitor to see when instances are idle using Cloud Monitoring and Cloud Functions and shut them down when not in use.\n**UI integration with Cloud Storage and BigQuery:** From within JupyterLab's navigation menu on a managed notebooks instance, you can use the Cloud Storage and BigQuery integration to browse data and other files that you have access to and load data into your notebook. | **UI integration with Cloud Storage and BigQuery:** There is no UI integration. However, you can use the BigQuery connector to connect to BigQuery data using code or you can also use the BigQuery magic (`%%`) command to run BigQuery SQLl commands on a Jupyter Notebook.\nFor Cloud Storage, you can use `gsutil` commands to write and read data in user‐managed notebooks.\n**Automated notebook runs** : You can set a notebook to run on a recurring schedule. Even while your instance is shut down, Vertex AI Workbench will run your notebook file and save the results for you to look at and share with others. | **Automated notebook runs** : This feature is not supported. You would use Cloud Scheduler to schedule the training jobs or the notebook.\n**Custom containers:** You can add your own custom container images to a managed notebook Jupyter instance. | **Custom containers** : You have the choice to add custom containers.\n**Dataproc or Serverless Spark integration:** You can process data quickly by running a notebook on a Dataproc cluster or Serverless Spark. This feature is in private preview now. After your cluster is set up, you can run a notebook file on it without leaving the JupyterLab interface. | **Dataproc or Serverless Spark integration:** This feature is not supported.\n**Frameworks:** All the frameworks are already preinstalled when you create the managed notebook. You can choose any framework supported once you have created a managed notebook. | **Frameworks:** You have the choice to create only one framework from all the supported frameworks. For example, TensorFlow, R, and PyTorch are supported by the user‐managed notebook, but while creating, you can only choose one framework for your JupyterLab environment.\n**Network and security:** You can run this in the VPC in the same project. Shared VPC control is not yet supported for managed notebooks. | **Network and security:** For users who have specific networking and security needs, user‐managed notebooks can be the best option. You can use VPC Service Controls to set up a user‐managed notebooks instance within a service perimeter and implement other built‐in networking and security features.\n\nBoth notebook options are prepackaged with JupyterLab and have a preinstalled suite of deep learning packages such as TensorFlow and PyTorch frameworks as well as other packages such as R, Spark, and Python. You can use CPU‐only or GPU‐enabled instances. Both notebook instances also integrate with GitHub so that you can sync your notebook with a GitHub repository.\n\nFirst we'll go over some of the feature differences between managed notebooks and user‐managed notebooks and then we will cover how to create and use these notebooks for model development.\n\nNext we explore the features of each type of notebook by creating a notebook in the Vertex AI GCP console.",
        "subsections": {
          "Creating a Managed Notebook": {
            "content": "Go to Vertex AI and choose Enable All APIs. By default in the GCP, all the service APIs are disabled. After enabling the APIs, click on Workbench, go to the Managed Notebooks tab, and choose New Notebook. Click the Create button to create the notebook using the default settings (Figure 8.4).\n\n**FIGURE 8.4** Creating a managed notebook\n\nNotice that there is a monthly billing estimate for running the notebook at the right‐hand side.\n\nAfter some time, you will find that the notebook is created and the Open JupyterLab button is enabled (see Figure 8.5). Click on the Open JupyterLab button to get inside your managed JupyterLab environment.\n\n**FIGURE 8.5** Opening the managed notebook\n\n* * *\n\nNotice the Upgrade available option with managed notebooks in Figure 8.5. Managed notebook instances are dual‐disk, with one boot disk and one data disk. The upgrade process upgrades the boot disk to a new image while preserving your data on the data disk. It's a manual upgrade process you have a choice to do. To learn more about how it works, check out `https://cloud.google.com/vertex-ai/docs/workbench/managed/upgrade`.\n\n* * *",
            "subsections": {},
            "summary": "### Creating a Managed Notebook in Vertex AI\nCreate a new notebook using the Workbench by enabling all APIs and clicking on New Notebook.\n\n### Key Steps:\n\n*   Click Create to create the notebook with default settings.\n*   A monthly billing estimate is displayed for running the notebook.\n*   Wait until the notebook is created, then click Open JupyterLab to access your environment."
          },
          "Exploring Managed JupyterLab Features": {
            "content": "After you click Open JupyterLab, you are redirected to the screen shown in Figure 8.6. You will find that you have all the frameworks available to use in this environment, including Serverless Spark as well as PySpark installed locally. The Serverless Spark feature is to run the Dataproc cluster within the notebook.\n\n**FIGURE 8.6** Exploring frameworks available in a managed notebook\n\nIn the tutorials folder, you will find existing notebooks to help you get started with building and training your models on this JupyterLab environment. JupyterLab also comes with a terminal option to run terminal commands on the entire notebook.\n\nNext, let's explore features supported by this type of notebook.",
            "subsections": {},
            "summary": "* *JupyterLab Environment Features:*\n  * _Existing notebooks for model building and training_\n  * Serverless Spark feature for running Dataproc clusters\n  * Terminal option for running terminal commands in notebooks"
          },
          "Data Integration": {
            "content": "Click the Browse GCS icon on the left navigation bar (Figure 8.7) to browse and load data from cloud storage folders.\n\n**FIGURE 8.7** Data integration with Google Cloud Storage within a managed notebook",
            "subsections": {},
            "summary": "* Open the _Browse_ icon in the left navigation bar\n* Load data from cloud storage folders using GCS\n* Browse and select desired files or folders to integrate into a managed notebook"
          },
          "BigQuery Integration": {
            "content": "Click the BigQuery icon on the left as shown in Figure 8.8 to get data from your BigQuery tables. The interface also has an Open SQL editor option to query these tables without leaving the JupyterLab interface.\n\n**FIGURE 8.8** Data Integration with BigQuery within a managed notebook",
            "subsections": {},
            "summary": "* *_Access BigQuery data_* \n    * Click **BigQuery icon** to retrieve data from tables\n    * Use _Open SQL editor_ option to query tables directly in JupyterLab"
          },
          "Ability to Scale the Compute Up or Down": {
            "content": "Click n1‐standard‐4 (see Figure 8.9). You will get the option to modify the hardware of the Jupyter environment. You can also attach a GPU to this instance without leaving the environment.",
            "subsections": {},
            "summary": "* **Attach GPU**: Click on *_n1-standard-4_* to access a virtual machine with adjustable hardware and options to attach a GPU _without exiting the environment_."
          },
          "Git Integration for Team Collaboration": {
            "content": "Click the left navigation branch icon and you get to the screen to integrate your existing git repository or clone a repository for project collaboration (see Figure 8.10). Alternatively, you can use the terminal and run the command `git clone <your‐repository name>` to clone your repository.\n\n**FIGURE 8.9** Scaling up the hardware from a managed notebook\n\n**FIGURE 8.10** Git integration within a managed notebook",
            "subsections": {},
            "summary": "* To integrate or clone a git repository, click on the left navigation branch icon\n* Alternatively, use the terminal with the command `git clone <your‐repository name>` to clone your repository"
          },
          "Schedule or Execute a Notebook Code": {
            "content": "Click Python, write “hello world,” and click run cell to execute the cell manually using the triangle black arrow. In order to execute this notebook automatically, click Execute as shown in Figure 8.11.\n\n**FIGURE 8.11** Scheduling or executing code in the notebook\n\nAfter clicking Execute, you will see the screen shown in Figure 8.12: Submit notebooks to Executor. This functionality is basically used to set up or trigger Vertex AI training jobs or to deploy scheduling of a Vertex AI Prediction endpoint without leaving the Jupyter interface.\n\n**FIGURE 8.12** Submitting the notebook for execution\n\nScroll down to the Type option to schedule the notebook run as shown in Figure 8.13.\n\n**FIGURE 8.13** Scheduling the notebook for execution\n\nWe covered all the features of managed notebooks such as integration with Cloud Storage/BigQuery, the ability to scale up or scale down the notebook hardware, git integration, and the ability to schedule the notebook either for a onetime run or at a scheduled time.",
            "subsections": {},
            "summary": "* To execute a cell manually, click the triangle black arrow to run it.\n    * To execute a cell automatically, click **Execute** \n        * This sends the code to be executed to Vertex AI without leaving the Jupyter interface."
          },
          "Creating a User‐Managed Notebook": {
            "content": "With user‐managed notebooks, you need to choose the execution environment during the creation of the notebook, as shown in Figure 8.14.\n\n**FIGURE 8.14** Choosing TensorFlow framework to create a user‐managed notebook\n\nYou need to pick a specific framework from options such as Python 3, TensorFlow, R, JAX, Kaggle, PyTorch, and so on. Let's go ahead and create a TensorFlow notebook without GPUs by following the path shown in Figure 8.14. Then click Create to create a user‐managed notebook (see Figure 8.15).\n\n**FIGURE 8.15** Create a user‐managed TensorFlow notebook\n\nYou can see that user‐managed notebooks come with advanced options to configure networking with shared VPCs.\n\nOnce the notebook is active, you will see the Open JupyterLab option. Click it to explore the notebook (see Figure 8.16).\n\n**FIGURE 8.16** Exploring the network\n\nThis notebook comes with TensorFlow already installed. You have git integration and terminal access available with user‐managed notebooks.\n\nYou can use both managed and user‐managed notebooks to trigger Vertex AI training using the Vertex AI Python SDK or to run predictions using the Vertex AI Training Python SDK.\n\n* * *\n\nYou do not need a large hardware or compute instance to develop the code in JupyterLab because the hardware is not used for training. You perform training using Vertex AI Training or Prediction SDKs. These APIs or SDKs create a training container outside the JupyterLab environment that shuts down automatically when training is over. Similarly, the prediction SDKs or APIs start the prediction container and host it and give you an endpoint to get predictions outside the notebook hardware environment.\n\n* * *\n\nIn this section we covered some feature differences between user‐managed and managed notebooks. We also covered how to create them.\n\nIn the next section, you will learn how to train a model using Vertex AI Training.",
            "subsections": {},
            "summary": "**Creating User-Managed Notebooks**\n\nTo create a user-managed notebook, choose a framework such as TensorFlow during creation.\n \n* _Options_: Python 3, TensorFlow, R, JAX, Kaggle, PyTorch\n* _Benefits_: Advanced options for networking with shared VPCs and automatic shutdown after training\n\nYou can access the notebook through Open JupyterLab and use tools like git integration and terminal access.\n\n* _Note_: No need for large hardware or compute instance as training is done outside the environment using Vertex AI APIs/SDKs."
          }
        },
        "summary": "* **Vertex AI Workbench Overview**\n  * A Jupyter Notebook-based environment for data science workflows\n  * Interacts with Vertex AI and other Google Cloud services\n  * Two types of notebooks: Managed and User-managed\n    * _Managed notebook_: Automated shutdown, UI integration with Cloud Storage and BigQuery, automated notebook runs, custom containers, and frameworks preinstalled.\n    * _User-managed notebook_: More control and fewer features compared to managed notebooks."
      },
      "Training a Model as a Job in Different Environments": {
        "content": "Vertex AI supports two types of training:\n\n  * **AutoML:** AutoML lets you create and train a model with minimal technical effort. We covered AutoML in detail in Chapter 4, “Choosing the Right ML Infrastructure.”\n  * **Custom training:** Custom training lets you create a training application optimized for your targeted outcome. You have complete control over training application functionality. Namely, you can target any objective, use any algorithm, develop your own loss functions or metrics, or do any other customization. In the following sections, we will focus on custom training with Vertex AI.",
        "subsections": {
          "Training Workflow with Vertex AI": {
            "content": "With Vertex AI, you can use the following options to create training jobs or resources for training either AutoML or custom models:\n\n  * **Training pipelines:** Training pipelines are the primary model training workflow in Vertex AI. You can use training pipelines to create an AutoML‐trained model or a custom‐trained model. For custom‐trained models, training pipelines orchestrate custom training jobs and hyperparameter tuning with additional steps like adding a dataset or uploading the model to Vertex AI for prediction serving. Figure 8.17 shows the training pipeline in the Vertex AI console.\n\n**FIGURE 8.17** Training in the Vertex AI console\n\nSource: Google LLC.\n\n  * **Custom jobs:** Custom jobs specify how Vertex AI runs your custom training code, including worker pools, machine types, and settings related to your Python training application and custom container. Custom jobs are only used by custom‐trained models and not AutoML models. When you create a custom job, you specify settings that Vertex AI needs to run your training code.\n  * **Hyperparameter tuning jobs:** Hyperparameter tuning searches for the best combination of hyperparameter values by optimizing metric values across a series of trials. Hyperparameter tuning is only used by custom‐trained models and not AutoML models. We will cover this later in this chapter in the section “Hyperparameter Tuning.”\n\n* * *\n\nFor prebuilt container training, Vertex AI supports PyTorch, TensorFlow, Scikit, and XGBoost framework. For TensorFlow models, you can use TensorFlow Hub to choose a model to deploy on GCP. TensorFlow Hub is a repository of trained models that is optimized to use on GCP.\n\n* * *",
            "subsections": {},
            "summary": "* **Training in Vertex AI**: Training pipelines create AutoML or custom models, orchestrating hyperparameter tuning and model uploading.\n    * _Supported frameworks_: PyTorch, TensorFlow, Scikit, XGBoost\n    * _TensorFlow Hub_ option for optimized GCP deployment"
          },
          "Training Dataset Options in Vertex AI": {
            "content": "Training starts with a dataset. In Vertex AI you have two choices for storing and managing datasets:\n\n  * **No Managed Dataset:** You can use data stored in Google Cloud Storage or data stored in BigQuery for training. Vertex AI managed notebooks integrates with Cloud Storage and BigQuery. So that you can directly use data for training from these sources, you can use Google Cloud Storage FUSE in Vertex AI Workbench to specify datasets in GCS buckets for the No Managed Dataset option. GCS FUSE provides a way for applications to upload and download Cloud Storage objects using standard file system semantics.\n  * **Managed dataset:** This is the preferred way to train machine learning models on Vertex AI. The following are some of the advantages of using a managed dataset for training on Vertex AI:\n    * Manage your datasets in a central location.\n    * Easily create labels and multiple annotation sets.\n    * Create tasks for human labeling using integrated data labeling.\n    * Track lineage to models for governance and iterative development.\n    * Compare model performance by training AutoML and custom models using the same datasets.\n    * Generate data statistics and visualizations.\n    * Automatically split data into training, test, and validation sets.\n\n* * *\n\nYou can configure your custom training jobs to mount NFS shares to the container where your code is running. This allows your jobs to access remote files as though they are local with high throughput and low latency.\n\n* * *\n\nVertex AI training provides two choices for training: prebuilt containers and custom containers. Let's go over each in detail.",
            "subsections": {},
            "summary": "### Training on Vertex AI\n#### Dataset Options\n\n* **No Managed Dataset**: Use data stored in Google Cloud Storage or BigQuery directly.\n\t+ _Direct access to data from Cloud Storage and BigQuery_\n* **Managed Dataset**: Store and manage datasets in a central location for easier label creation, annotation, and governance.\n\n#### Training Jobs\n* Configure training jobs to mount remote files via NFS shares for high throughput and low latency."
          },
          "Pre‐built Containers": {
            "content": "Vertex AI supports scikit‐learn, TensorFlow, PyTorch, and XGBoost containers hosted on the container registry for prebuilt training. Google manages all the container images and their versions. In order to set up a training with prebuilt container, please follow the steps below:\n\n  1. You need to organize your code according to the application structure as shown in Figure 8.18. You should have a root folder with `setup.py` and a trainer folder with `task.py` (training code), which is the entry point for a Vertex AI training job. You can use standard dependencies or libraries not in the prebuilt container by specifying it in `setup.py`.\n  2. You need to upload your training code as Python source distribution to a Cloud Storage bucket before you start training with a prebuilt container. You use the `sdist` command to create a source distribution—for example, `python setup.py sdist ‐‐formats=gztar,zip`. Figure 8.18 shows the folder structure and architecture for a prebuilt container.\n\n**FIGURE 8.18** Vertex AI training architecture for a prebuilt container\n\nFigure 8.19 shows how the GCP console looks to set up a Vertex AI training pipeline for a prebuilt container.\n\nAfter choosing the training container, you can choose what type of compute instances you want to train.\n\nThe following command builds a Docker image based on a prebuilt training container image and your local Python code, pushes the image to Container Registry, and creates a custom job.\n[code]\n     gcloud ai custom-jobs create \\\n      --region=LOCATION \\\n      --display-name=JOB_NAME \\\n      --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,executor-image-uri=EXECUTOR_IMAGE_URI,local-package-path=WORKING_DIRECTORY,script=SCRIPT_PATH\n\n    LOCATION: The region where the container or Python package will be run.\n    JOB_NAME: Required. A display name for the CustomJob.\n    MACHINE_TYPE: The type of machine. Refer to available machine types for training.\n    REPLICA_COUNT: The number of worker replicas to use. In most cases, set this to 1 for your first worker pool.\n    EXECUTOR_IMAGE_URI: The URI of the container image that runs the provided code. Refer to the available pre‐built containers for training.\n    This image acts as the base image for the new Docker image that you are building with this command.\n    WORKING_DIRECTORY: A directory in your local file system containing the entry point script that runs your training code (see the following list item).\n\n    The path relative to WORKING_DIRECTORY on your local file system, to the script that is the entry point for your training code. For example, if you want to run /custom job/trainer/task.py and WORKING_DIRECTORY is /custom job, then use trainer/task.py for this value.\n\n[/code]\n\n**FIGURE 8.19** Vertex AI training console for pre‐built containersSource: Google LLC.",
            "subsections": {},
            "summary": "* **Vertex AI Setup**: Organize code according to Vertex AI structure (_root folder with `setup.py` and `trainer/task.py`)\n* **Training Pipeline**: Upload Python source distribution to Cloud Storage bucket, then use the `gcloud ai custom-jobs create` command to build a Docker image, push it to Container Registry, and create a custom job.\n    * Specify:\n        *_LOCATION_* (region)\n        *_JOB_NAME_* (display name for CustomJob)\n        *_MACHINE_TYPE_* (machine type for training)\n        *_EXECUTOR_IMAGE_URI_* (prebuilt container image)"
          },
          "Custom Containers": {
            "content": "A custom container is a Docker image that you create to run your training application. The following are some of the benefits of using custom container versus prebuilt:\n\n  * **Faster start‐up time.** If you use a custom container with your dependencies preinstalled, you can save the time that your training application would otherwise take to install dependencies when starting up.\n  * **Use the ML framework of your choice.** If you can't find a Vertex AI prebuilt container with the ML framework you want to use, you can build a custom container with your chosen framework and use it to run jobs on Vertex AI. For example, you can use a custom container to train with PyTorch.\n  * **Extended support for distributed training.** With custom containers, you can do distributed training using any ML framework.\n  * **Use the newest version.** You can also use the latest build or minor version of an ML framework. For example, you can build a custom container to train with tf‐nightly.\n\nFigure 8.20 shows the architecture of how custom container training works on Google Cloud. You build a container using a Dockerfile and training file with a recommended folder structure. You build your Dockerfile and push it to an Artifact Registry. For Vertex AI training with a custom container, you specify the dataset (managed), custom container image URI you pushed to the repository, and compute (VM) instances to train on.\n\n**FIGURE 8.20** Vertex AI training architecture for custom containers\n\nTo create a custom container for Vertex AI training, you need to create a Dockerfile, build the Dockerfile, and push it to an Artifact Registry. These are the steps:\n\n  1. Create a custom container and training file.\n     1. Set up your files as per required folder structure: you need to create a root folder. Then create a Dockerfile and a folder named trainer/. In that trainer folder you need to create `task.py` (your training code). This `task.py` file is the entry point file.\n     2. Create a Dockerfile. Your Dockerfile needs to include commands as shown in the following code that includes tasks such as choose a base image, install additional dependencies, copy your training code to the image, and configure the entry point to invoke your training code.\n[code]# Specifies base image and tag\n             FROM image:tag\n             WORKDIR /root\n\n             # Installs additional packages\n             RUN pip install pkg1 pkg2 pkg3\n\n             # Downloads training data\n             RUN curl https://example-url/path-to-data/data-filename --output /root/data-filename\n\n             # Copies the trainer code to the docker image\n             COPY your-path-to/model.py /root/model.py\n             COPY your-path-to/task.py /root/task.py\n\n             # Sets up the entry point to invoke the trainer\n             ENTRYPOINT [\"python\", \"task.py\"]\n\n[/code]\n\n  2. Build and run your Docker container.\n\nYou can use the following code to build the docker image:\n[code] export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n          export REPO_NAME=REPOSITORY_NAME\n          export IMAGE_NAME=IMAGE_NAME\n          export IMAGE_TAG=IMAGE_TAG\n          export IMAGE_URI=us-central1-docker.pkg.dev/${PROJECT_ID}/${REPO_NAME}/${IMAGE_NAME}:${IMAGE_TAG}\n\n          docker build -f Dockerfile -t ${IMAGE_URI} ./\n\n[/code]\n\nFor local run, use the following command:\n[code] docker run ${IMAGE_URI}\n\n[/code]\n\n  3. Push the container image to Artifact Registry.\n\nIf the Docker build works, you can push the container to Artifact Registry by running the following command.\n[code] docker push ${IMAGE_URI}\n\n[/code]\n\n* * *\n\nBy running your training job in a custom container, you can use ML frameworks, non‐ML dependencies, libraries, and binaries that are not otherwise supported on Vertex AI.\n\n* * *\n\n  4. After pushing the image to the repository, you can start training by creating a custom job using the following command:\n[code]gcloud ai custom-jobs create \\\n           --region=_LOCATION_ \\\n           --display-name=_JOB_NAME_ \\\n           --worker-pool-spec=machine-type=_MACHINE_TYPE_ ,replica-count=_REPLICA_COUNT_ ,container-image-uri=_CUSTOM_CONTAINER_IMAGE_URI_\n\n[/code]",
            "subsections": {},
            "summary": "* **Custom Containers for Vertex AI**\n* \n    * _Use custom containers to run training applications with preferred ML frameworks and dependencies._\n    * _Faster start-up time, extended support for distributed training, and use of the latest framework versions._\n* \n    * _Steps to create a custom container:_\n        1. Create a Dockerfile and push it to an Artifact Registry.\n            ```dockerfile\nFROM image:tag\nWORKDIR /root\n\nRUN pip install pkg1 pkg2 pkg3\n\nCOPY your-path-to/model.py /root/model.py\nCOPY your-path-to/task.py /root/task.py\n\nENTRYPOINT [\"python\", \"task.py\"]\n```\n\n        2. Build and run the Docker container.\n            ```bash\ndocker build -f Dockerfile -t ${IMAGE_URI} .\ndocker run ${IMAGE_URI}\n```\n        \n        3. Push the container image to Artifact Registry.\n            ```bash\ndocker push ${IMAGE_URI}\n```"
          },
          "Distributed Training": {
            "content": "You need to specify multiple machines (nodes) in a training cluster in order to run a distributed training job with Vertex. The training service allocates the resources for the machine types you specify. The running job on a given node is called a _replica_. A group of replicas with the same configuration is called a worker pool. You can configure any custom training job as a distributed training job by defining multiple worker pools. You can also run distributed training within a training pipeline or a hyperparameter tuning job. Use an ML framework that supports distributed training. In your training code, you can use the CLUSTER_SPEC or TF_CONFIG environment variables to reference specific parts of your training cluster. Please refer to Table 8.4 to understand worker pool tasks in distributed training.\n\n**TABLE 8.4** Worker pool tasks in distributed training\n\nPosition in workerPoolSpecs[] | Task Performed in Cluster\n---|---\nFirst (`workerPoolSpecs[0]`) | Primary, chief, scheduler, or “master.” Exactly ‐one replica is designated the _primary replica_. This task manages the others and reports status for the job as a whole.\nSecond (`workerPoolSpecs[1]`) | Secondary, replicas, workers.\nOne or more replicas may be designated as _workers_. These replicas do their portion of the work as you designate in your job configuration.\nThird (`workerPoolSpecs[2]`) | Parameter servers and Reduction Server\n\n  * Parameter servers: If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters to coordinate shared model state between the workers.\n  * _Reduction Server_ is an all‐reduce algorithm that can increase throughput and reduce latency for distributed training. You can use this option if you are doing distributed training with GPU workers. Your training code uses TensorFlow or PyTorch and is configured for multi‐host data‐parallel training with GPUs using NCCL all‐reduce.\n\nFor TensorFlow, you can also use Horovod**,** which supports Tensor Fusion to batch small tensors for all‐reduce.\nFourth (`workerPoolSpecs[3]`) | Evaluators: These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n\nSetting up Vertex AI Reduction Server distributed training: Vertex AI makes Reduction Server available in a Docker container image that you can use for one of your worker pools during distributed training. You need to follow some prerequisites mentioned in this link: `https://cloud.google.com/vertex‐ai/docs/training/distributed‐training` before you set up a Reduction Server container in your workerpoolspec[2].The following command provides an example of how to create a `CustomJob` resource that uses Reduction Server for distributed training by setting up workerpool:\n[code]\n     gcloud ai custom-jobs create \\\n      --region=_LOCATION_ \\\n      --display-name=_JOB_NAME_ \\\n      --worker-pool-spec=machine-type=n1-highmem-96,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,accelerator-count=8,container-image-uri=_CUSTOM_CONTAINER_IMAGE_URI_ \\\n      --worker-pool-spec=machine-type=n1-highmem-96,replica-count=4,accelerator-type=NVIDIA_TESLA_V100,accelerator-count=8,container-image-uri=_CUSTOM_CONTAINER_IMAGE_URI_ \\\n      --worker-pool-spec=machine-type=n1-highcpu-16,replica-count=16,container-image-uri=us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\n\n[/code]\n\nWe covered distributed training strategies using TensorFlow in Chapter 7, “Model Building.” In order to learn more how Reduction Server works, refer to this deep‐dive blog, https://cloud.google.com/blog/topics/developers‐practitioners/optimize‐training‐performance‐reduction‐server‐vertex‐ai",
            "subsections": {},
            "summary": "* _Distributed Training_ : Specify multiple machines in a training cluster to run a distributed training job with Vertex.\n    * **Task Distribution**: Configure worker pools with specific tasks:\n        * Primary: manages the others and reports status for the job\n        * Secondary: performs replicas' work\n        * Parameter servers and Reduction Server (optional): coordinates shared model state between workers\n* _Reduced Training Performance_ : Use Reduction Server to increase throughput and reduce latency, especially with GPU workers."
          }
        },
        "summary": "* *_Vertex AI Training Methods_* \n  * AutoML: Minimal technical effort for creating and training models\n  * Custom training: Full control over model functionality and targeted outcome"
      },
      "Hyperparameter Tuning": {
        "content": "Hyperparameters are parameters of the training algorithm itself that are not learned directly from the training process. Let us look at an example of a simple feed‐forward neural network trained DNN model using gradient descent. One of the hyperparameters in the gradient descent is the learning rate, which we covered in Chapter 7. The learning rate must be set up front before any learning can begin. Therefore, finding the right learning rate involves choosing a value, training a model, evaluating it, and trying again.\n\nFigure 8.21 summarizes the difference between a parameter and hyperparameter.\n\n**FIGURE 8.21** ML model parameter and hyperparameter",
        "subsections": {
          "Why Hyperparameters Are Important": {
            "content": "Hyperparameter selection is crucial for the success of your neural network architecture because hyperparameters heavily influence the behavior of the learned model. For instance, if the learning rate is set too low, the model will miss important patterns in the data, but if it's too large, the model will find patterns in accidental coincidences too easily. Finding good hyperparameters involves solving two problems:\n\n  * **How to efficiently search the space of possible hyperparameters:** There are algorithms to help with hyperparameter search space optimization. The Vertex AI default algorithm applies Bayesian optimization to arrive at the optimal solution. Table 8.5 is a summary of the search algorithms supported by Vertex AI hyperparameter tuning.\n  * **How to manage a large set of experiments for hyperparameter tuning:** Without an automated technology like Vertex AI hyperparameter tuning, you would need to make manual adjustments to the hyperparameters over the course of many training runs to arrive at the optimal values. Hyperparameter tuning makes the process of determining the best hyperparameter settings easier and less tedious. Hyperparameter tuning takes advantage of the compute infrastructure of Google Cloud to test different hyperparameter configurations when training your model. It can give you optimized values for hyperparameters, which maximizes your model's predictive accuracy.\n\n**TABLE 8.5** Search algorithm options for hyperparameter tuning on GCP\n\nGrid Search | Random Search | Bayesian Search\n---|---|---\nGrid search is essentially an exhaustive search through a manually specified set of hyperparameters. For example, a model with hyperparameters learning_rate and num_layers.\nLearning_rate = [0.1,0.5,1]\nNum_layers = [5,10,20]\nGrid search will train on each pair [learning rate, num_layers] for each training, for example, [0.1,5] and measures performance either using cross‐validation on the training set or a separate validation set. The hyperparameter setting that gives the maximum score is the final output. | Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. This works best under the assumption that not all hyperparameters are equally important. In this search pattern, random combinations of parameters are considered in every iteration. | This is the Vertex AI default search algorithm. Bayesian optimization in turn takes into account past evaluations when choosing the hyperparameter set to evaluate next.\nOne of the drawbacks of grid search is that when it comes to dimensionality, it suffers when evaluating the number of hyperparameters grows exponentially. | The downside of random search, however, is that it doesn't use information from prior experiments to select the next setting. | It uses past evaluations.\nTo use grid search, all parameters must be of type INTEGER, CATEGORICAL, or DISCRETE. | The chances of finding the optimal parameter are comparatively higher in random search than grid search because of the random search pattern where the model might end up being trained on the optimized parameters without any aliasing. | Hyperparameter tuning uses an algorithm called Gaussian Process Bandits, which is a form of Bayesian optimization.",
            "subsections": {},
            "summary": "* **Hyperparameter Tuning**: Finding optimal hyperparameters for neural networks to maximize predictive accuracy.\n  * _Automated algorithms_ use compute infrastructure to test different configurations and provide optimized values.\n  * *_Search algorithms_*:\n    * Grid search: Exhaustive search through a manually specified set of hyperparameters\n    * Random search: Uses random combinations of parameters, considering the assumption that not all are equally important\n    * Bayesian optimization (default Vertex AI algorithm): Takes into account past evaluations to choose the next hyperparameter set"
          },
          "Techniques to Speed Up Hyperparameter Optimization": {
            "content": "There are several techniques to speed up hyperparameter optimization:\n\n  * If you have a large dataset, use a simple validation set instead of cross‐validation. This will increase the speed by a factor of ~k, compared to k‐fold cross‐validation. This approach won't work well if you don't have enough data.\n  * Parallelize the problem across multiple machines by using distributed training with hyperparameter optimization. Each machine can fit models with different choices of hyperparameters. This will increase the speed by a factor of ~n for n machines.\n  * Avoid redundant computations by pre‐computing or caching the results of computations that can be reused for subsequent model fits.\n  * If using grid search, decrease the number of hyperparameter values you are willing to consider. This can lead to potentially large speedups because the total number of combinations scales multiplicatively.\n\n* * *\n\nYou can improve performance by using the random search algorithm since it uses fewer trails.\n\n* * *\n\nTo learn more, visit the following pages:\n\n`https://cloud.google.com/blog/products/ai-machine-learning/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization`\n\n`https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview`",
            "subsections": {},
            "summary": "* **Hyperparameter Optimization Techniques**\n  * Use simple validation set instead of cross-validation for large datasets (factor ~k)\n  * Parallelize on multiple machines using distributed training with hyperparameter optimization (factor ~n)\n  * Pre-compute and cache redundant computations to avoid unnecessary re-running\n  * Reduce number of hyperparameter values to consider in grid search for speedups"
          },
          "How Vertex AI Hyperparameter Tuning Works": {
            "content": "Hyperparameter tuning works by running multiple trials of your training application with values for the hyperparameters you specify and set within limits. Hyperparameter tuning requires communication between the Vertex AI and training application. The training application defines all the information that your model needs. You define the hyperparameters (variables) that you want to adjust and target variables that are used to evaluate each trial. Hyperparameter tuning optimizes target variables that you specify, called hyperparameter metrics. Metrics must be numeric.\n\nWhen configuring a hyperparameter tuning job, you define the name and goal of each metric. The goal specifies whether you want to tune your model to maximize or minimize the value of this metric.\n\nFollow these steps to configure hyperparameter tuning jobs using gcloud CLI commands with custom jobs:\n\n  1. For a custom container, install the `cloud‐ml hypertune` Python package in your Dockerfile.\n  2. In `trainer/task.py`, add hyperparameter tuning code in main function and add arguments as shown in the code below.\n[code]Def main():\n           args = get_args()\n           train_data, validation_data = create_dataset()\n           model = create_model(args.num_units, args.learning_rate, args.momentum)\n           history = model.fit(train_data, epochs=NUM_EPOCHS, validation_data=validation_data)\n\n           # DEFINE METRIC\n           hp_metric = history.history[‘val_accuracy’][-1]\n\n           hpt = hypertune.HyperTune()\n           hpt.report_hyperparameter_tuning_metric(\n                hyperparameter_metric_tag=’accuracy’,\n                metric_value=hp_metric,\n                global_step=NUM_EPOCHS)\n\n[/code]\n\n  3. Build and push this container to Artifact Registry. After setting the training, you can configure a hyperparameter tuning job using a training pipeline or a custom job.\n\n**FIGURE 8.22** Configure hyperparameter tuning by training the pipeline UISource: Google LLC.\n\nYou can create a hyperparameter tuning job using the console, gcloud, command line, Java, or Python for a prebuilt container or a custom container. The following steps walk you through setting the hyperparameter tuning by a custom job using gcloud CLI commands:\n\n  1. Create a YAML file named `config.yaml` with some API fields that you want to specify for your new `HyperparameterTuningJob`:\n[code]studySpec:\n           metrics:\n           - metricId: METRIC_ID\n              goal: METRIC_GOAL\n           parameters:\n           - parameterId: HYPERPARAMETER_ID\n              doubleValueSpec:\n                minValue: DOUBLE_MIN_VALUE\n                maxValue: DOUBLE_MAX_VALUE\n          trialJobSpec:\n           workerPoolSpecs:\n              - machineSpec:\n                  machineType: MACHINE_TYPE\n                replicaCount: 1\n                containerSpec:\n                  imageUri: CUSTOM_CONTAINER_IMAGE_URI\n\n         METRIC_ID: The name of a hyperparameter metric to optimize. Your training code must report this metric when it runs.\n         METRIC_GOAL: The goal for your hyperparameter metric, either MAXIMIZE or MINIMIZE.\n         HYPERPARAMETER_ID: The name of a hyperparameter to tune. Your training code must parse a command‐line flag with this name. For this example, the hyperparameter must take floating‐point values.\n         DOUBLE_MIN_VALUE: The minimum value (a number) that you want Vertex AI to try for this hyperparameter.\n         DOUBLE_MAX_VALUE: The maximum value (a number) that you want Vertex AI to try for this hyperparameter.\n         MACHINE_TYPE: The type of VM to use for training.\n         CUSTOM_CONTAINER_IMAGE_URI: The URI of a Docker container image with your training code.\n\n[/code]\n\n_Note: You can also specify a search algorithm. If one is not specified, Google Cloud picks up a Bayesian algorithm by default_.\n\n  2. In the same directory as your `config.yaml` file, run the following shell command to create a custom job to start hyperparameter tuning:\n[code]gcloud ai hp-tuning-jobs create \\\n              --region=_LOCATION_ \\\n              --display-name=_DISPLAY_NAME_ \\\n              --max-trial-count=_MAX_TRIAL_COUNT_ \\\n              --parallel-trial-count=_PARALLEL_TRIAL_COUNT_ \\\n              --config=config.yaml\n\n         LOCATION: The region where you want to create the HyperparameterTuningJob.\n         DISPLAY_NAME: A display name of your choice.\n         MAX_TRIAL_COUNT: The maximum number of trials to run.\n         PARALLEL_TRIAL_COUNT: The maximum number of trials to run in parallel.\n\n[/code]\n\nYou can track the progress of this job on a Vertex AI console in Vertex AI Training.",
            "subsections": {},
            "summary": "* **Hyperparameter Tuning**: Runs multiple trials with adjusted hyperparameters to optimize target variables (hyperparameter metrics) and improve model performance.\n* **Key Steps**:\n    * Install `cloud-ml hypertune` package for custom container\n    * Configure metric definitions and hyperparameter tuning code in Python\n* **Creating a Custom Job**: \n    * Create YAML file with API fields for HyperparameterTuningJob\n        * `studySpec`: defines metrics, parameters, and trial job specs\n            - `metrics`: specifies hyperparameter metric to optimize\n            - `parameters`: defines hyperparameter to tune with specified range\n            - `trialJobSpec`: configures worker pool and container specs"
          },
          "Vertex AI Vizier": {
            "content": "Vertex AI Vizier is a black‐box optimization service that helps you tune hyperparameters in complex ML models. Below are the criteria to use Vertex AI Vizier to train ML models:\n\n  * Vertex AI Vizier doesn't have a known objective function to evaluate.\n  * Vertex AI Vizier is too costly to evaluate by using the objective function, usually due to the complexity of the system.\n  * Vertex AI Vizier optimizes hyperparameters of ML models, but it can also perform other optimization tasks such as tuning model parameters and works with any system that you can evaluate.\n\nSome of the examples or use cases where you can use Vertex AI Vizier for hyperparameter tuning are as follows:\n\n  * Optimize the learning rate, batch size, and other hyperparameters of a neural network recommendation engine.\n  * Optimize usability of an application by testing different arrangements of user interface elements.\n  * Minimize computing resources for a job by identifying an ideal buffer size and thread count.\n  * Optimize the amounts of ingredients in a recipe to produce the most delicious version.",
            "subsections": {
              "How Vertex AI Vizier Differs from Custom Training": {
                "content": "Vertex AI Vizier is an independent service for optimizing complex models with many parameters. It can be used for both ML and non‐ML use cases. It can be used with training jobs or with other systems (even multicloud). Hyperparameter tuning for custom training is a built‐in feature that uses Vertex AI Vizier for training jobs. It helps determine the best hyperparameter settings for an ML model. By default, it uses Bayesian optimization.\n\nFor more info, see\n\n`https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#aiplatform_create_hyperparameter_tuning_job_python_package_sample-gcloud`",
                "subsections": {},
                "summary": "* **Vertex AI Vizier**: Independent service for optimizing complex models with many parameters.\n    * Supports both ML and non‐ML use cases, and can be used in training jobs or with other systems (including multicloud).\n        *_Hyperparameter Tuning_*: Built-in feature that uses Bayesian optimization to determine the best hyperparameter settings for an ML model."
              }
            },
            "summary": "* **Vertex AI Vizier**: Black-box optimization service for complex ML models\n    * _Tunes hyperparameters without a known objective function_\n    * _Optimizes model parameters and works with any evaluable system_\n    * _Useful for cost-sensitive tasks or when traditional evaluation methods are impractical_"
          }
        },
        "summary": "* **Hyperparameters vs Model Parameters**: \n    * _Model parameters_ are learned during training, while _hyperparameters_ are set before training\n* **Learning Rate Hyperparameter**\n    * The learning rate must be chosen before training can begin\n* **Importance of Choosing Hyperparameters**: Finding the right hyperparameters is crucial for effective model training"
      },
      "Tracking Metrics During Training": {
        "content": "In the following sections, we will cover how you can track and debug machine learning model metrics by using tools such as an interactive shell, the TensorFlow Profiler, and the What‐If Tool.",
        "subsections": {
          "Interactive Shell": {
            "content": "Using an interactive shell to inspect your training container can help you debug problems with your training code or your Vertex AI configuration. You can browse the file system and run debugging utilities in each prebuilt container or custom container running on Vertex AI. You can use an interactive shell to run tracing and profiling tools, analyze GPU usage, and check Google Cloud permissions available to the container. You can enable an interactive shell for a custom training resource by setting the enableWebAccess API field to true while setting up custom jobs programmatically or checking Enable training debugging in the Vertex AI console training pipeline, as shown in Figure 8.23.\n\n**FIGURE 8.23** Enabling an interactive shell in the Vertex AI consoleSource: Google LLC.\n\nWhen Vertex AI finishes running your job or trial or changes from RUNNING state to COMPLETED state, you will lose access to your interactive shell. The interactive shell is only available while the job is in the RUNNING state. Once the job is running, you will see links to the interactive shell web page on the job details page. The Launch Web Terminal link is created for each node for the job, as shown in Figure 8.24.\n\n**FIGURE 8.24** Web terminal to access an interactive shellSource: Google LLC.\n\nYou can also find logs in Cloud Monitoring as Vertex AI exports metrics to Cloud Monitoring. Vertex AI also shows some of these metrics in the Vertex AI console. You can view the logs by clicking on the View logs link on the Vertex AI training page, shown in Figure 8.24.\n\nTable 8.6 lists some other tools to track metrics or profile training metrics.\n\n**TABLE 8.6** Tools to track metric or profile training metrics\n\nVisualize Python Execution with py‐spy | Retrieve Information about GPU Usage | Analyze Performance with Perf\n---|---|---\npy‐spy is a sampling profiler for Python programs. It lets you visualize what your Python program is spending time on without restarting the program or modifying the code in any way. | GPU‐enabled containers running on nodes with GPUs typically have several command‐line tools preinstalled that can help you monitor GPU usage. You can use `nvidia‐smi` to monitor GPU utilization of various processes or use `nvprof` to collect a variety of GPU profiling information. | Perf lets you analyze the performance of your training node. It's a way to do Linux profiling with performance counters.",
            "subsections": {},
            "summary": "* **Interactive Shell in Vertex AI**: \n    * An interactive shell is available for debugging and troubleshooting, running tracing and profiling tools.\n    * Available while job is in RUNNING state, but access is lost after completion.\n\n* **Vertex AI Logs and Metrics**:\n    * View logs by clicking on the View logs link\n    * Metrics are exported to Cloud Monitoring and shown in Vertex AI console\n\n* **Tools for Tracking Training Metrics**: \n    * `_py-spy_`: visualizes Python execution time, analyzes performance with Perf\n    * `nvidia-smi` and `nvprof`: monitors GPU usage"
          },
          "TensorFlow Profiler": {
            "content": "Vertex AI TensorBoard is an enterprise‐ready managed version of TensorBoard. Vertex AI TensorBoard Profiler lets you monitor and optimize your model training performance by helping you understand the resource consumption of training operations. This can pinpoint and fix performance bottlenecks to train models faster and cheaper. There are two ways to access the Vertex AI TensorBoard Profiler dashboard from the Google Cloud console:\n\n  * From the custom jobs page\n  * From the experiments page\n\nTo capture a profiling session, your training job must be in the RUNNING state.\n\nTF Profiler allows you to profile your remote Vertex AI training jobs on demand and visualize the results in Vertex TensorBoard. For details, see Profile model training performance using Profiler at `https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-profiler`.",
            "subsections": {},
            "summary": "* **Vertex AI TensorBoard Profiler**: monitors and optimizes model training performance\n    * _Helps pinpoint and fix performance bottlenecks to train models faster and cheaper_\n    * Available from the custom jobs page or experiments page in the Google Cloud console\n        * Only works when the training job is in the RUNNING state"
          },
          "What‐If Tool": {
            "content": "You can use the What‐If Tool (WIT) within notebook environments to inspect AI Platform Prediction models through an interactive dashboard. The What‐If Tool integrates with TensorBoard, Jupyter Notebooks, Colab notebooks, and JupyterHub. It is also preinstalled on Vertex AI Workbench user‐managed notebooks and TensorFlow instances.\n\nTo use WIT, you need to install the witwidget library, which is already installed in Vertex AI Workbench. Then configure WitConfigBuilder to either inspect a model or compare two models. The following is some example code to inspect a model:\n[code]\n     PROJECT_ID = 'YOUR_PROJECT_ID'\n     MODEL_NAME = 'YOUR_MODEL_NAME'\n     VERSION_NAME = 'YOUR_VERSION_NAME'\n     TARGET_FEATURE = 'mortgage_status'\n     LABEL_VOCAB = ['denied', 'approved']\n     config_builder = (WitConfigBuilder(test_examples.tolist(), features.columns.tolist() + ['mortgage_status'])\n      .set_ai_platform_model(PROJECT_ID, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n      .set_target_feature(TARGET_FEATURE)\n      .set_label_vocab(LABEL_VOCAB)\n\n[/code]\n\nThen pass the config builder to WitWidget, and set a display height.\n[code]\n     WitWidget(config_builder, height=800)\n\n[/code]\n\nThe What‐If Tool displays an interactive visualization within your notebook. Try this Colab notebook to explore how the What‐If Tool works:\n\n`https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb`",
            "subsections": {},
            "summary": "* **What-If Tool**: Interactive dashboard for inspecting AI Platform Prediction models\n    * Integrates with TensorBoard, Jupyter Notebooks, Colab notebooks, and JupyterHub\n    * Preinstalled on Vertex AI Workbench user-managed notebooks and TensorFlow instances\n*   _How to use_:\n    1. Install `witwidget` library\n    2. Configure `WitConfigBuilder`\n    3. Pass config builder to `WitWidget`\n    * Example code: \n        ```python\n     PROJECT_ID = 'YOUR_PROJECT_ID'\n     MODEL_NAME = 'YOUR_MODEL_NAME'\n     VERSION_NAME = 'YOUR_VERSION_NAME'\n     TARGET_FEATURE = 'mortgage_status'\n     LABEL_VOCAB = ['denied', 'approved']\n     config_builder = (WitConfigBuilder(test_examples.tolist(), features.columns.tolist() + ['mortgage_status'])\n      .set_ai_platform_model(PROJECT_ID, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n      .set_target_feature(TARGET_FEATURE)\n      .set_label_vocab(LABEL_VOCAB)\n\n     WitWidget(config_builder, height=800)\n[/code]"
          }
        },
        "summary": "* **Tracking ML Model Metrics**: *Using tools like an interactive shell, TensorFlow Profiler, and What-If Tool*\n \n  (No sub-points necessary)"
      },
      "Retraining/Redeployment Evaluation": {
        "content": "After the model is trained and deployed in the real world, over time model performance changes; your model is sensitive to change as user behavior and training data keeps changing with time. Although all machine learning models decay, the speed of decay varies with time. Data drift, concept drift, or both mostly cause this. Let us understand these terms.",
        "subsections": {
          "Data Drift": {
            "content": "_Data drift_ is a change in the statistical distribution of production data from the baseline data used to train or build the model. You can detect data drift if the feature attribution of your model changes or the data itself changes. For example, suppose you built a model with temperature data collected from an IoT sensor in Fahrenheit degrees but the unit changed to Celsius. This means there has been a change in your input data, so the data has drifted. You can detect data drift by examining the feature distribution or correlation between features or checking the data schema over baseline using a monitoring system.",
            "subsections": {},
            "summary": "* **Data Drift**: A change in the statistical distribution of production data from baseline data used to train a model\n*  *Causes:* Changes in input data, such as unit conversions (_e.g._ temperature from Fahrenheit to Celsius)\n*  *Detection Methods:* Examine feature distribution, correlation between features, or check data schema over baseline"
          },
          "Concept Drift": {
            "content": "_Concept drift_ is a phenomenon where the statistical properties of the target variable you're trying to predict change over time. For example, say you build a model to classify positive and negative sentiment of Reddit feed around certain topics. Over time, people's sentiments about these topics change. Tweets belonging to a positive sentiment may evolve over time to be negative.\n\nIn order to detect drift, you need to monitor your deployed model, which can be done by Vertex AI Model Monitoring. We will cover this topic in detail in Chapter 12, “Model Monitoring, Tracking and Auditing Metadata.”",
            "subsections": {},
            "summary": "* _Concept Drift_: Change in the statistical properties of the target variable over time\n* Causes: Shifts in user behavior or opinions on a particular topic\n* *Example*: Sentiment around specific topics changes as people's opinions evolve"
          },
          "When Should a Model Be Retrained?": {
            "content": "Decide on a retraining strategy based on Table 8.7. We will discuss how you can use Vertex AI pipelines and Vertex AI Model Monitoring to set up retraining in Chapter 12.\n\n**TABLE 8.7** Retraining strategies\n\nPeriodic training | You can choose an interval such as weekly, monthly, or yearly for retraining your model. It depends on how frequently your training data gets updated. Retraining your model based on an interval only makes sense if it aligns with your business use case. The selection of a random period for model retraining can give you a worse model than the previous model.\n---|---\nPerformance‐based trigger |  If your model performance falls below your set threshold, which is the ground truth or baseline data, this automatically triggers the retraining pipeline. This approach assumes that you have implemented a sophisticated monitoring system in production.\nData changes trigger | If data drift happens, that should trigger a build for model retraining. Usually model performance changes in production, which can lead to data drift.\nRetraining on demand | This is a manual and traditional way of retraining your models that employs traditional techniques.",
            "subsections": {},
            "summary": "* *_Retraining Strategies_* \n  * **Periodic Training**: Retrain based on a fixed interval (e.g., weekly, monthly, yearly) when training data updates.\n  * **Performance-based Trigger**: Automatically trigger retraining when model performance falls below a set threshold.\n  * *_Data Changes Trigger_*: Trigger build for model retraining upon data drift or changes in production."
          }
        },
        "summary": "* **Model Decay**: Machine learning models lose performance over time due to changing user behavior and training data\n    * _Causes:_\n        * *_Data Drift_*: Distribution shift in the data\n        * *_Concept Drift_*: Change in the underlying relationships between variables"
      },
      "Unit Testing for Model Training and Serving": {
        "content": "Testing machine learning systems is hard because the code, model, and data all control the behavior of the system. Testing ML models means testing the model code as well as testing the data and the model. Unit tests allow us to test whether a unit of code is functioning as we expect it to.\n\nModel testing involves explicit checks for behaviors that we expect our model to follow. Testing allow us to identify some bugs early. We can run some tests without needing trained parameters. These tests include the following:\n\n  * Checking the shape of your model output and ensuring it aligns with the labels in your dataset.\n  * Checking the output ranges and ensuring it aligns with your expectations (e.g., the output of a classification model should be a distribution with class probabilities that sum to 1).\n  * Making sure a single gradient step on a batch of data yields a decrease in your loss.\n  * Making assertions about your datasets.\n  * Checking for label leakage between your training and validation datasets.",
        "subsections": {
          "Testing for Updates in API Calls": {
            "content": "You can test updates to an API call by retraining your model, but that would be resource intensive. Rather, you can write a unit test to generate random input data and run a single step of gradient descent to complete without runtime errors.",
            "subsections": {},
            "summary": "* **Testing API Updates**: Write a unit test to generate random input data and run a single step of gradient descent\n    * Use this approach to test for runtime errors and functional correctness\n    * _Avoid_ retraining the entire model, as it would be resource-intensive"
          },
          "Testing for Algorithmic Correctness": {
            "content": "To check your model for algorithmic correctness, follow these steps:\n\n  1. Train your model for a few iterations and verify that the loss decreases.\n  2. Train your algorithm without regularization. If your model is complex enough, it will memorize the training data and your training loss will be close to 0.\n  3. Test specific subcomputations of your algorithm. For example, you can test that a part of CNN runs once per element of the input data.",
            "subsections": {},
            "summary": "* To verify algorithmic correctness, train your model and check:\n  * Loss decreases over iterations\n  * Training loss approaches 0 without regularization\n  * Specific subcomputations run as expected (e.g., CNN running once per input element)"
          }
        },
        "summary": "* _Testing Machine Learning Systems_\n* *_Types of Tests_*:\n    * **Model Input Validation**: checking model output shape, output ranges, and loss convergence\n    * **Data Integrity Checks**: verifying dataset assertions and ensuring no label leakage\n    * **Unit Tests**: testing individual units of code to ensure they function as expected"
      },
      "Summary": {
        "content": "In this chapter, we discussed various file types such as structured, unstructured, and semi‐structured and how they can be stored and ingested for AI/ML workloads in GCP. We divided the file ingestion into Google Cloud Platform into stages such as collect, process, store, and analyze and discussed services that can help at each stage. We covered Pub/Sub and Pub/Sub Lite to collect real‐time data and BigQuery Data Transfer Service and Datastream to migrate data from third‐party sources and databases to Google Cloud. In the process phase, we covered how we can transform the data using services such as Cloud Dataflow, Cloud Data Fusion, Cloud Dataproc, Cloud Composer, and Cloud Dataprep.\n\nThen we talked about how you can train your model using Vertex AI training. Vertex AI training supports frameworks such as scikit‐learn, TensorFlow, PyTorch, and XGBoost. We talked about how you can train a model using prebuilt containers and custom containers.\n\nWe also covered why and how you can unit test the data and model for machine learning.\n\nThen we covered hyperparameter tuning and various search algorithms for hyperparameter tuning available in Google Cloud, as well as Vertex AI Vizier and how it's different than hyperparameter tuning.\n\nYou learned how you can track and debug your training model in Vertex AI metrics using Vertex AI interactive shell, TensorFlow Profiler, and the What‐If Tool.\n\nYou also learned about data drift and concept drift and when you should retrain your model to avoid drift.",
        "subsections": {},
        "summary": "* **File Ingestion**: File ingestion stages include collection, processing, storage, and analysis\n    * Services used for each stage:\n        * _Pub/Sub_ for real-time data collection\n        * BigQuery Data Transfer Service and Datastream for migrating third-party sources and databases\n    * Data transformation using services like Cloud Dataflow, Cloud Data Fusion, etc.\n\n* **Model Training**: Training with Vertex AI supports popular frameworks such as scikit-learn, TensorFlow, PyTorch, XGBoost\n    * _Prebuilt containers_ and custom containers can be used for training\n    * Hyperparameter tuning using algorithms like Grid Search and Random Search"
      },
      "Exam Essentials": {
        "content": "* **Know how to ingest various file types into training.** Understand the various file types, such as structured (for example, CSV), unstructured (for example, text files), and semi‐structured (for example, JSON files). Know how these file types can be stored and ingested for AI/ML workloads in GCP. Understand how the file ingestion into Google Cloud works by using a Google Cloud data analytics platform into stages such as collect, process, store, and analyze. For collecting data into Google Cloud Storage, you can use Pub/Sub and Pub/Sub Lite to collect real‐time data as well as BigQuery Data Transfer Service and Datastream to migrate data from third‐party sources and databases to Google Cloud. In the process phase, understand how we can transform the data or run Spark/Hadoop jobs for ETL using services such as Cloud Dataflow, Cloud Data Fusion, Cloud Dataproc, Cloud Composer, and Cloud Dataprep. know how to use Vertex AI Workbench environment by using common frameworks: understand the feature differences and framework supported by both managed and user managed notebooks. Understand when you should use user managed notebook vs managed notebook. Understand how to create these notebooks and what features they support out of the box.\n  * **Know how to use the Vertex AI Workbench environment by using common frameworks.** Understand the feature differences and framework supported by both managed and user‐managed notebooks. Understand when you should use user‐managed notebooks versus managed notebooks. Understand how to create these notebooks and what features they support out of the box.\n  * **Know how to train a model as a job in different environments.** Understand options for Vertex AI training such as AutoML and custom training. Then understand how you can perform custom training by using either a prebuilt container or a custom container using Vertex AI training along with architecture. Understand using a training pipeline versus custom jobs to set up training in Vertex AI. Vertex AI training supports frameworks such as scikit‐learn, TensorFlow, PyTorch, and XGBoost. Also, understand how to set up distributed training using Vertex AI custom jobs.\n  * **Be able to unit test for model training and serving.** Understand why and how you can unit test the data and model for machine learning. Understand how to test for updates in APIs after model endpoints are updated and how to test for algorithm correctness.\n  * **Understand hyperparameter tuning.** Understand hyperparameter tuning and various search algorithms for hyperparameter tuning such as grid search, random search, and Bayesian search. Understand when to use which search algorithm to speed up performance. Know how to set up hyperparameter tuning using custom jobs. Last, also understand Vertex AI Vizier and how it's different from setting up hyperparameter tuning.\n  * **Track metrics during training.** You can use Interactive shell, Tensorflow Profiler and What‐If tool to track metrics during model training.\n  * **Conduct a retraining/redeployment evaluation.** Understand bias variance trade‐off while training a neural network. Then you need to understand strategies to handle underfitting and strategies to handle overfitting, such as regularization. Know the difference between L1 and L2 regularization and when to apply which approach.",
        "subsections": {},
        "summary": "* **File Ingestion**: Understand how to ingest various file types into training, including structured, unstructured, and semi-structured files, using services like Pub/Sub, BigQuery Data Transfer Service, and Cloud Datastream.\n  * Know how to collect real-time data with Pub/Sub Lite\n  * Use Cloud Dataflow, Cloud Data Fusion, and other services for ETL transformations\n* **Vertex AI Workbench**: Understand the feature differences between managed and user-managed notebooks, and when to use each.\n  * Create notebooks using Vertex AI Workbench and understand their features\n* **Model Training and Serving**:\n  * Know how to unit test model training and serving data\n  * Track metrics during training with Interactive shell, Tensorflow Profiler, and What-If tool\n* **Hyperparameter Tuning**: Understand the different search algorithms (grid search, random search, Bayesian search) and when to use each.\n  * Use custom jobs for hyperparameter tuning\n  * Understand Vertex AI Vizier and its differences from setting up hyperparameter tuning"
      },
      "Review Questions": {
        "content": "1. You are a data scientist for a financial firm who is developing a model to classify customer support emails. You created models with TensorFlow Estimators using small datasets on your on‐premises system, but you now need to train the models using large datasets to ensure high performance. You will port your models to Google Cloud and want to minimize code refactoring and infrastructure overhead for easier migration from on‐prem to cloud. What should you do?\n     1. Use Vertex AI custom jobs for training.\n     2. Create a cluster on Dataproc for training.\n     3. Create an AutoML model using Vertex AI training.\n     4. Create an instance group with autoscaling.\n  2. You are a data engineer building a demand‐forecasting pipeline in production that uses Dataflow to preprocess raw data prior to model training and prediction. During preprocessing, you perform z‐score normalization on data stored in BigQuery and write it back to BigQuery. Because new training data is added every week, what should you do to make the process more efficient by minimizing computation time and manual intervention?\n     1. Translate the normalization algorithm into SQL for use with BigQuery.\n     2. Normalize the data with Apache Spark using the Dataproc connector for BigQuery.\n     3. Normalize the data with TensorFlow data transform.\n     4. Normalize the data by running jobs in Google Kubernetes Engine clusters.\n  3. You are an ML engineer for a fashion apparel company designing a customized deep neural network in Keras that predicts customer purchases based on their purchase history. You want to explore model performance using multiple model architectures, to store training data, and to compare the evaluation metric while the job is running. What should you do?\n     1. Create multiple models using AutoML Tables.\n     2. Create an experiment in Kubeflow Pipelines to organize multiple runs.\n     3. Run multiple training jobs on the Vertex AI platform with an interactive shell enabled.\n     4. Run multiple training jobs on the Vertex AI platform with hyperparameter tuning.\n  4. You are a data scientist who has created an ML pipeline with hyperparameter tuning jobs using Vertex AI custom jobs. One of your tuning jobs is taking longer than expected and delaying the downstream processes. You want to speed up the tuning job without significantly compromising its effectiveness. Which actions should you take? (Choose three.)\n     1. Decrease the number of parallel trials.\n     2. Change the search algorithm from grid search to random search.\n     3. Decrease the range of floating‐point values.\n     4. Change the algorithm to grid search.\n     5. Set the early stopping parameter to TRUE.\n  5. You are a data engineer using PySpark data pipelines to conduct data transformations at scale on Google Cloud. However, your pipelines are taking over 12 hours to run. In order to expedite pipeline runtime, you do not want to manage servers and need a tool that can run SQL. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting speed and processing requirements?\n     1. Use Data Fusion's GUI to build the transformation pipelines, and then write the data into BigQuery.\n     2. Convert your PySpark commands into Spark SQL queries to transform the data and then run your pipeline on Dataproc to write the data into BigQuery using BigQuery Spark connector.\n     3. Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.\n     4. Ingest your data into Cloud SQL, convert your PySpark commands into Spark SQL queries to transform the data, and then use SQL queries from BigQuery for machine learning.\n  6. You are a lead data scientist manager who is managing a team of data scientists using a cloud‐based system to submit training jobs. This system has become very difficult to administer. The data scientists you work with use many different frameworks such as Keras, PyTorch, Scikit, and custom libraries. What is the most managed way to run the jobs in Google Cloud?\n     1. Use the Vertex AI training custom containers to run training jobs using any framework.\n     2. Use the Vertex AI training prebuilt containers to run training jobs using any framework.\n     3. Configure Kubeflow to run on Google Kubernetes Engine and receive training jobs through TFJob.\n     4. Create containerized images on Compute Engine using GKE and push these images on a centralized repository.\n  7. You are training a TensorFlow model on a structured dataset with 500 billion records stored in several CSV files. You need to improve the input/output execution performance. What should you do?\n     1. Load the data into HDFS.\n     2. Load the data into Cloud Bigtable, and read the data from Bigtable using a TF Bigtable connector.\n     3. Convert the CSV files into shards of TFRecords, and store the data in Cloud Storage.\n     4. Load the data into BigQuery using Dataflow jobs.\n  8. You are the senior solution architect of a gaming company. You have to design a streaming pipeline for ingesting player interaction data for a mobile game. You want to perform ML on the streaming data. What should you do to build a pipeline with the least overhead?\n     1. Use Pub/Sub with Cloud Dataflow streaming pipeline to ingest data.\n     2. Use Apache Kafka with Cloud Dataflow streaming pipeline to ingest data.\n     3. Use Apache Kafka with Cloud Dataproc to ingest data.\n     4. Use Pub/Sub Lite streaming connector with Cloud Data Fusion.\n  9. You are a data scientist working on a smart city project to build an ML model to detect anomalies in real‐time sensor data. You will use Pub/Sub to handle incoming requests. You want to store the results for analytics and visualization. How should you configure the below pipeline:\n\nIngest data using Pub/Sub‐> 1\\. Preprocess ‐> 2\\. ML training\n\n‐> 3\\. Storage ‐> Visualization in Data Studio\n\n     1. 1\\. Dataflow, 2. Vertex AI Training, 3. BigQuery\n     2. 1\\. Dataflow, 2. Vertex AI AutoML, 3. Bigtable\n     3. 1\\. BigQuery, 2. Vertex AI Platform, 3. Cloud Storage\n     4. 1\\. Dataflow, 2. Vertex AI AutoML, 3. Cloud Storage\n  10. You are a data scientist who works for a Fintech company. You want to understand how effective your company's latest advertising campaign for a financial product is. You have streamed 900 MB of campaign data into BigQuery. You want to query the table and then manipulate the results of that query with a pandas DataFrame in a Vertex AI platform notebook. What will be the least number of steps needed to do this?\n     1. Download your table from BigQuery as a local CSV file, and upload it to your AI platform notebook instance. Use pandas `read_csv` to ingest the file as a pandas DataFrame.\n     2. Export your table as a CSV file from BigQuery to Google Drive, and use the Google Drive API to ingest the file into your notebook instance.\n     3. Use the Vertex AI platform notebook's BigQuery cell magic to query the data, and ingest the results as a pandas DataFrame using pandas BigQuery client.\n     4. Use the `bq extract` command to export the table as a CSV file to Cloud Storage, and then use `gsui cp` to copy the data into the notebook Use pandas `read_csv` to ingest the file.\n  11. You are a data scientist working on a fraud detection model. You will use Pub/Sub to handle incoming requests. You want to store the results for analytics and visualization. How should you configure the following pipeline: 1. Ingest data ‐> 2\\. Preprocess ‐> 3\\. ML training and visualize in Data/Looker Studio\n     1. 1\\. Dataflow, 2. Vertex AI Training, 3. BigQuery\n     2. 1\\. Pub/Sub, 2. Dataflow, 3. BigQuery ML\n     3. 1\\. Pub/Sub, 2. Dataflow, 3. Vertex AI Training\n     4. 1\\. Dataflow, 2. Vertex AI AutoML, 3. Cloud Storage\n  12. You are an ML engineer working for a public health team to create a pipeline to classify support tickets on Google Cloud. You analyzed the requirements and decided to use TensorFlow to build the classifier so that you have full control of the model's code, serving, and deployment. You will use Kubeflow Pipelines for the ML platform. To save time, you want to build on existing resources and use managed services instead of building a completely new model. How should you build the classifier?\n     1. Use an established text classification model and train using Vertex AI Training as is to classify support requests.\n     2. Use an established text classification model and train using Vertex AI Training to perform transfer learning.\n     3. Use AutoML Natural Language to build the support requests classifier.\n     4. Use the Natural Language API to classify support requests.\n  13. You are training a TensorFlow model for binary classification with a lot of categorical features using Vertex AI custom jobs. You are looking for UI tools to track metrics of your model such as CPU utilization and network I/O and features used while training. Which tools will you pick? (Choose two.)\n     1. Interactive shell\n     2. TensorFlow Profiler\n     3. Jupyter Notebooks\n     4. Looker Studio\n     5. Looker\n  14. You are training a TensorFlow model to identify semi‐finished products using Vertex AI custom jobs. You want to monitor the performance of the model. Which of the following can you use?\n     1. TensorFlow Profiler\n     2. TensorFlow Debugger\n     3. TensorFlow Trace\n     4. TensorFlow Checkpoint\n  15. You are a data scientist working for a start‐up on several projects with TensorFlow. Your data is in Parquet format and you need to manage input and output. You are looking for the most cost‐effective solution to manage the input while training TensorFlow models on Google Cloud. Which of the following should you use?\n     1. TensorFlow I/O\n     2. Cloud Dataproc\n     3. Cloud Dataflow\n     4. BigQuery to TFRecords\n  16. You are training a TensorFlow model for binary classification with many categorical features using Vertex AI custom jobs. Your manager has asked you about the classification metric and also to explain the inference. You would like to show them an interactive demo with visual graphs. Which tool should you use?\n     1. TensorBoard\n     2. What‐If Tool\n     3. Looker\n     4. Language Interpretability Tool (LIT)",
        "subsections": {},
        "summary": "* **Porting Models to Google Cloud**: Use Vertex AI custom jobs for training.\n    * To minimize code refactoring and infrastructure overhead, use an existing model as a starting point and perform transfer learning using AutoML or Vertex AI Training.\n    * Alternatively, create an instance group with autoscaling to support large datasets.\n\n* **Improving Data Loading Performance**: Load data into Cloud Storage as TFRecords.\n    * Convert CSV files into shards of TFRecords for efficient input/output execution performance.\n    * Use the `bq extract` command to export the table as a CSV file, and then use `gsui cp` to copy the data into Cloud Storage.\n\n* **Configuring Streaming Pipeline**: Use Pub/Sub with Cloud Dataflow streaming pipeline to ingest data.\n    * To perform ML on streaming data, configure the pipeline to preprocess and train models in real-time using Vertex AI Training or AutoML Natural Language.\n\n* **Querying BigQuery with pandas DataFrame**: Use the Vertex AI platform notebook's BigQuery cell magic to query the data and ingest the results as a pandas DataFrame.\n    * Alternatively, export the table as a CSV file from BigQuery to Cloud Storage and use `gsui cp` to copy the data into the notebook instance.\n\n* **Tracking Model Metrics**: Use Interactive shell or TensorFlow Profiler for UI tools to track metrics of your model such as CPU utilization and network I/O.\n    * Additionally, consider using Looker Studio or Looker for visualization and explanation of classification metric and inference results.\n\n* **Monitoring Performance**: Use TensorFlow Profiler or TensorFlow Debugger for monitoring the performance of your model during training.\n    * Alternatively, use TensorFlow Checkpoint to track model checkpoints and TensorFlow Trace to visualize the model's execution graph."
      }
    },
    "summary": ""
  },
  "Chapter 9Model Explainability on Vertex AI": {
    "content": "",
    "subsections": {
      "Model Explainability on Vertex AI": {
        "content": "For a team developing ML models, the responsibility to explain model predictions increases as the impact of predictions on business outcomes increases. For example, consumers are likely to accept a movie recommendation from an ML model without needing an explanation. The consumer may or may not agree with the recommendation, but the need to justify the prediction is relatively low on the model developers.\n\nOn the contrary, if an ML model predicts whether a credit loan application is approved or a patient's drug dosage is correct, the model developers are responsible for explaining the prediction. They need to address questions such as “Why was my loan rejected?” or “Why should I take 10 mg of this drug?” For this reason, gaining visibility into the training process and developing human‐explainable ML models is important.",
        "subsections": {
          "Explainable AI": {
            "content": "_Explainability_ is the extent to which you can explain the internal mechanics of an ML or deep learning system in human terms. It is in contrast to the concept of the black box, in which even designers cannot explain why an AI arrives at a specific decision.\n\nThere are two types of explainability, global and local:\n\n  * _Global explainability_ aims at making the overall ML model transparent and comprehensive.\n  * _Local explainability_ focuses on explaining the model's individual predictions.\n\nThe ability to explain an ML model and its predictions builds trust and improves ML adoption. The model is no longer a black box. This increases the comfort level of the consumers of model predictions. For model owners, the ability to understand the uncertainty inherent in ML models helps with debugging the model when things go wrong and improving the model for better business outcomes.\n\nDebugging machine learning models is complex because of deep neural nets. As the number of variables increases, it becomes really hard to see what feature contributed to which outcome. Linear models are easily explained and interpreted since the input parameters have a linear relationship with the output: (X = ax + y), where X is the predicted output depending on x and y (input parameters). With models based on decision trees such as XGBoost and deep neural nets, this mathematical relationship to determine the output from a set of inputs gets complex, leading to difficulty in debugging these models. That is why explainable techniques are needed to explain the model.",
            "subsections": {},
            "summary": "* **Explainability** measures the ability to understand internal mechanics of an ML or DL system.\n    * _Global_ explainability aims for overall model transparency and comprehensiveness.\n        - Increasing trust and adoption through transparent predictions\n        - Improving business outcomes by understanding uncertainty\n    * _Local_ explainability focuses on individual predictions, helping with debugging and improvement."
          },
          "Interpretability and Explainability": {
            "content": "_Interpretability_ and _explainability_ are often used interchangeably. However, there is a slight difference in what they mean. Interpretability has to do with how accurately a machine learning model can associate a cause to an effect. Explainability has to do with explaining the ability of the parameters hidden in deep neural nets (which we covered in Chapter 7, “Model Building”) to justify the results.",
            "subsections": {},
            "summary": "* **Key Difference**: \n  * _Interpretability_ is about associating causes and effects\n  * _Explainability_ is about justifying model results through its parameters"
          },
          "Feature Importance": {
            "content": "Feature importance is a technique that explains the features that make up the training data using a score (importance). It indicates how useful or valuable the feature is relative to other features. In the use case of individual income prediction using XGBoost, the importance score indicates the value of each feature in the construction of the boosted decision trees within the model. The more a model uses an attribute to make key decisions with decision trees, the higher the attribute's relative importance.\n\nThe following are the most important benefits of using feature importance:\n\n  * **Variable selection** : Suppose you are training with 1,000 variables. You can easily figure out which variables are not important or contributing less to your model prediction and easily remove those variables before deploying the model in production. This can save a lot of compute and infrastructure costs and training time.\n  * **Target/label or data leakage in your model** : Data leakage occurs when by mistake you have added your target variable (the feature you are trying to predict) in your training dataset as a feature. We covered this in Chapter 2, “Exploring Data and Building Data Pipelines.”",
            "subsections": {},
            "summary": "* **Feature Importance**: Technique that assigns scores to features based on their contribution to model predictions\n    * Helps reduce computational costs and infrastructure requirements by identifying non-contributing variables\n    * Detects data leakage when target variable is included as a feature in training dataset"
          },
          "Vertex Explainable AI": {
            "content": "Vertex Explainable AI integrates feature attributions into Vertex AI and helps you understand your model's outputs for classification and regression tasks. Vertex AI tells you how much each feature in the data contributed to the predicted result. You can then use this information to verify that the model is behaving as expected, recognize bias in your model, and get ideas for ways to improve your model and your training data. These are supported services for Vertex Explainable AI:\n\n  * AutoML image models (classification models only)\n  * AutoML tabular models (classification and regression models only)\n  * Custom‐trained TensorFlow models based on tabular data\n  * Custom‐trained TensorFlow models based on image data",
            "subsections": {
              "Feature Attribution": {
                "content": "Google Cloud's current offering in Vertex AI is centered around instance‐level feature attributions, which provide a signed per‐feature attribution score proportional to the feature's contribution to the model's prediction. Feature attributions indicate how much each feature in your model contributed to the predictions for each given instance. When you request predictions, you get the predicted values as appropriate for your model. When you request explanations, you get the predictions along with feature attribution information.\n\nFor more information, refer to `https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#feature-based_explanations`.\n\n* * *\n\nFeature attribution functionality to get explainability is integrated into the Google Cloud console for AutoML Tables and AutoML Images. You can set feature attribution while training custom TensorFlow models using the Vertex AI Explainable SDK with Vertex AI Prediction.\n\n* * *\n\nThe Vertex Explainable AI offers three methods to use for feature attributions: sampled Shapley, integrated gradients, and XRAI.\n\n  * **Sampled Shapley:** This assigns credit for the outcome to each feature and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values. Figure 9.1 shows how you can use Shapley plots to determine the black box feature importance (`https://github.com/slundberg/shap`). You have four input variables in the model that is giving you an output. Using Shapley values, you can know which of these four features (age, sex, BP, BMI) contributed positively or negatively to the output or prediction.\n\n**FIGURE 9.1** SHAP model explainability\n\n  * **Integrated gradients:** There is a gradients‐based method to efficiently compute feature attributions. This is mostly used in deep neural networks with image use cases. The gradient is calculated, which informs which pixel has the strongest effect on the model's predicted class probabilities. For example, an image classification model is trained to predict whether a given image contains a dog or a cat. If you request predictions from this model on a new set of images, then you receive a prediction for each image (“dog” or “cat”). If you request explanations, you get the predicted class along with an overlay for the image, showing which pixels in the image contributed most strongly to the resulting prediction, as shown in Figure 9.2. For more information, see `https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#integrated-gradients`.\n\n**FIGURE 9.2** Feature attribution using integrated gradients for cat image\n\nSource: Google LLC / `www.tensorflow.org/tutorials/interpretability/integrated_gradients` last accessed November 21, 2022.\n\nIntegrated gradients provide feature importance on individual examples; however, they do not provide global feature importance across an entire dataset. Also they do not explain feature interactions and combinations.\n\n  * **XRAI (eXplanation with Ranked Area Integrals):** Based on the integrated gradients method, XRAI assesses overlapping regions of the image to create a saliency map, which highlights relevant regions of the image rather than pixels. The XRAI method combines the integrated gradients method with additional steps to determine which _regions_ of the image contribute the most to a given class prediction. XRAI does pixel‐level segmentation, oversegmentation, and region selection to provide explanations in images. For more information, see `https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#xrai`.\n\nYou do not need to understand the details of these methods for the exam. However, you would need to know when to use which technique for the use cases and data types in Vertex AI; see Table 9.1.\n\n**TABLE 9.1** Explainable techniques used by Vertex AI\n\nMethod | Supported Data Types | Model Types | Use Case | Vertex AI–Equivalent Model\n---|---|---|---|---\nSampled Shapley | Tabular | Nondifferentiable models (explained after the table), such as ensembles of trees and neural networks. | Classification and regression on tabular data | Custom‐trained models (any prediction container)\nAutoML tabular models\nIntegrated gradients | Image and tabular data | Differentiable models (explained after the table), such as neural networks. Recommended especially for models with large feature spaces. Recommended for low‐contrast images, such as X‐rays. | Classification and regression on tabular data\nClassification on image data | Custom‐trained TensorFlow models that use a TensorFlow prebuilt container to serve predictions\nAutoML image models\nXRAI (eXplanation with Ranked Area Integrals) | Image data | Models that accept image inputs. Recommended especially for natural images, which are any real‐world scenes that contain multiple objects. | Classification on image data | Custom‐trained TensorFlow models that use a TensorFlow pre‐built container to serve predictions\nAutoML image models\n\nWe mention differentiable models and nondifferentiable models in Table 9.1. Let's talk about the basic difference between them:\n\n  * **Differentiable models:** You can calculate the derivative of all the operations in your TensorFlow graph. This property helps to make backpropagation possible in such models. For example, neural networks are differentiable. To get feature attributions for differentiable models, use the integrated gradients method.\n  * **Nondifferentiable models:** These include nondifferentiable operations in the TensorFlow graph, such as operations that perform decoding and rounding tasks. For example, a model built as an ensemble of trees and neural networks is nondifferentiable. To get feature attributions for nondifferentiable models, use the sampled Shapley method.\n\n* * *\n\nThe integrated gradients method does not work for nondifferentiable models. Sampled Shapley works on differentiable and nondifferentiable models, but it's more compute intensive.\n\n* * *\n\nFor an in‐depth explanation, refer to the _AI Explanations Whitepaper_ at `https://cloud.google.com/blog/products/ai-machine-learning/example-based-explanations-to-build-better-aiml-models`.",
                "subsections": {},
                "summary": "**Vertex AI Feature Attribution**\n\n* **Method 1:** *Sampled Shapley* - assigns credit to each feature and considers different permutations, providing a sampling approximation of exact Shapley values.\n* **Method 2:** *Integrated gradients* - calculates the gradient, informing which pixel has the strongest effect on the model's predicted class probabilities. Used for deep neural networks with image use cases.\n* **Method 3:** *XRAI (eXplanation with Ranked Area Integrals)* - assesses overlapping regions of the image to create a saliency map, highlighting relevant regions and providing explanations in images.\n\n**Supported Data Types:**\n\n| Method | Supported Data Types |\n| --- | --- |\n| Sampled Shapley | Tabular, nondifferentiable models (e.g., ensembles of trees) |\n| Integrated gradients | Image, tabular data |\n| XRAI | Image data |\n\n**Model Types and Use Cases:**"
              },
              "Vertex AI Example–Based Explanations": {
                "content": "Example‐based explanations are used for misclassification analysis and can enable active learning so that data can be selectively labeled. For instance, if out of 10 total explanations for an image, 5 are from class “bird” and five are from class “plane,” the image can be a candidate for human annotation, further enriching the data.\n\nExample‐based explanations are not limited to images. They can generate embeddings for multiple types of data such as images, text, and table. Refer to this blog to learn more about this: `https://cloud.google.com/blog/products/ai-machine-learning/example-based-explanations-to-build-better-aiml-models`.\n\nThis feature is in public preview, and you might not get questions from this topic on the exam. However, it's a good topic to understand for the Explainable AI options provided by Google Cloud.",
                "subsections": {},
                "summary": "* **Example-Based Explanations**: enable selective data labeling and are not limited to images\n* *Can generate embeddings for images, text, and tables*\n* _Currently in public preview, but useful to understand for Google Cloud's Explainable AI options_"
              }
            },
            "summary": "* **Vertex Explainable AI** explains model outputs for classification and regression tasks, providing feature attribution.\n* _Supported services include_:\n  * AutoML image models\n  * AutoML tabular models\n  * Custom-trained TensorFlow models"
          },
          "Data Bias and Fairness": {
            "content": "Data can be biased when certain parts of the data are not collected or are misrepresented. This might happen using data collected through surveys, data that is based on systemic or historical beliefs, or a data sample that is not random or is too small. Any of these can lead to skewed outcomes, as biased data does not accurately represent the machine learning model use case. It also presents another problem of system prejudice or fairness in the data.\n\nML fairness ensures that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as race, gender, disabilities, or sexual or political orientation—for example, granting a credit card based on gender or denying an application based on race.\n\nThe following list includes some of the ways you can detect bias and fairness in data in Vertex AI:\n\n  * The Explainable AI feature attributions technique, which is already present in AutoML tables, helps detect bias and fairness in the tabular or structured dataset.\n  * Use Vertex AI to inspect models through an interactive dashboard with the integrated `What‐If Tool`. (We covered the What‐If Tool in Chapter 8, “Model Training and Hyperparameter Tuning.”) This tool can be used to detect bias in the dataset using the features overview functionality, which automatically detects bias from the data. You can refer to this link for more details: `https://pair-code.github.io/what-if-tool/learn/tutorials/features-overview-bias`.\n  * Alternatively, for detecting bias and fairness in NLP (natural language processing) models, you can utilize the open source `Language Interpretability Tool`.\n\nBy using these tools and techniques, you can detect bias and fairness in your dataset before training models on them during the data exploration or data preprocessing phase, which we covered in Chapter 2, “Exploring Data and Building Data Pipelines,” and Chapter 3, “Feature Engineering.”",
            "subsections": {},
            "summary": "* **Bias in Data**: Data can be biased due to incomplete or misrepresented information, leading to skewed outcomes and unfair model treatment.\n* **ML Fairness**: Ensures models do not discriminate against individuals based on characteristics like race, gender, disabilities, or orientation.\n* \n    * _**Detection Methods**_: Utilize Explainable AI's feature attributions technique for tabular data, the `What‐If Tool` interactive dashboard, and the open source `Language Interpretability Tool` to detect bias and fairness in datasets."
          },
          "ML Solution Readiness": {
            "content": "We will talk about ML solution readiness in terms of these two concepts:\n\n  * Responsible AI\n  * Model governance\n\nGoogle believes in Responsible AI principles, which we covered in Chapter 1, “Framing ML Problems.” Google shares best practices with customers through Google `Responsible AI practices`, `fairness best practices`, `technical references`, and `technical ethics` materials.\n\nResponsible AI tools are an increasingly effective way to inspect and understand AI models. Some of the tools are listed here:\n\n  * `Explainable AI`, which we covered earlier in this chapter—how you can have Explainable AI in your models using Vertex AI offerings.\n  * `Model cards`, which explain what a model does, its intended audience, and who maintains it. A model card also provides insight into the construction of the model, including its architecture and the training data used. Refer to this link to learn more about model cards: `https://modelcards.withgoogle.com/about`.\n  * `TensorFlow open source toolkit` to provide model transparency in a structured, accessible way.\n\nModel governance is a core function in companies that provides guidelines and processes to help employees implement the company's AI principles. These principles can include avoiding models that create or enforce bias and being able to justify AI‐made decisions.\n\nSome of the ways you can achieve model governance are as follows:\n\n  * Make sure there is a human in the loop to review model output or prediction for sensitive and high‐impact workloads.\n  * Have a `responsibility assignment matrix` for each model by task.\n  * Maintain model cards to track model versioning and data lineage.\n  * Evaluate the model on benchmark datasets that cover both standard cases and edge cases and validate the model against fairness indicators to help detect implicit bias.\n  * Use `what‐if analysis tools` to understand the importance of different data features.\n  * All the model readiness best practices from data cleaning, training, and tuning to deploying the model are mentioned in this Google Cloud documentation: `https://cloud.google.com/architecture/guidelines-for-developing-high-quality-ml-solutions`.",
            "subsections": {},
            "summary": "* **Responsible AI**: refers to principles guiding AI development that prioritize fairness, transparency, and accountability\n  * _Google's Responsible AI practices_ provide customers with materials on best practices for fair AI development.\n  * Tools like `_Explainable AI` and `_Model cards` help inspect and understand AI models.\n\n* **Model Governance**:\n  * Provides guidelines and processes to implement company AI principles.\n  * Ensures models are transparent, fair, and bias-free."
          },
          "How to Set Up Explanations in the Vertex AI": {
            "content": "If you are using a custom‐trained model (TensorFlow, Scikit, or XGBoost), which we covered in Chapter 8, you need to configure explanations for custom‐trained models (`https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based`) to create a model that supports Vertex Explainable AI. For an AutoML tabular classification or regression, you do not need any specific configuration to use Vertex Explainable AI (`https://cloud.google.com/vertex-ai/docs/explainable-ai/getting-explanations`).\n\nAfter having a model resource with Explainable AI created, you can perform the following explanations:\n\n  * **Online explanations** : Synchronous requests to the Vertex AI API, similar to online predictions that return predictions with feature attributions. For online explanations, instead of sending a `projects.locations.endpoints.predict` request to the Vertex AI API, send a `projects.locations.endpoints.explain` request.\n  * **Batch explanations:** Asynchronous requests to the Vertex AI API that return predictions with feature attributions. Batch explanations are an optional part of batch prediction requests. To get batch explanations, set the generateExplanation field to true when you create a batch prediction job.\n  * **Local kernel explanations:** Perform these in the User‐Managed Vertex AI Workbench notebook we covered in Chapter 6, “Building Secure ML Pipelines.” You can generate explanations for your custom‐trained model by running the Vertex Explainable AI within your notebook's local kernel or runtime without deploying the model to Vertex AI to get explanations. Using local explanations allows you to try different Vertex Explainable AI settings without adjusting your Vertex AI model deployment for each change.\n\n* * *\n\nThe Explainable AI SDK is preinstalled in user‐managed notebook instances. Within your notebook, you can use the Explainable AI SDK to save your model artifact and automatically identify metadata about your model's inputs and outputs for the explanation request.\n\n* * *\n\nIf you're using TensorFlow, you can use the Explainable AI SDK's `save_model_with_metadata()` method to infer your model's inputs and outputs and save this explanation metadata with your model. Next, load the model into the Explainable AI SDK using `load_model_from_local_path()`. Finally, call `explain()` with instances of data, and visualize the feature attributions.",
            "subsections": {},
            "summary": "* **Configuring Vertex Explainable AI for Custom-Trained Models**: Configure explanations for custom-trained models to create a model that supports Vertex Explainable AI.\n    * _Use the Vertex AI API to configure explainable AI for custom-trained models._\n    * *_Enable batch explanations by setting generateExplanation field to true._\n\n* **Explaining Models using Vertex Explainable AI**\n    * *_Send synchronous requests (online explanations) and asynchronous requests (batch explanations)._ _\n    * *_Use local kernel explanations in User-Managed Vertex AI Workbench notebooks without deploying the model to Vertex AI._\n    \n* **Integrating Explainable AI SDK into Notebooks**: The Explainable AI SDK is preinstalled in user-managed notebook instances. Use it to save model artifacts and identify metadata for explanation requests."
          }
        },
        "summary": "* **Model Explainability**: As model impact on business outcomes increases, so does the responsibility to explain predictions\n* **Explainability Scope**: High-impact predictions (e.g., loan approvals, drug dosages) require detailed explanations for stakeholders\n* *_Human-Explainable ML_* models are crucial for gaining trust and addressing stakeholder questions"
      },
      "Summary": {
        "content": "In this chapter, we discussed what explainable AI is and the difference between explainability and interpretability. Then we covered the term _feature importance_ and why it's important to explain the models. We covered data bias and fairness as well as ML solution readiness.\n\nLast, we covered the explainable AI technique on the Vertex AI platform and feature attribution. We covered three primary techniques for model feature attribution used on the Vertex AI platform: sampled Shapley, XRAI, and integrated gradients.",
        "subsections": {},
        "summary": "* *_Explainable AI_*: Explainability vs. Interpretability, feature importance, data bias, fairness, ML solution readiness\n    * Key Techniques:\n        * Sampled Shapley\n        * XRAI\n        * Integrated Gradients\n    * Focus on Vertex AI platform for explainable AI"
      },
      "Exam Essentials": {
        "content": "* **Understand model explainability on Vertex AI.** Know what explainability is and the difference between global and local explanations. Why is it important to explain models? What is feature importance? Understand the options of feature attribution on the Vertex AI platform such as Sampled Shapley algorithm, integrated gradients, and XRAI. We covered data bias and fairness and how feature attributions can help with determining bias and fairness from the data. ML Solution readiness talks about Responsible AI and ML model governance best practices. Understand that explainable AI in Vertex AI is supported for the TensorFlow prediction container using the Explainable AI SDK and for the Vertex AI AutoML tabular and AutoML image models.",
        "subsections": {},
        "summary": "* **Explainability**: Understanding why machine learning models make predictions, e.g., feature importance\n* **Key Features**:\n  * Sampled Shapley algorithm\n  * Integrated Gradients\n  * XRAI (eXplainable Reinforcement Algorithm for Image)\n* _Supporting Explainability_: \n    * TensorFlow prediction container with Explainable AI SDK\n    * Vertex AI AutoML tabular and image models"
      },
      "Review Questions": {
        "content": "1. You are a data scientist building a linear model with more than 100 input features, all with values between –1 and 1. You suspect that many features are non‐informative. You want to remove the non‐informative features from your model while keeping the informative ones in their original form. Which technique should you use?\n     1. Use principal component analysis to eliminate the least informative features.\n     2. When building your model, use Shapley values to determine which features are the most informative.\n     3. Use L1 regularization to reduce the coefficients of noninformative features to 0.\n     4. Use an iterative dropout technique to identify which features do not degrade the model when removed.\n  2. You are a data scientist at a startup and your team is working on a number of ML projects. Your team trained a TensorFlow deep neural network model for image recognition that works well and is about to be rolled out in production. You have been asked by leadership to demonstrate the inner workings of the model. What explainability technique would you use on Google Cloud?\n     1. Sampled Shapley\n     2. Integrated gradient\n     3. PCA\n     4. What‐If Tool analysis\n  3. You are a data scientist working with Vertex AI and want to leverage Explainable AI to understand which are the most essential features and how they impact model predictions. Select the model types and services supported by Vertex Explainable AI. (Choose three.)\n     1. AutoML Tables\n     2. Image classification\n     3. Custom DNN models\n     4. Decision trees\n     5. Linear learner\n  4. You are an ML engineer working with Vertex Explainable AI. You want to understand the most important features for training models that use image and tabular datasets. Which of the feature attribution techniques can you use? (Choose three.)\n     1. XRAI\n     2. Sampled Shapley\n     3. Minimum likelihood\n     4. Interpretability\n     5. Integrated gradients\n  5. You are a data scientist training a TensorFlow model with graph operations as operations that perform decoding and rounding tasks. Which technique would you use to debug or explain this model in Vertex AI?\n     1. Sampled Shapley\n     2. Integrated gradients\n     3. XRAI\n     4. PCA\n  6. You are a data scientist working on creating an image classification model on Vertex AI. You want these images to have feature attribution. Which of the attribution techniques is supported by Vertex AI AutoML images? (Choose two.)\n     1. Sampled Shapely\n     2. Integrated gradients\n     3. XRAI\n     4. DNN\n  7. You are a data scientist working on creating an image classification model on Vertex AI. You want to set up an explanation for testing your TensorFlow code in user‐managed notebooks. What is the suggested approach with the least effort?\n     1. Set up local explanations using Explainable AI SDK in the notebooks.\n     2. Configure explanations for the custom TensorFlow model.\n     3. Set up an AutoML classification model to get explanations.\n     4. Set the generateExplanation field to true when you create a batch prediction job.\n  8. You are a data scientist who works in the aviation industry. You have been given a task to create a model to identify planes. The images in the dataset are of poor quality. Your model is identifying birds as planes. Which approach would you use to help explain the predictions with this dataset?\n     1. Use Vertex AI example–based explanations.\n     2. Use the integrated gradients technique for explanations.\n     3. Use the Sampled Shapley technique for explanations.\n     4. Use the XRAI technique for explanations.",
        "subsections": {},
        "summary": "* **Removing non-informative features**: \n    * Use principal component analysis to eliminate the least informative features (*_PCA_*).\n    * Use L1 regularization to reduce the coefficients of noninformative features to 0.\n    * Use an iterative dropout technique to identify which features do not degrade the model when removed.\n\n* **Explainable AI on Google Cloud**: \n    * *_Integrated gradient_* for demonstrating the inner workings of a model.\n\n* **Vertex AI Explainable AI supported models and services**: \n    * Custom DNN models\n    * Decision trees\n    * Linear learner\n\n* **Feature attribution techniques in Vertex AI**: \n    * Sampled Shapley\n    * Integrated gradients\n    * XRAI\n\n* **Debugging or explaining a model with graph operations**: \n    *_Sampled Shapley_*.\n\n* **Supported feature attribution techniques by Vertex AI AutoML images**: \n    * *_Integrated gradients_*\n    * *_XRAI_*\n\n\n* **Suggested approach for setting up an explanation in user-managed notebooks**: \n    *_Set the generateExplanation field to true when you create a batch prediction job._*\n\n* **Approach for explaining predictions with poor quality images**: \n    *_Use Vertex AI example–based explanations_.*"
      }
    },
    "summary": ""
  },
  "Chapter 10Scaling Models in Production": {
    "content": "",
    "subsections": {
      "Scaling Prediction Service": {
        "content": "For the exam, you need to understand how you would deploy a model trained using TensorFlow after training. Figure 10.1 recaps the training option in TensorFlow and various distribution strategies we covered using CPUs, GPUs, and TPUs with TensorFlow. After training, you get a saved model. A saved model contains a complete TensorFlow program, including trained parameters (i.e., `tf.Variables`) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TensorFlow Lite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub.\n\n**FIGURE 10.1** TF model serving options\n\nA saved model is what you get when you call `tf.saved_model.save()`. Saved models are stored as a directory on disk. The file, `saved_model.pb`, within that directory is a protocol buffer describing the function `tf.Graph`.",
        "subsections": {
          "TensorFlow Serving": {
            "content": "TensorFlow (TF) Serving allows you to host a trained TensorFlow model as an API endpoint through a model server. TensorFlow Serving handles the model serving and version management and lets you serve models. It allows you to load your models from different sources.\n\nTensorFlow Serving allows two types of API endpoints : REST and gRPC.\n\nThese are the steps to set up TF Serving:\n\n  1. Install TensorFlow Serving with Docker.\n  2. Train and save a model with TensorFlow.\n  3. Serve the saved model using TensorFlow Serving.\n\nSee `www.tensorflow.org/tfx/serving/api_rest` for more information.\n\n* * *\n\nYou can install TensorFlow Serving without Docker, but using Docker is recommended and is certainly the easiest way to proceed. To manage TensorFlow Serving, you can choose to use a managed TensorFlow prebuilt container on Vertex AI.\n\n* * *",
            "subsections": {
              "Serving a Saved Model with TensorFlow Serving": {
                "content": "The TensorFlow ModelServer running on host:port accepts the following REST API requests:\n[code]\n     POST http://host:port/<URI>:<VERB>\n\n     URI: /v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]\n     VERB: classify|regress|predict\n\n[/code]\n\nTo call the `predict()` REST endpoint, you need to define a JSON data payload because TF Serving expects data as JSON, as shown here:\n[code]\n     data = json.dumps({\"signature_name\": \"serving_default\",\n      \"instances\": instances.tolist()})\n        headers = {\"content-type\": \"application/json\"}\n        json_response = requests.post(url, data=data, headers=headers)\n        predictions = json.loads(json_response.text)['predictions']\n\n[/code]\n\nThe `signature_name` specifies the input/output data type. `Instances` is the data/input/instance you want to predict on. You should pass this as a list.\n\nTF Serving handles all the model and API infrastructure for you so that you can focus on model optimization. For the exam, you might be given an instance and signature of a TF Serving JSON load and have to provide the `predict()` function output.\n\nThe following is an example of a tensor instance:\n[code]\n     {\n      // List of 2 tensors each of [1, 2] shape\n      \"instances\": [ [[1, 2]], [[3, 4]] ]\n     }\n\n[/code]\n\nTo know what your `predict()` response format will be, you need to look at the SavedModel's SignatureDef (for more information, see `www.tensorflow.org/tfx/serving/signature_defs`). You can use the SignatureDef CLI to inspect a saved model. The given SavedModel SignatureDef contains the following output(s):\n[code]\n     outputs['class_ids'] tensor_info:\n     dtype: DT_INT64\n     shape: (-1, 1)\n     name: dnn/head/predictions/ExpandDims:0\n     outputs['classes'] tensor_info:\n     dtype: DT_STRING\n     shape: (-1, 1)\n     name: dnn/head/predictions/str_classes:0\n     outputs['logits'] tensor_info:\n     dtype: DT_FLOAT\n     shape: (-1, 3)\n     name: dnn/logits/BiasAdd:0\n     outputs['probabilities'] tensor_info:\n     dtype: DT_FLOAT\n     shape: (-1, 3)\n     name: dnn/head/predictions/probabilities:0\n     Method name is: tensorflow/serving/predict\n\n[/code]\n\nBased on the previous SignatureDef, the `predict()` request returns a JSON object in the response body, as shown in the following code:\n[code]\n     {\n     \"predictions\": [\n     {\n     \"class_ids\": [3],\n     \"probabilities\": [2.0495e-7, 0.0243, 0.9756],\n     \"classes\": [\"2\"],\n     \"logits\": [-5.74621, 5.94074, 9.62958]\n     }\n     ]\n     }\n\n[/code]\n\nThe previous example has a class ID integer with shape (−1,1) classes as strings with shape (−1,1), logits as float with shape (−1,3), and probability as float with shape (−1,3).\n\nSince both the shape of class ID and class is (−1,1), there will be only one value in the prediction for them. Similarly, for both probability and logit, the shape (−1,3) will give three tensor values in response. −1 means we can pass on any size to the saved model.\n\n* * *\n\nFor the exam, you need to understand what the tensor shape means in the SignatureDef of the saved model and what the predict response can be, based on the shape of tensors in the SignatureDef.\n\n* * *\n\nOnce the model is available in the model directory or folder, TF Serving automatically loads a new model. Moreover, when a new model version is available, TF Serving automatically unloads the old model and loads the newer version. This technique is very efficient and can be easily added into an MLOps pipeline. This helps you to focus more on model optimization instead of model serving infrastructure. We will cover the topic System design using TFX/Kubeflow in Chapter 11, “Designing ML Training Pipelines.”\n\nFor more information, see `https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker`.",
                "subsections": {},
                "summary": "* **REST API Requests**: \n    * The TensorFlow ModelServer accepts REST API requests using the following format: `POST http://host:port/<URI>:<VERB>`\n    * Accepted VERBS are classify, regress, and predict\n* **JSON Data Payload**: \n    * To call the `predict()` endpoint, a JSON data payload is required in the following format:\n        ```json\n{\n  \"signature_name\": \"serving_default\",\n  \"instances\": instances.tolist()\n}\n```\n    * The `signature_name` specifies the input/output data type and should be passed as a string. \n* **Tensor Instance**: \n    * A tensor instance is an example of what you would pass in the `instances` field:\n        ```json\n{\n  // List of 2 tensors each of [1, 2] shape\n  \"instances\": [ [[1, 2]], [[3, 4]] ]\n}\n```\n* **Predict Response**: \n    * The `predict()` response contains the following output(s):\n        ```\noutputs['class_ids'] tensor_info:\ndtype: DT_INT64\nshape: (-1, 1)\nname: dnn/head/predictions/ExpandDims:0\n\noutputs['classes'] tensor_info:\ndtype: DT_STRING\nshape: (-1, 1)\nname: dnn/head/predictions/str_classes:0\n\noutputs['logits'] tensor_info:\ndtype: DT_FLOAT\nshape: (-1, 3)\nname: dnn/logits/BiasAdd:0\n\noutputs['probabilities'] tensor_info:\ndtype: DT_FLOAT\nshape: (-1, 3)\nname: dnn/head/predictions/probabilities:0\n```\n    * The response is a JSON object containing the predictions in the following format:\n        ```json\n{\n  \"predictions\": [\n  {\n  \"class_ids\": [3],\n  \"probabilities\": [2.0495e-7, 0.0243, 0.9756],\n  \"classes\": [\"2\"],\n  \"logits\": [-5.74621, 5.94074, 9.62958]\n  }\n  ]\n}\n```"
              }
            },
            "summary": "### TensorFlow Serving Overview\n#### Key Features\n\n* Hosts trained TensorFlow models as API endpoints through model servers\n* Handles model serving and version management\n* Supports loading models from different sources\n\n#### Setup Steps\n\n1. Install TensorFlow Serving with Docker\n2. Train and save a model with TensorFlow\n3. Serve the saved model using TensorFlow Serving"
          }
        },
        "summary": "* **Deploying a TensorFlow Model**: After training, save the model using `tf.saved_model.save()` and deploy it for sharing or serving.\n    * *Saved models are self-contained, including trained parameters and computation.*\n        * They can be used with TensorFlow Lite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub."
      },
      "Serving (Online, Batch, and Caching)": {
        "content": "In Chapter 5, “Architecting ML Solutions,” we covered two types of serving options in ML systems, batch prediction (or offline serving) and online prediction, and their recommended architectures. In this chapter, we will cover some best practices for your serving and caching strategy.",
        "subsections": {
          "Real‐Time Static and Dynamic Reference Features": {
            "content": "There are two types of input features that are fetched in real time to invoke the model for prediction: static reference features and dynamic reference features. Let's look at Table 10.1 to understand the differences.\n\n**TABLE 10.1** Static vs. dynamic features\n\nStatic Reference Features | Dynamic Real‐Time Features\n---|---\nTheir values do not change in real time. Instead, the values are usually updated in a batch. | Real‐time features are computed on the fly in an event‐stream processing pipeline.\nThese types of features are usually available in a data warehouse—for example, customer ID and movie ID. | For real‐time features, you need a list of aggregated values for a particular window (fixed, sliding, or session) in a certain period of time and not an overall aggregation of values within that period of time.\nUse cases are estimating the price of a house based on the location of the house or recommending similar products given the attributes of the products that a customer is currently viewing. | Use cases can be predicting whether an engine will fail in the next hour given real‐time sensor data. Another use case can be in recommending the next news article to read based on the list of last N viewed articles by the user during the current session.\nThese types of static reference features are stored in a NoSQL database that _is_ optimized for singleton lookup operations, such as Firestore. BigQuery is not optimized for singleton reads, for example, a query like \"Select 100 columns from several tables for a specific customer ID,” where the result is a single row with many columns. | You can use a Dataflow streaming pipeline to implement this use case for dynamic feature read. For dynamic feature creation, the pipeline captures and aggregates (sum, mean, and so on) the events in real time and stores them in a low‐latency read/write database. Cloud Bigtable is a good option for a low‐latency read/write database for feature values.\nStatic reference architecture (see Figure 10.2). | Dynamic reference architecture (see Figure 10.3).\n\n**FIGURE 10.2** Static reference architecture\n\n**FIGURE 10.3** Dynamic reference architecture",
            "subsections": {},
            "summary": "* *_Two types of input features:_*\n    * **Static Reference Features**: values that do not change in real time, updated in batches.\n    * **Dynamic Real‐Time Features**: computed on the fly, used for real-time predictions.\n* *_Use cases:_*\n  * Predicting price based on location or recommending similar products.\n  * Predicting engine failure based on sensor data or recommending next news article.\n* *_Storage and processing:_*\n  * Static features: NoSQL databases like Firestore, big data warehouses.\n  * Dynamic features: low-latency read/write databases like Cloud Bigtable, streaming pipelines."
          },
          "Pre‐computing and Caching Prediction": {
            "content": "Another approach to improve online prediction latency is to pre‐compute predictions in an offline batch scoring job and store them in a low‐latency read data store like Memorystore or Datastore for online serving. In these cases, the client (mobile, web, data pipeline worker, backend, or frontend API) doesn't call the model for online prediction. Instead, the client fetches the pre‐computed predictions from the data store, assuming the prediction exists. The process works like this:\n\n  1. Data is ingested, processed, and stored in a key‐value store.\n  2. A trained and deployed model runs a batch prediction job on the prepared data to produce predictions for the input data. Each prediction is identified by a key.\n  3. A data pipeline exports the predictions referenced by a key to a low‐latency data store that's optimized for singleton reads.\n  4. A client sends a prediction request referenced by a unique key.\n  5. The ML gateway reads from the data store using the entry key and returns the corresponding prediction.\n\nThe client receives the prediction response. Figure 10.4 shows the flow.\n\n**FIGURE 10.4** Caching architecture\n\nPrediction requests can be used for two categories of lookup keys:\n\n  * Specific entity where predictions relate to a single entity based on an ID such as customerid, or a deviceid, or a specific combination of input features. If you have too many entities (high cardinality), it can be challenging to precompute prediction in a limited amount of time. An example is forecasting daily sales by item when you have hundreds of thousands or millions of items. In that case, you can use a hybrid approach as per the architecture diagram, where you precompute predictions for the top N entities, such as for the most active customers or the most viewed products. You can then use the model directly for online prediction for the rest of the entities.\n  * Specific combination of input features to figure out whether an anonymous or a new customer will buy something on your website. When you have a combination of input features, you would create a hashed approach where the key is a hashed combination of all possible input features and the value is the prediction.",
            "subsections": {},
            "summary": "* **Pre-computing predictions**: Store pre-computed predictions in a low-latency data store for online serving.\n  * Benefits: Reduced latency, improved performance\n  * Challenges: Limited to single entity or feature combinations, high cardinality may be challenging to handle\n  * Hybrid approach: Precompute top N entities and use model directly for others"
          }
        },
        "summary": "* *_Best Practices for Serving Strategy_*: Batch prediction and online prediction use different architectures\n* *_Caching Strategies_*: Implement cache to reduce computation time and improve performance\n* *_Key Considerations_*: Balance serving speed with model accuracy and stability"
      },
      "Google Cloud Serving Options": {
        "content": "In Google Cloud, you can deploy your models for either online predictions or batch predictions. You can perform batch and online predictions for both AutoML and custom models. In the following sections, we will cover how to set up online and batch jobs using Vertex AI.",
        "subsections": {
          "Online Predictions": {
            "content": "To set up a real‐time prediction endpoint, you might have these two use cases:\n\n  * **Models trained in Vertex AI using Vertex AI training:** This can be AutoML or custom models, as explained in Chapter 8, “Model Training and Hyperparameter Tuning.”\n  * **Model trained elsewhere (on‐premise, on another cloud, or in local device):** If you already have a model trained elsewhere, you need to import the model to Google Cloud before deploying and creating an endpoint.\n\nIf you are importing as a prebuilt container, ensure that your model artifacts have filenames that exactly match the following examples:\n\n  *     * **TensorFlow SavedModel:** `saved_model.pb`\n    * **scikit‐learn:** `model.joblib` or `model.pkl`\n    * **XGBoost:** `model.bst`, `model.joblib`, or `model.pkl`\n\nIf you are importing a custom container, you need to create a container image and push the image possibly using Cloud Build to Artifact Registry as per the requirements for custom container hosting.\n\nFor both options, you need to follow these steps to set up Vertex AI predictions:\n\n  1. Deploy the model resource to an endpoint.\n  2. Make a prediction.\n  3. Undeploy the model resource if the endpoint is not in use.",
            "subsections": {
              "Deploying the Model": {
                "content": "When you deploy the model using the Vertex AI Prediction endpoint, you specify the autoscaling for the endpoint's container (TensorFlow, scikit‐learn, or XGBoost). Vertex AI automatically provisions the container resources and sets up autoscaling for your endpoint. You can deploy more than one model to an endpoint, and you can deploy a model to more than one endpoint. There are two ways you can deploy the model using an API and using the Google Cloud console.",
                "subsections": {
                  "Deploying a Model Using an API": {
                    "content": "You can deploy your model by calling the Vertex AI `deploy` function on the `Model` resource. The `deploy` method will create an endpoint and also deploy your model on the endpoint. You need to provide a traffic‐split, machine‐type GPU and the min nodes to set the `deploy` method. The following `deploy` function returns an `Endpoint` object.\n[code]\n        Endpoint = model.deploy(\n             deployed_model_display_name=DEPLOYED_NAME,\n             traffic_split=TRAFFIC_SPLIT,\n             machine_type=DEPLOY_COMPUTE,\n             accelerator_type=DEPLOY_GPU.name,\n             accelerator_count=DEPLOY_NGPU,\n             min_replica_count=MIN_NODES,\n             max_replica_count=MAX_NODES,\n         )\n\n[/code]\n\nThe previous Vertex AI function takes the following parameters:\n\n  * `deployed_model_display_name`: A human readable name for the deployed model.\n  * `TRAFFIC_SPLIT`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n  * If only one model, then specify as { “0”: 100 }, where “0” refers to this model being uploaded and 100 means 100% of the traffic.\n  * If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify as { “0”: percent, `model_id`: percent, … }, where `model_id` is the model ID of an existing model to the deployed endpoint. The percentages must add up to 100.\n\n* * *\n\nTraffic splits help split traffic between two versions of a model. These versions can be used for A/B testing, which we will cover in the next section.\n\n* * *\n\n  * `machine_type`: The type of machine to use for training.\n  * `accelerator_type`: The hardware accelerator type.\n  * `accelerator_count`: The number of accelerators to attach to a worker replica.\n  * `starting_replica_count`: The number of compute instances to initially provision.\n  * `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n\nYou can deploy the model using the previous API or using the Google Cloud console.",
                    "subsections": {},
                    "summary": "* **Deploying a Model**: Deploy your model by calling the Vertex AI `deploy` function on the `Model` resource.\n    * _Required parameters:_ \n      * `deployed_model_display_name`\n      * `traffic_split`\n      * `machine_type`\n      * `accelerator_type`\n      * `accelerator_count`\n      * `min_replica_count`\n      * `max_replica_count`\n\n* **Endpoint Creation**: The `deploy` function returns an `Endpoint` object and creates an endpoint with the deployed model.\n    * _Example usage:_ \n      ```python\nEndpoint = model.deploy(\n  ...\n)\n```"
                  },
                  "Deploying a Model Using the Google Cloud Console": {
                    "content": "If you are using the GCP console, you can deploy the model by going to the Model Registry. The Model Registry is a centralized place to track versions of both AutoML and custom models. You can also choose the version you want to deploy. Vertex AI – trained AutoML and custom models appear in the Model Registry automatically. You can select the model and version, click the three dots, and select Deploy To Endpoint to deploy your model.\n\nWhen you click Deploy To Endpoint in the Model Registry, you are redirected to the page to define the endpoint, as shown in Figure 10.5. In the Model settings section, you can define traffic split, compute resources, and autoscaling options, and in the Model monitoring section, you can enable the option to monitor the model in production for any drift.\n\n**FIGURE 10.5** Deploying to an endpoint\n\n* * *\n\nYour Model Registry has an IMPORT option to upload models from anywhere. If you have models trained in BigQuery ML, you can register them with the Vertex AI Model Registry.\n\n* * *",
                    "subsections": {},
                    "summary": "* To deploy a model on GCP using the console, navigate to the Model Registry and select the desired AutoML or custom model version.\n* Click **Deploy To Endpoint** and configure endpoint settings, such as traffic split and compute resources.\n* Use the IMPORT option in the Model Registry to upload models from BigQuery ML."
                  }
                },
                "summary": "* **Deployment Options**: \n  * Deploy models using Vertex AI's Prediction endpoint with autoscaling for container resources.\n  * Can deploy multiple models to a single endpoint or multiple endpoints for individual models."
              },
              "Make Predictions": {
                "content": "Before you can run the data through the endpoint, you need to preprocess it to match the format that your custom model defined in `task.py` expects. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n\n  * You will see in the output for each prediction the confidence level for the prediction.\n[code]predictions = endpoint.predict(instances=x_test)\n\n[/code]\n\nYou get a REST endpoint ID that you can embed in a website or API gateway or App Engine to create an ML gateway for real‐time prediction.\n\n  * If you're using one of the prebuilt containers to serve predictions using TensorFlow, scikit‐learn, or XGBoost, your prediction input instances need to be formatted as JSON.\n  * If your model uses a custom container, your input must be formatted as JSON, and there is an additional parameters field that can be used for your container.\n\nGo to the Endpoints page in the Vertex AI GCP console, select the endpoint, and click the sample request. You will then get instructions on how to preprocess your data to get predictions using the endpoint in REST and Python, as shown in Figure 10.6.\n\n**FIGURE 10.6** Sample prediction request",
                "subsections": {},
                "summary": "* To run data through the endpoint, you must pre-process it according to the `task.py` model format.\n    * Pre-processing requirements vary depending on the container used (prebuilt or custom).\n        * For prebuilt containers (TensorFlow, scikit-learn, XGBoost), input must be formatted as JSON.\n        * For custom containers, input must be formatted as JSON and may include additional parameters."
              },
              "A/B Testing of Different Versions of a Model": {
                "content": "A/B testing compares the performance of two versions of a model in machine learning to see which one appeals more to visitors/viewers. It tests a control (A) version against a variant (B) version to measure which one is most successful based on your key metrics.\n\nDeploying two models to the same endpoint lets you gradually replace one model with the other. In scenarios for A/B testing where you don't want to create sudden changes in your application, you can add the new model to the same endpoint, serving a small percentage of traffic, and gradually increase the traffic split for the new model until it is serving 100 percent of the traffic.\n\nFor A/B testing in Vertex AI, we covered the traffic‐split parameter while deploying a model. You can provide the model_id and the split value to split the traffic to this traffic‐split parameter while hosting two models to the same endpoint. Here's an example using the gcloud command in the Cloud Shell:\n[code]\n     gcloud ai endpoints update <Endpoint> --region=us-central1\n      --traffic-split=[DEPLOYED_MODEL_ID=Value] --service-account= <Service account>\n     --project= <Project>\n     --traffic-split=[_DEPLOYED_MODEL_ID_ =_VALUE_ ,…]\n\n[/code]\n\nIn the above code, traffic‐split parameters take a list of deployed model id and their value set as traffic‐split. Check out this link to learn more about the parameters to deploy models:\n\n`https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/update`\n\nSince the resources are associated with the model rather than the endpoint, you could deploy models of different types to the same endpoint. However, the best practice is to deploy models of a specific type (for example, AutoML text, AutoML tabular, custom‐trained) to an endpoint. This configuration is easier to manage.\n\nOut of the box, other than using traffic‐split, Vertex AI does not have all the capabilities that you typically have for A/B testing like controlling the model traffic, experimentation, tracking results, comparison, and so on.\n\nHowever, you can use the Vertex AI model evaluations feature with Vertex AI Model Monitoring to create a setup for A/B testing. This feature is in experimental release right now.\n\nThe Vertex AI model evaluation feature allows you to run model evaluation jobs (measure model performance on a test dataset) regardless of which Vertex service is used to train the model (AutoML, managed pipelines, custom training, etc.), and store and visualize the evaluation results across multiple models in the Vertex AI Model Registry. With these capabilities, Vertex AI model evaluation enables users to decide which model(s) can progress to online testing or be put into production and, once they're in production, when models need to be retrained.",
                "subsections": {},
                "summary": "* **A/B Testing** is a method of comparing two versions of a machine learning model to see which one performs better.\n    * It involves deploying two models to the same endpoint and gradually replacing one with the other.\n    * This allows for smooth transitions without disrupting the application.\n\n* _Vertex AI A/B testing capabilities_:\n    * Not all features are available out-of-the-box, including controlling model traffic and experimentation tracking.\n    * The Vertex AI model evaluation feature can be used to create an A/B testing setup in experimental release."
              },
              "Undeploy Endpoints": {
                "content": "If you do not have a critical business and you need to get endpoints up and running for a few hours in a day or during weekdays, you would need to undeploy the endpoints. You incur charges for a running endpoint. So, it's better to undeploy them when they're not in use. You can use the `undeploy` function to undeploy, as shown in the following Python code snippet:\n[code]\n     deployed_model_id = endpoint.list_models()[0].id\n     endpoint.undeploy(deployed_model_id=deployed_model_id)\n\n[/code]",
                "subsections": {},
                "summary": "* **Deployments are costly**: Running endpoints incurs charges.\n* **Undeploy for cost savings**: Undeploy endpoints when not in use to avoid unnecessary costs.\n* *_Use `undeploy` function_*: Deploy the `list_models` Python code snippet to undeploy an endpoint."
              },
              "Send an Online Explanation Request": {
                "content": "If you have configured your model for the Vertex Explainable AI, then you can get online explanations. Online explanation requests have the same format as online prediction requests, and they return similar responses; the only difference is that online explanation responses include feature attributions as well as predictions. The following sample Python code sends an online explanation request using the Vertex AI Python SDK:\n[code]\n         aiplatform.init(project=project, location=location)\n\n         endpoint = aiplatform.Endpoint(endpoint_id)\n\n         response = endpoint.explain(instances=[instance:dict], parameters={})\n\n[/code]",
                "subsections": {},
                "summary": "* **Requesting Online Explanations**: Send an online explanation request using the Vertex AI Python SDK.\n    * Format similar to online prediction requests\n    * Returns predictions and feature attributions \n    * Example code provided"
              }
            },
            "summary": "* To deploy a real-time prediction endpoint, you can use models trained in Vertex AI using AutoML or custom models\n    * Or import a model trained elsewhere (on-premise, another cloud, or local device)\n        * Ensure model artifacts have correct filenames for import (e.g. `saved_model.pb`, `model.joblib`)\n* To set up the endpoint:\n  * Deploy the model resource\n  * Make a prediction\n  * Undeploy the model resource if not in use"
          },
          "Batch Predictions": {
            "content": "In batch prediction, you point to the model and the input data (production data) in Google Cloud Storage and run a batch prediction job. The job runs the prediction using the model on the input data and saves the predictions output in Cloud Storage.\n\nYou need to make sure your input data is formatted as per the requirements for either an AutoML model (vision, video, image, text, tabular) or a custom model (prebuilt or custom container). To get batch predictions from a custom‐trained model, prepare your input data in one of the ways described in Table 10.2.\n\n**TABLE 10.2** Input data options for batch training in Vertex AI\n\nInput | Description\n---|---\nJSON Lines | Use a JSON Lines file to specify a list of input instances to make predictions about. Store the JSON Lines file in a Cloud Storage bucket.\nTFRecord | You can optionally compress the TFRecord files with gzip. Store the TFRecord files in a Cloud Storage bucket. Vertex AI reads each instance in your TFRecord files as binary, then base64‐encodes the instance as a JSON object with a single key named b64.\nCSV files | Specify one input instance per row in a CSV file. The first row must be a header row. You must enclose all strings in double quotation marks (\").\nFile list | Create a text file where each row is the Cloud Storage URI to a file. Example: `gs://path/to/image/image1.jpg`.\nBigQuery | Specify a BigQuery table as projectId.datasetId.tableId. Vertex AI transforms each row from the table to a JSON instance.\n\nSimilar to online prediction, you can use either Vertex AI APIs or the Google Cloud console to create a batch prediction job (see Figure 10.7).\n\n**FIGURE 10.7** Batch prediction job in Console\n\nYou can select for output either a BigQuery table or a Google Cloud Storage bucket. You can also enable model monitoring (in Preview) to detect skew.\n\n* * *\n\nFor the exam, it's important to understand when to apply batch predictions and when to recommend online predictions in a use case. If the data is to be predicted in near real time, suggest online predictions; otherwise, suggest batch prediction.\n\n* * *",
            "subsections": {},
            "summary": "### Batch Prediction\n\nBatch prediction involves:\n* Pointing to the model and input data in Google Cloud Storage\n* Running a job that predicts using the model on the input data\n* Saving the output in Cloud Storage\n\nFormat input data according to AutoML or custom model requirements:\n* JSON Lines\n* TFRecord files\n* CSV files\n* File list\n* BigQuery table\n\nCreate batch prediction job through Vertex AI APIs or Google Cloud console."
          }
        },
        "summary": "* **Prediction Methods**: \n  * Batch predictions\n  * Online predictions (both for AutoML and custom models)\n*"
      },
      "Hosting Third‐Party Pipelines (MLflow) on Google Cloud": {
        "content": "In this section, we will cover various ways you can host third‐party pipelines on Google Cloud that are specific to MLflow. MLflow (`https://mlflow.org/docs/latest/concepts.html`) is an open source platform for managing the end‐to‐end machine learning life cycle. It is library agnostic, and you can use it with any machine learning library and in any programming language. It tackles four primary functions:\n\n  * **MLflow Tracking** For experiment tracking and to record and compare parameters and results. You can think of this similar to Vertex AI Experiments used to track experiments in Vertex AI. We will cover experiments in upcoming Chapter 14.\n  * **MLflow Projects** For packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production.\n  * **MLflow Models** For managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms.\n  * **MLflow Model Registry** For providing a central model store to collaboratively manage the full life cycle of an MLFlow Model, including model versioning, stage transitions, and annotations. This is similar to the Vertex AI Model Registry, which is a centralized place to store models with versioning and annotations.\n\nYou can use each of these components on their own. For example, maybe you want to track experiments using MLflow without using MLflow Model Registry or Projects.\n\nYou would use MLflow on Google Cloud to leverage the scalability and availability provided by the Google Cloud Vertex AI platform for model training and hosting. You have access to high‐performance compute such as GPUs, TPUs, and CPUs to run MLflow training and host workloads on Google Cloud.\n\nTo run or install MLflow on Google Cloud:\n\n  * Create a PostgreSQL DB for storing model metadata.\n  * Create a Google Cloud Storage bucket for storing artifacts.\n  * Create a Compute Engine instance to install MLFlow and run the MLFlow server, or you can run MLFlow Projects with Docker environments on Kubernetes.\n\nYou can also run MLFlow using a Google Cloud plug‐in without needing to install the Docker image provided by MLFlow. Refer to this link for plug‐in details:\n\n`https://pypi.org/project/google-cloud-mlflow`",
        "subsections": {},
        "summary": "* **MLflow Overview**\n  * An open-source platform for managing the end-to-end machine learning life cycle\n  * Library agnostic, compatible with various machine learning libraries and programming languages\n  * Tackles four primary functions: tracking, projects, models, and model registry\n* **Hosting on Google Cloud**\n  * Leverage scalability and availability of Google Cloud Vertex AI platform for model training and hosting\n  * Access to high-performance compute resources (GPUs, TPUs, CPUs)\n  * Can run MLflow using a Google Cloud plug-in without installing the Docker image"
      },
      "Testing for Target Performance": {
        "content": "Some of the things you need to consider while testing your model for performance in production are as follows:\n\n  * You need to check for training‐serving skew in the model and test the quality of the model in production with the real‐time data.\n  * You can monitor the model age and performance throughout an ML pipeline.\n  * You can test that model weights and outputs are numerically stable. For example, during model training, your weights and layer outputs should not be NaN or null. You need to write tests to check for NaN and null values of model weights and layer outputs. Moreover, you need to test that more than half of the outputs of a layer are not zero.\n\nVertex AI has services such as Vertex AI Model Monitoring and Vertex AI Feature Store that can help solve some of the testing issues for your deployed model in production, such as detecting skew and monitoring model performance over time.\n\nFor more information, see `https://developers.google.com/machine-learning/testing-debugging/pipeline/production`.",
        "subsections": {},
        "summary": "* **Testing for Model Performance**\n  * Monitor training-serving skew and test with real-time data\n  * Check model age and performance throughout the pipeline\n  * Test numerically stable weights and outputs (e.g., no NaN or null values)\n  \n* _Automated Testing Solutions_\n  * Vertex AI Model Monitoring and Feature Store can detect skew and monitor performance over time."
      },
      "Configuring Triggers and Pipeline Schedules": {
        "content": "For this section, you need to know how to trigger a training or prediction job on the Vertex AI platform based on the use case.\n\nFor example, to trigger a real‐time prediction job, you can set up the prediction endpoint on the Google Cloud App Engine to create a serverless ML gateway. We covered the architecture pattern in the section “Serving (Online, Batch, and Caching)” earlier in this chapter.\n\nThe following are some of the services that will help create a trigger and schedule the training or prediction jobs on Vertex AI:\n\n  * Cloud Scheduler can help you set up a cron job schedule to schedule your Vertex AI training or prediction jobs.\n  * With Vertex AI managed notebooks, you can execute and schedule a Vertex training and prediction job using the Jupyter Notebook. Refer to this link to learn more: `https://cloud.google.com/vertex-ai/docs/workbench/managed/schedule-managed-notebooks-run-quickstart`.\n  * Cloud Build is the CI/CD offering on GCP. If you want to retrain a model or build a Dockerfile and push it for custom training, use Cloud Build to kick it off. Cloud Run is a managed offering to deploy containers. Imagine that the container you're deploying is a Flask or FastAPI app that has your model. You can use Cloud Build to deploy your application to Cloud Run.\n  * You can also use event‐driven serverless Cloud Functions and Cloud Pub/Sub. Cloud Functions are serverless and stateless functions as a service (FaaS), and Pub/Sub implements the publisher‐subscriber pattern. You can use an event‐based Cloud Storage trigger for a Cloud Function so that if a new version of a model is added to a bucket, you can activate a Cloud Function to start the deployment.\n\nIt's a challenge to use Cloud Function and Pub/Sub when there are multiple Cloud Functions doing multiple things. For example, Cloud Function A triggers data transform, then cloud function B triggers model training, and finally, Cloud Function C deploys the model. In this case, you would need an orchestrator to orchestrate these events in a pipeline. To orchestrate data cleaning, data transformation, model deployment, and model training using Cloud Functions, you can use Cloud _Workflows_.\n\n**Cloud Workflows** orchestrate multiple HTTP‐based services into a durable and stateful workflow. Workflows are great for chaining microservices together, automating infrastructure tasks like starting or stopping a VM, and integrating with external systems. Workflow connectors also support simple sequences of operations in Google Cloud services such as Cloud Storage and BigQuery.\n\nIf you do not want to orchestrate using a Cloud Function, use Cloud Scheduler and Workflows.\n\nVertex AI provides the orchestration choices such as Vertex AI pipelines listed in Table 10.3, which we will cover in detail in the next chapter (Chapter 11, “Designing ML Training Pipelines”).\n\n**TABLE 10.3** ML orchestration options\n\nOption | Description\n---|---\nVertex AI Pipelines | Vertex AI Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifact.\nCloud Composer | Cloud Composer is designed to orchestrate data‐driven workflows (particularly ETL/ELT). It's built on the Apache Airflow project, but Cloud Composer is fully managed. Cloud Composer supports your pipelines wherever they are, including on‐premises or across multiple cloud platforms. All logic in Cloud Composer, including tasks and scheduling, is expressed in Python as directed acyclic graph (DAG) definition files.\n\n* * *\n\nWe also covered the TFX pipelines and Kubeflow pipelines to orchestrate Kubeflow and TensorFlow workloads in Chapter 5.\n\n* * *\n\nRead this blog for more information: `https://cloud.google.com/blog/topics/developers-practitioners/choosing-right-orchestrator-google-cloud`.",
        "subsections": {},
        "summary": "* **Triggering Training or Prediction Jobs on Vertex AI**\n    * Cloud Scheduler can set up a cron job schedule to trigger jobs.\n    * Managed Notebooks allow execution and scheduling of training and prediction jobs using Jupyter Notebook.\n    * Cloud Build, Cloud Run, event-driven Cloud Functions, and Pub/Sub are also used for triggering and scheduling jobs."
      },
      "Summary": {
        "content": "In this chapter, we covered the details of TF Serving in scaling prediction service.\n\nWe discussed the `predict` function in TF Serving and how to know the output based on the SignatureDef of the saved TF model.\n\nThen we discussed architecture for online serving. We dove deep into static and dynamic reference architectures. Then we discussed the architecture to use pre‐computing and caching while serving predictions.\n\nWe also covered how to deploy models using online and batch mode with Vertex AI Prediction and Google Cloud serving options. We covered the reasons for performance degradation in production when testing for target performance such as training‐serving skew, change in data quality, and so on. You learned about tools such as Vertex AI Model Monitoring that can help in testing for models in production.\n\nLast, you learned about ways to configure triggers and schedules to automate a model pipeline, such as Cloud Run, Cloud Build, Cloud Scheduler, Vertex AI managed notebooks, and Cloud Composer.",
        "subsections": {},
        "summary": "* **TF Serving Overview**\n  * Covered details of TF Serving for scaling prediction services\n  * Discussed output based on `SignatureDef` of saved TF models\n* **Online Serving Architecture**\n  * Static and dynamic reference architectures\n  * Pre-computing and caching while serving predictions\n* **Model Deployment and Monitoring**\n  * Online and batch mode deployment options\n  * Tools for testing model performance, such as Vertex AI Model Monitoring"
      },
      "Exam Essentials": {
        "content": "* **Understand TensorFlow Serving.** Understand what TensorFlow Serving is and how to deploy a trained TensorFlow model using TF Serving. Know the different ways to set up TF Serving with Docker. Understand the TF Serving prediction response based on a saved model's SignatureDef tensors.\n  * **Understand the scaling prediction services (online, batch, and caching).** Understand the difference between online batch and caching. For online serving, understand the differences in architecture and use cases with respect to input features that are fetched in real time to invoke the model for prediction (static reference features and dynamic reference features). Also, understand the caching strategies to improve serving latency.\n  * **Understand the Google Cloud serving options.** Understand how to set up real‐time endpoints using Google Cloud Vertex AI Prediction for custom models or models trained outside Vertex AI; understand how to set up predictions using both APIs and the GCP console setup. Also, understand how to set up a batch job for any model using Vertex AI batch prediction.\n  * **Test for target performance.** Understand why model performance in production degrades. Also understand at a high level how Vertex AI services such as Vertex AI Model Monitoring can help with performance degradation issues.\n  * **Configure triggers and pipeline schedules.** Understand ways to set up a trigger to invoke a trained model or deploy a model for prediction on Google Cloud. Know how to schedule the triggers, such as using Cloud Scheduler and the Vertex AI managed notebooks scheduler. Also, learn how to automate the pipeline with Workflows, Vertex AI Pipelines, and Cloud Composer.",
        "subsections": {},
        "summary": "* **TensorFlow Serving Overview**\n    * *A framework for serving machine learning models.*\n    * *Deploy a trained TensorFlow model using TF Serving and set up various deployment options.*\n\n* **Scaling Prediction Services**\n    * *Online batch and caching strategies to improve serving latency.*\n    * *Understand differences in architecture and use cases with real-time input features.*\n\n* **Google Cloud Serving Options**\n    * *Setup real-time endpoints, batch jobs, and automate pipeline workflows.*\n    * *Use Google Cloud Vertex AI APIs, console setup, and managed notebooks for custom models.*\n\n* **Model Performance and Optimization**\n    * *Understand why model performance degrades in production.*\n    * *Use Vertex AI services for model monitoring and optimization.*\n\n* **Configure Triggers and Pipelines**\n    * *Set up triggers to invoke trained models or deploy new ones on Google Cloud.*\n    * *Automate pipeline workflows with Cloud Scheduler, Workflows, and Cloud Composer."
      },
      "Review Questions": {
        "content": "1. You are a data scientist working for an online travel agency. You have been asked to predict the most relevant web banner that a user should see next in near real time. The model latency requirements are 300ms@p99, and the inventory is thousands of web banners. You want to implement the simplest solution on Google Cloud. How should you configure the prediction pipeline?\n     1. Embed the client on the website, and cache the predictions in a data store by creating a batch prediction job pointing to the data warehouse. Deploy the gateway on App Engine, and then deploy the model using Vertex AI Prediction.\n     2. Deploy the model using TF Serving.\n     3. Deploy the model using the Google Kubernetes engine.\n     4. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on Vertex AI.\n  2. You are a data scientist training a text classification model in TensorFlow using the Vertex AI platform. You want to use the trained model for batch predictions on text data stored in BigQuery while minimizing computational overhead. What should you do?\n     1. Submit a batch prediction job on Vertex AI that points to input data as a BigQuery table where text data is stored.\n     2. Deploy and version the model on the Vertex AI platform.\n     3. Use Dataflow with the SavedModel to read the data from BigQuery.\n     4. Export the model to BigQuery ML.\n  3. You are a CTO of a global bank and you appointed an ML engineer to build an application for the bank that will be used by millions of customers. Your team has built a forecasting model that predicts customers' account balances three days in the future. Your team will use the results in a new feature that will notify users when their account balance is likely to drop below a certain amount. How should you serve your predictions?\n     1. Create a Pub/Sub topic for each user. Deploy a Cloud Function that sends a notification when your model predicts that a user's account balance will drop below the threshold.\n     2. Create a Pub/Sub topic for each user. Deploy an application on the App Engine environment that sends a notification when your model predicts that a user's account balance will drop below the threshold.\n     3. Build a notification system on Firebase. Register each user with a user ID on the Firebase Cloud Messaging server, which sends a notification when the average of all account balance predictions drops below the threshold.\n     4. Build a notification system on a Docker container. Set up cloud functions and Pub/Sub, which sends a notification when the average of all account balance predictions drops below the threshold.\n  4. You are a data scientist and you trained a text classification model using TensorFlow. You have downloaded the saved model for TF Serving. The model has the following SignatureDefs:\n[code]input ['text'] tensor_info:\n          dtype: String\n          shape: (-1, 2)\n          name: dnn/head/predictions/textclassifier\n          SignatureDefs for output.\n          output ['text'] tensor_info:\n          dtype: String\n          shape: (-1, 2)\n          name: tfserving/predict\n\n[/code]\n\nWhat is the correct way to write the predict request?\n\n     1. `json.dumps({'signature_name': 'serving_default'\\ 'instances': [fab', 'be1, 'cd']]})`\n     2. `json dumps({'signature_name': 'serving_default'! 'instances': [['a', 'b', 'c', 'd', 'e', 'f']]})`\n     3. `json.dumps({'signature_name': 'serving_default, 'instances': [['a', 'b\\ 'c'1, [d\\ 'e\\ T]]})`\n     4. `json dumps({'signature_name': f,serving_default', 'instances': [['a', 'b'], [c\\ 'd'], ['e\\ T]]})`\n  5. You are an ML engineer who has trained a model on a dataset that required computationally expensive preprocessing operations. You need to execute the same preprocessing at prediction time. You deployed the model on the Vertex AI platform for high‐throughput online prediction. Which architecture should you use?\n     1. Send incoming prediction requests to a Pub/Sub topic. Set up a Cloud Function that is triggered when messages are published to the Pub/Sub topic. Implement your preprocessing logic in the Cloud Function. Submit a prediction request to the Vertex AI platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.\n     2. Stream incoming prediction request data into Cloud Spanner. Create a view to abstract your preprocessing logic. Query the view every second for new records. Submit a prediction request to the Vertex AI platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.\n     3. Send incoming prediction requests to a Pub/Sub topic. Transform the incoming data using a Dataflow job. Submit a prediction request to the Vertex AI platform using the transformed data. Write the predictions to an outbound Pub/Sub queue.\n     4. Validate the accuracy of the model that you trained on preprocessed data. Create a new model that uses the raw data and is available in real time. Deploy the new model on to the Vertex AI platform for online prediction.\n  6. As the lead data scientist for your company, you are responsible for building ML models to digitize scanned customer forms. You have developed a TensorFlow model that converts the scanned images into text and stores them in Cloud Storage. You need to use your ML model on the aggregated data collected at the end of each day with minimal manual intervention. What should you do?\n     1. Use the batch prediction functionality of the Vertex AI platform.\n     2. Create a serving pipeline in Compute Engine for prediction.\n     3. Use Cloud Functions for prediction each time a new data point is ingested.\n     4. Deploy the model on the Vertex AI platform and create a version of it for online inference.\n  7. As the lead data scientist for your company, you need to create a schedule to run batch jobs using the Jupyter Notebook at the end of each day with minimal manual intervention. What should you do?\n     1. Use the schedule function in Vertex AI managed notebooks.\n     2. Create a serving pipeline in Compute Engine for prediction.\n     3. Use Cloud Functions for prediction each time a new data point is ingested.\n     4. Use Cloud Workflow to schedule the batch prediction Vertex AI job by cloud function.\n  8. You are a data scientist working for an online travel agency. Your management has asked you to predict the most relevant news article that a user should see next in near real time. The inventory is in a data warehouse, which has thousands of news articles. You want to implement the simplest solution on Google Cloud with the least latency while serving the model. How should you configure the prediction pipeline?\n     1. Embed the client on the website, deploy the gateway on App Engine, and then deploy the model using Vertex AI Prediction.\n     2. Deploy the model using TF Serving.\n     3. Deploy the model using Google Kubernetes Engine.\n     4. Embed the client on the website, deploy the gateway on App Engine, deploy the database on Cloud Bigtable for writing and for reading the user's navigation context, and then deploy the model on Vertex AI.",
        "subsections": {},
        "summary": "* As a data scientist working on Google Cloud, implement the simplest solution to predict web banners in near real-time with latency < 300ms@p99.\n    * Embed client on website, cache predictions in data store by creating batch prediction job pointing to data warehouse, deploy gateway on App Engine, and deploy model using Vertex AI Prediction\n* To make batch predictions on text data stored in BigQuery while minimizing computational overhead:\n    * Deploy and version model on Vertex AI platform\n* When serving predictive models for high-stakes applications with millions of users, consider using a pub/sub notification system.\n    * Create Pub/Sub topic for each user, deploy Cloud Function to send notifications when account balance is predicted to drop below threshold\n* For the correct way to write predict requests for the TF Serving model:\n    * Use `json.dumps({'signature_name': 'serving_default', 'instances': [[a, b], [c, d]]})`\n* To execute preprocessing logic at prediction time with Vertex AI platform:\n    * Send incoming prediction requests to a Pub/Sub topic and implement preprocessing in Cloud Function\n* For using your pre-trained ML model on aggregated data collected daily:\n    * Deploy the model on the Vertex AI platform for online inference, or use batch prediction functionality of the Vertex AI platform\n* To schedule batch jobs using Jupyter Notebook at end of each day with minimal manual intervention:\n    * Use schedule function in Vertex AI managed notebooks\n\n* As a data scientist working on Google Cloud, predict the most relevant news article that a user should see next in near real-time with latency < 300ms@p99.\n    * Embed client on website, deploy gateway on App Engine, deploy database on Cloud Bigtable for writing and reading user's navigation context, deploy model using Vertex AI Prediction"
      }
    },
    "summary": ""
  },
  "Chapter 11Designing ML Training Pipelines": {
    "content": "",
    "subsections": {
      "Orchestration Frameworks": {
        "content": "You need an orchestrator to manage the various steps, such as cleaning data, transforming data, and training a model, in the ML pipeline. The orchestrator runs the pipeline in a sequence and automatically moves from one step to the next based on the defined conditions. For example, a defined condition might be executing the model‐serving step after the model‐evaluation step if the evaluation metrics meet the predefined thresholds. Orchestrating the ML pipeline is useful in both the development and production phases:\n\n  * During the development phase, orchestration helps the data scientists to run the ML experiment instead of having to manually execute each step.\n  * During the production phase, orchestration helps automate the execution of the ML pipeline based on a schedule or certain triggering conditions.\n\nWe will cover Kubeflow Pipelines, Vertex AI Pipelines, Apache Airflow, and Cloud Composer in the following sections as different ML pipeline orchestrators you can use.",
        "subsections": {
          "Kubeflow Pipelines": {
            "content": "Before understanding how Kubeflow Pipelines works, you should understand what Kubeflow is. Kubeflow is the ML toolkit for Kubernetes. (You can learn more about it at `www.kubeflow.org/docs/started/architecture`.) Kubeflow builds on Kubernetes as a system for deploying, scaling, and managing complex systems. Using Kubeflow, you can specify any ML framework required for your workflow, such as TensorFlow, PyTorch, or MXNet. Then you can deploy the workflow to various clouds or local and on‐premises platforms for experimentation and for production use. Figure 11.3 shows how you can use Kubeflow as a platform for arranging the components of your ML system on top of Kubernetes.\n\n**FIGURE 11.3** Kubeflow architecture\n\nWhen you develop and deploy an ML system, the ML workflow typically consists of several stages. Developing an ML system is an iterative process. You need to evaluate the output at various stages of the ML workflow and apply changes to the model and parameters when necessary to ensure the model keeps producing the results you need.\n\nKubeflow Pipelines is a platform for building, deploying, and managing multistep ML workflows based on Docker containers.\n\nA pipeline is a description of an ML workflow in the form of a graph, including all of the components in the workflow and how the components relate to each other. The pipeline configuration includes the definition of the inputs (parameters) required to run the pipeline and the inputs and outputs of each component.\n\nWhen you run a pipeline, the system launches one or more Kubernetes pods corresponding to the steps (components) in your workflow (pipeline). The pods start Docker containers, and the containers in turn start your programs, as shown in Figure 11.4.\n\n**FIGURE 11.4** Kubeflow components and pods\n\nThe Kubeflow Pipelines platform consists of the following:\n\n  * A user interface (UI) for managing and tracking experiments, jobs, and runs\n  * An engine for scheduling multistep ML workflows\n  * An SDK for defining and manipulating pipelines and components\n  * Notebooks for interacting with the system using the SDK\n  * Orchestration, experimentation, and reuse\n\nYou can install Kubeflow Pipelines on Google Cloud on GKE or use managed Vertex AI Pipelines to run Kubeflow Pipelines on Google Cloud.\n\n* * *\n\nYou can also install Kubeflow Pipelines on on‐premises or local systems for testing purposes.\n\n* * *",
            "subsections": {},
            "summary": "### Overview of Kubeflow Pipelines\nKubeflow Pipelines is a platform for building, deploying, and managing multistep ML workflows based on Docker containers.\n* _Key Components:_ \n  * User interface for managing experiments, jobs, and runs\n  * Engine for scheduling multistep ML workflows\n  * SDK for defining and manipulating pipelines and components\n* _Deployment Options:_ \n  * Google Cloud on GKE or managed Vertex AI Pipelines\n  * On-premises or local systems for testing"
          },
          "Vertex AI Pipelines": {
            "content": "You can use Vertex AI Pipelines to run Kubeflow Pipelines or TensorFlow Extended pipelines without spinning any servers to set up the Kubeflow infrastructure or the TFX infrastructure. Vertex AI Pipelines automatically provisions the underlying infrastructure and manages it for you. You can bring your existing Kubeflow or TFX pipeline code and run it serverless on Vertex AI Pipelines.\n\nVertex AI Pipelines also provides data lineage. _Lineage_ in machine learning means tracking the movement of data over time from the source system to transformations and to the data's consumption by a model. This includes all the transformations the data underwent along the way, starting from source system, how the data was transformed, what changed, and why. Each pipeline run produces _metadata_ and ML _artifacts_ , such as models or datasets. _Artifact lineage_ describes all the factors that resulted in an artifact, such as training data or hyperparameters used for model training. By using artifact lineage, you can understand the differences in performance or accuracy over several pipeline runs:\n\n  * Pipelines let you automate, monitor, and experiment with interdependent parts of an ML workflow.\n  * ML pipelines are portable, scalable, and based on containers.\n  * Each individual part of your pipeline workflow (for example, creating a dataset or training a model) is defined by code. This code is referred to as a _component_. Each instance of a component is called a _step_. Components are composed of a set of inputs, a set of outputs, and the location of a container image. A component's container image is a package that includes the component's executable code and a definition of the environment that the code runs in.\n  * Pipeline components are self‐contained sets of code that perform one part of a pipeline's workflow, such as data preprocessing, data transformation, and training a model.\n  * You can build custom components, or you can reuse pre‐built components. To use features of Vertex AI like AutoML in your pipeline, use the Google Cloud pipeline components. You can learn more about using Google Cloud pipeline components in your pipeline at `https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#google-cloud-components`.\n\nFigure 11.5 shows Vertex AI Pipelines.\n\n**FIGURE 11.5** Vertex AI Pipelines\n\nThe pipeline in Figure 11.5 first created a dataset and then started an AutoML training job. The pipeline has set up a threshold to deploy a model only when the threshold for _area under the curve (AUC)_ is more than 95 percent. You can add conditions and parameters to the pipeline before deploying the model in production, as shown in Figure 11.6.\n\n**FIGURE 11.6** Vertex AI Pipelines condition for deployment\n\nYou can see the lineage of pipeline resources by clicking the resource and clicking Lineage. For example, click Endpoint, and then in the panel on the right you will see View Lineage, which will redirect you to the lineage graph, as shown in Figure 11.7.\n\n**FIGURE 11.7** Lineage tracking with Vertex AI Pipelines\n\nYou can also track lineage by using Vertex ML Metadata, a feature of Vertex AI that stores artifacts and metadata for pipelines run using Vertex AI Pipelines, as shown in Figure 11.8.\n\n**FIGURE 11.8** Lineage tracking in Vertex AI Metadata store\n\nBy building a pipeline with the Kubeflow Pipelines SDK, you can implement your workflow by building custom components or reusing pre‐built components, such as the Google Cloud Pipeline Components.\n\n* * *\n\nVertex AI Pipelines can run pipelines built using the Kubeflow Pipelines SDK v1.8.9 or higher or TensorFlow Extended v0.30.0 or higher.\n\n* * *\n\nRefer to `https://cloud.google.com/vertex-ai/docs/pipelines/migrate-kfp` to learn more about how you can migrate your existing pipelines in Kubeflow Pipelines to Vertex AI Pipelines.",
            "subsections": {},
            "summary": "* **Vertex AI Pipelines**: automatically provisions and manages infrastructure for Kubeflow or TFX pipelines\n    * Run serverless on Vertex AI Pipelines with existing code\n    * Provides data lineage and artifact metadata for pipeline runs\n* **Pipeline Components**: self-contained sets of code that perform one part of a workflow, such as data preprocessing or training a model\n    * Can build custom components or reuse pre-built ones\n    * Use Google Cloud pipeline components for features like AutoML"
          },
          "Apache Airflow": {
            "content": "Apache Airflow is an open source workflow management platform for data engineering pipelines. It started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows.\n\nAirflow (`https://airflow.apache.org/docs/apache-airflow/stable/concepts/overview.html`) is a platform that lets you build and run workflows. A workflow is represented as a directed acyclic graph (DAG) and contains individual pieces of work called _tasks_ , arranged with dependencies and data flows taken into account. It comes with a UI, a scheduler, and an executor.",
            "subsections": {},
            "summary": "* **Apache Airflow**: open source workflow management platform for data engineering pipelines\n    * Started as Airbnb's solution in 2014 for complex workflows\n        *_Features_*:\n            * Directed acyclic graph (DAG) to represent workflows\n            * Tasks with dependencies and data flows"
          },
          "Cloud Composer": {
            "content": "Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow.\n\nBy using Cloud Composer instead of a local instance of Apache Airflow, you can benefit from the best of Airflow with no installation or management overhead. Cloud Composer helps you create Airflow environments quickly and use Airflow‐native tools, such as the powerful Airflow web interface and command‐line tools, so you can focus on your workflows and not your infrastructure. Cloud Composer is designed to orchestrate data‐driven workflows (particularly ETL/ELT).\n\nCloud Composer is best for batch workloads that can handle a few seconds of latency between task executions. You can use Cloud Composer to orchestrate services in your data pipelines, such as triggering a job in BigQuery or starting a Dataflow pipeline.",
            "subsections": {},
            "summary": "* *_Cloud Composer_* \n  * A fully managed workflow orchestration service built on Apache Airflow\n  * No installation or management overhead, allows for fast environment creation and use of Airflow-native tools"
          },
          "Comparison of Tools": {
            "content": "Table 11.1 compares three orchestrators.\n\n**TABLE 11.1** Kubeflow Pipelines vs. Vertex AI Pipelines vs. Cloud Composer\n\n****|  Kubeflow Pipelines | Vertex AI Pipelines | Cloud Composer\n---|---|---|---\n**Management and support for frameworks** | Kubeflow Pipelines is used to orchestrate ML workflows in any supported framework such as TensorFlow, PyTorch, or MXNet using Kubernetes.\nIt can be set up on‐premises or in any cloud. | Managed serverless pipeline to orchestrate either Kubeflow Pipelines or TFX Pipeline. No need to manage the infrastructure. | Managed way to orchestrate ETL/ELT pipelines using Apache Airflow. It's a Python‐based implementation.\nYou would use a workflow to solve complex data processing workflows in MLOps.\n**Failure handling** | You would need to handle failures on metrics as this is not supported out of the box. | Since Vertex AI pipelines runs the Kubeflow Pipelines, you can use the Kubeflow failure management on metrics. | Failure management for built‐in GCP metrics to take action on failure or success.",
            "subsections": {},
            "summary": "* *_Orchestrator Comparison_* \n    * *Service*: \n        + **Kubeflow Pipelines**: Orchestrate ML workflows with TensorFlow, PyTorch, MXNet using Kubernetes.\n        + **Vertex AI Pipelines**: Managed serverless pipeline for Kubeflow Pipelines or TFX Pipeline, no infrastructure management needed.\n        + **Cloud Composer**: Orchestrates ETL/ELT pipelines using Apache Airflow, Python-based implementation.\n    * *_Failure Handling_* \n        + Kubeflow: Handle failures manually with metrics\n        + Vertex AI Pipelines: Leverage Kubeflow failure management on metrics\n        + Cloud Composer: Built-in GCP metrics for action on failure or success"
          }
        },
        "summary": "* *_ML Pipeline Orchestration_*: A process that manages steps like data cleaning, transformation, and model training to automate the machine learning (ML) workflow.\n  * *_Benefits_*:\n    * **Development Phase**: Automates manual execution of ML experiment steps\n    * **Production Phase**: Automates pipeline execution based on schedule or conditions"
      },
      "Identification of Components, Parameters, Triggers, and Compute Needs": {
        "content": "We covered how to configure triggers and pipeline schedules in Chapter 10, “Scaling Models in Production,” with respect to scheduling training jobs. In this part of the chapter, we will cover how to trigger an MLOps pipeline. You can automate the ML production pipelines to retrain your models with new data. You can trigger your pipeline on demand, on a schedule, on the availability of new data, on model performance degradation, on significant changes in the statistical properties of the data or based on other conditions.\n\nThe availability of new data is a trigger to retrain the ML model:\n\n  * To train a new ML model with new data, the previously deployed continuous training (CT) pipeline is executed. No new pipelines or components are deployed; only a new prediction service or newly trained model is served at the end of the pipeline.\n  * To train a new ML model with a new implementation, a new pipeline is deployed through a CI/CD pipeline.\n\nFigure 11.9 shows the relationship between the CI/CD pipeline and the ML CT pipeline.\n\n**FIGURE 11.9** Continuous training and CI/CD",
        "subsections": {
          "Schedule the Workflows with Kubeflow Pipelines": {
            "content": "Let's look at how CI/CD works for Kubeflow Pipelines. See Figure 11.10.\n\n**FIGURE 11.10** CI/CD with Kubeflow Pipelines\n\nKubeflow Pipelines provides a Python SDK to operate the pipeline programmatically using kfp.Client. By using the Kubeflow Pipelines SDK (`www.kubeflow.org/docs/components/pipelines/v1/sdk/sdk-overview`), you can invoke Kubeflow Pipelines using the following services:\n\n  * On a schedule, you can use Cloud Scheduler.\n  * Responding to an event, you can use Pub/Sub and Cloud Functions. For example, the event can be the availability of new data files in a Cloud Storage bucket.\n  * As part of a bigger data and process workflow, you can use Cloud Composer or Cloud Data Fusion.\n  * Kubeflow Pipelines also provides a built‐in scheduler, Argo, for recurring pipelines in Kubeflow Pipelines.\n  * As an alternative to using Cloud Build, you can use other build systems such as Jenkins. A ready‐to‐go deployment of Jenkins is available on Google Cloud Marketplace.\n  * You can use Apache Airflow, a popular orchestration and scheduling framework, for general‐purpose workflows, which you can run using the fully managed Cloud Composer service. For more information on how to orchestrate data pipelines with Cloud Composer and Cloud Build, refer to `https://cloud.google.com/architecture/cicd-pipeline-for-data-processing`.\n\n* * *\n\nYou can configure the automated Cloud Build pipeline to skip triggers, for example, if only documentation files are edited or if the experimentation notebooks are modified.\n\n* * *",
            "subsections": {},
            "summary": "**CI/CD with Kubeflow Pipelines**\n\n* Use Kubeflow Pipelines SDK to operate pipelines programmatically\n    * Invoke pipelines using services such as Cloud Scheduler, Pub/Sub, and Cloud Functions\n        + Schedule on a timeline or respond to events\n    * Alternative build systems: Jenkins, Apache Airflow\n        + Jenkins available on Google Cloud Marketplace"
          },
          "Schedule Vertex AI Pipelines": {
            "content": "There are two ways to schedule the Vertex AI pipeline:\n\n  * **Schedule pipeline execution with Cloud Scheduler:** You can schedule the execution of a precompiled pipeline using Cloud Scheduler with an event‐driven Cloud Function with an HTTP trigger. To schedule a pipeline with Cloud Scheduler, build and compile a simple pipeline, upload the compiled pipeline JSON to a Cloud Storage bucket, create a Cloud Function with an HTTP trigger, and create a Cloud Scheduler job. You can manually run your job (optional).\n  * **Trigger a pipeline run with Cloud Pub/Sub** : You can schedule the execution of a precompiled pipeline using an event‐driven Cloud Function with a Cloud Pub/Sub trigger. In Cloud Functions, a Pub/Sub trigger enables a function to be called in response to Pub/Sub messages. When you specify a Pub/Sub trigger for a function, you also specify a Pub/Sub topic. Your function will be called whenever a message is published to the specified topic.",
            "subsections": {},
            "summary": "* *Scheduling with Cloud Scheduler*: \n  - upload compiled pipeline JSON to Cloud Storage\n  - create Cloud Function and schedule job\n* *Scheduling with Cloud Pub/Sub*:\n  - specify Pub/Sub trigger in Cloud Function\n  - define Pub/Sub topic for function call"
          }
        },
        "summary": "* _Triggers_ can be set to automate retraining of ML models with new data, on:\n\t+ Availability of new data\n\t+ Model performance degradation\n\t+ Significant changes in statistical properties of the data\n* Triggering a pipeline can result in two possible scenarios:\n\t+ Executing a continuous training (CT) pipeline without deploying new pipelines or components\n\t+ Deploying a new pipeline and serving a newly trained model"
      },
      "System Design with Kubeflow/TFX": {
        "content": "In the following sections, we will discuss system design with the Kubeflow DSL, and then we will cover system design with TFX.",
        "subsections": {
          "System Design with Kubeflow DSL": {
            "content": "Pipelines created in Kubeflow Pipelines are stored as YAML files executed by a program called Argo. Kubeflow Pipelines runs on Argo Workflows by default. Kubeflow exposes a Python domain‐specific language (DSL) for authoring pipelines. The DSL is a Python representation of the operations performed in the ML workflow and built with ML workloads specifically in mind. The DSL also allows for some simple Python functions to be used as pipeline stages without you having to explicitly build a container.\n\nFor each container (with the Kubeflow Python SDK), you need to do the following:\n\n  1. Create the container either as a simple Python function or with any Docker container.\n  2. Create an operation that references that container as well as the command‐line arguments, data mounts, and variables to pass the container.\n  3. Sequence the operations, defining which may happen in parallel and which must complete before moving on to a further step.\n  4. Compile this pipeline, defined in Python, into a YAML file that Kubeflow Pipelines can consume.",
            "subsections": {
              "Kubeflow Pipelines Components": {
                "content": "For a component to be invoked in the pipeline, you need to create a _component op_. You can create a component op using one of the following methods:\n\n  * **Implementing a lightweight Python component:** This component doesn't require that you build a new container image for every code change and is intended for fast iteration in a notebook environment. In a lightweight Python function, we define a Python function and then let Kubeflow take care of packaging that function into a container and creating an operation.\n  * **Creating a reusable component:** This functionality requires that your component includes a component specification in the `component.yaml` file. Component sops are automatically created from the `component.yaml` files using the `ComponentStore.load_components` function in the Kubeflow Pipelines SDK during pipeline compilation.\n  * **Using predefined Google Cloud components:** Kubeflow Pipelines provides predefined components that execute various managed services such as AutoML on Google Cloud by providing the required parameters. These components help you execute tasks using services such as BigQuery, Dataflow, Dataproc, and the AI platform. Similar to using reusable components, these component ops are automatically created from the predefined component specifications through `ComponentStore.load_components`. Other predefined components are available for executing jobs in Kubeflow and other platforms.\n\nFigure 11.11 shows how in Kubeflow Pipelines, a containerized task can invoke other services such as BigQuery jobs, AI platform (distributed) training jobs, and Dataflow jobs. Cloud SQL is used as a metadata store.\n\n**FIGURE 11.11** Kubeflow Pipelines on GCP",
                "subsections": {},
                "summary": "* **Component Ops**: To invoke a component in the pipeline, create a _component op_ using one of three methods:\n    * Lightweight Python component\n    * Reusable component with `component.yaml` file\n    * Predefined Google Cloud components\n* **Auto-Generated Components**: Component specs are automatically created from `component.yaml` files using `ComponentStore.load_components`\n* **Predefined Services**: Kubeflow Pipelines provides managed services like BigQuery, Dataflow, and AI platform through predefined components"
              }
            },
            "summary": "* **Kubeflow Pipelines Overview**\n    * Uses Argo Workflows by default\n    * Exposes a Python domain-specific language (DSL) for authoring pipelines\n* **Creating a Pipeline**\n  * Create container as simple Python function or Docker container\n  * Define operation referencing container, command-line arguments, data mounts, and variables\n  * Sequence operations defining parallelism and dependencies"
          },
          "System Design with TFX": {
            "content": "Before discussing TFX pipelines, let's understand what TFX is. TFX is a Google production‐scale machine learning (ML) platform based on TensorFlow. It provides a configuration framework and shared libraries to make your ML code production‐ready. TFX is a platform for building and managing ML workflows in a production environment. TFX provides the following:\n\n  * TFX pipelines let you orchestrate your ML workflow on several platforms, such as Apache Airflow, Apache Beam, and Kubeflow Pipelines.\n  * TFX provides components that you can use as a part of a pipeline or as a part of your ML training script.\n  * TFX provides libraries, which provide the base functionality for many of the standard components. TFX includes both libraries and pipeline components. We covered TFX libraries such as TFDV, etc., in Chapter 2, “Exploring Data and Building Data Pipelines.” Figure 11.12 illustrates the relationships between TFX libraries and pipeline components.\n\n**FIGURE 11.12** TFX pipelines, libraries, and components\n\nSource: `www.tensorflow.org/tfx/guide#tfx_standard_components`\n\nA TFX pipeline typically includes the following components:\n\n  * ExampleGen is the initial input component of a pipeline that ingests and optionally splits the input dataset.\n  * StatisticsGen calculates statistics for the dataset.\n  * SchemaGen examines the statistics and creates a data schema.\n  * ExampleValidator looks for anomalies and missing values in the dataset.\n  * Transform performs feature engineering on the dataset.\n  * Trainer trains the model.\n  * Tuner tunes the hyperparameters of the model.\n  * Evaluator performs deep analysis of the training results and helps you validate your exported models, ensuring that they are “good enough” to be pushed to production.\n  * Model Validator checks that the model is actually servable from the infrastructure and prevents bad models from being pushed.\n  * Pusher deploys the model on a serving infrastructure.\n  * Model Server performs batch processing on a model with unlabeled inference requests.\n\nIt's important to have an understanding of the previous components for the exam.\n\n* * *\n\nThe TFX ModelValidator component was used to check if a model was good enough to be used in production. Since the component Evaluator has already computed all the metrics you want to validate against, ModelValidator is deprecated and fused with Evaluator so you don't have to duplicate the computations.\n\n* * *\n\nA TFX pipeline is a sequence of components that implement an ML pipeline that is specifically designed for scalable, high‐performance machine learning tasks. Components are built using TFX libraries, which can also be used individually. TFX uses a metadata store to maintain the state of the pipeline run.\n\nYou can orchestrate the TFX pipelines using an orchestration system such as Apache Airflow or Kubeflow.\n\nYou can orchestrate TFX pipelines on GCP by using one of the following methods:\n\n  * Use Kubeflow Pipelines running on GKE.\n  * Use Apache Airflow or Cloud Composer.\n  * Use Vertex AI Pipelines.\n\nIf you are using TFX and using Kubeflow Pipelines to orchestrate TFX stages such as data processing, training, and TFServing, then you can use the TFX Pipeline DSL and use TFX components with Kubeflow. TFX provides a command‐line interface (CLI) that compiles the pipeline's Python code to a YAML file and describes the Argo workflow. Then you can submit the file to Kubeflow Pipelines.",
            "subsections": {},
            "summary": "* **TFX**: A Google production-scale machine learning platform based on TensorFlow, providing a configuration framework and shared libraries for building and managing ML workflows.\n  * Orchestrates ML workflows on several platforms (Apache Airflow, Apache Beam, Kubeflow Pipelines)\n  * Provides components and libraries for pipelines\n* TFX Pipeline Components:\n  * ExampleGen: ingests and splits input dataset\n  * Trainer: trains the model\n  * Evaluator: validates training results and ensures models are \"good enough\"\n* **Orchestration**: TFX pipelines can be run using Apache Airflow, Kubeflow Pipelines, or Cloud Composer."
          }
        },
        "summary": "* **System Design Overview**\n    * Kubeflow DSL: a declarative way to define workflows\n    * TFX: a more structured approach to system design using a pipeline model"
      },
      "Hybrid or Multicloud Strategies": {
        "content": "_Multicloud_ means there is an interconnection between two different cloud providers and describes setups that combine at least two public cloud providers. These are some of the ways you can operate as multicloud with GCP:\n\n  * GCP AI APIs can be integrated anywhere on‐premises or in any application. You can call the pretrained APIs using an AWS Lambda function and take advantage of GCP ML.\n  * BigQuery Omni lets you run BigQuery analytics on data stored in Amazon S3 or Azure Blob Storage. If you want to join the data present in AWS or Azure with the data present in Google Cloud regions or if you want to utilize BigQuery ML capabilities, use the `LOAD DATA` SQL statement. This SQL statement lets you transfer data from BigQuery Omni–accessible S3 buckets into BigQuery native tables.\n  * To train custom ML models on Vertex AI, Vertex AI is integrated with BigQuery and you can access the data from BigQuery Omni (data in S3, Azure) to train an AutoML or custom ML model using Vertex AI Training. You can also use BigQuery ML on the data.\n\n_Hybrid cloud_ means combining a private computing environment, usually an existing on‐premises data center and public cloud computing environment.\n\nAnthos is the hybrid and multicloud cloud modernization platform. The following list includes some of the features of Anthos that support hybrid ML development:\n\n  * BigQuery Omni is powered by Anthos, and you will be able to query data without having to manage the underlying infrastructure.\n  * The hybrid AI offering is Speech‐to‐Text On‐Prem, which is now generally available on Anthos through the Google Cloud Marketplace.\n  * You can run GKE on‐premises, which means you can set up Kubeflow Pipelines for your ML workflow using Anthos.\n  * You can also run the service Cloud Run on‐premises through Anthos, which gives you the ability to set up API‐based pretrained AI services.",
        "subsections": {},
        "summary": "* **Multicloud**: combination of at least two public cloud providers (e.g., GCP, AWS, Azure)\n    * Integrations: GCP AI APIs with AWS Lambda, BigQuery Omni with S3 and Azure Blob Storage\n    * Benefits: seamless data transfer, integration of machine learning capabilities\n* **Hybrid Cloud**: combination of private on-premises environment and public cloud computing environment\n    * Anthos platform provides hybrid ML development features:\n        * Simplified data querying with BigQuery Omni\n        * Hybrid AI offering with Speech-to-Text On-Prem\n        * Integration with GKE, Kubeflow Pipelines, and Cloud Run"
      },
      "Summary": {
        "content": "In this chapter, we covered orchestration for ML pipelines using tools such as Kubeflow, Vertex AI Pipelines, Apache Airflow, and Cloud Composer. We also covered the difference between all these tools and when to use each one for ML workflow automation. You saw that Vertex AI Pipelines is a managed serverless way to run Kubeflow and TFX workflows, while you can run Kubeflow on GCP on Google Kubernetes Engine.\n\nThen we covered ways to schedule ML workflows using Kubeflow and Vertex AI Pipelines. For Kubeflow, you would use Cloud Build to trigger a deployment, while for Vertex AI Pipelines, you can use Cloud Function event triggers to schedule the pipeline. You can also run these pipelines manually.\n\nWe covered system design with Kubeflow and TensorFlow. In Kubeflow Pipelines, you create every task into a component and orchestrate the components. Kubeflow comes with a UI and TensorBoard to visualize these components. You can run TFX pipelines on Kubeflow. For TFX, we covered three TFX components and TFX libraries to define ML pipelines. To orchestrate an ML pipeline, TFX supports bringing your own orchestrator or runtime. You can use Cloud Composer, Kubeflow, or Vertex AI Pipelines to run TFX ML workflows.\n\nFinally, we covered the high‐level definition of _hybrid_ and _multicloud_. You saw how to use BigQuery Omni and Anthos to set up hybrid and multicloud environments on GCP. You can use BigQuery Omni connectors to get data from AWS S3 and Azure storage. You can use Anthos to set up Kubeflow Pipelines on GKE on‐premises.",
        "subsections": {},
        "summary": "* **Orchestration Tools for ML**:\n  * Kubeflow, Vertex AI Pipelines, Apache Airflow, and Cloud Composer are used for ML workflow automation.\n  * Each tool has its own strengths: Vertex AI Pipelines is managed serverless, Kubeflow runs on GKE.\n\n* **Scheduling ML Workflows**: \n  * Kubeflow uses Cloud Build to trigger deployments, while Vertex AI Pipelines uses Cloud Function event triggers.\n  * Pipelines can be run manually or scheduled.\n\n* **System Design with Kubeflow and TensorFlow**:\n  * Kubeflow creates tasks as components and orchestrates them through a UI and TensorBoard.\n  * TFX pipelines can also be run on Kubeflow, supporting custom orchestrators."
      },
      "Exam Essentials": {
        "content": "* **Understand the different orchestration frameworks.** Know what an orchestration framework is and why it's needed. You should know what Kubeflow Pipelines is and how you can run Kubeflow Pipelines on GCP. You should also know Vertex AI Pipelines and how you can run Kubeflow and TFX on Vertex AI Pipelines. Also learn about Apache Airflow and Cloud Composer. Finally, compare and contrast all four orchestration methods for automating ML workflows.\n  * **Identify the components, parameters, triggers, and compute needs on these frameworks.** Know ways to schedule ML workflows using Kubeflow and Vertex AI Pipelines. For Kubeflow, understand how you would use Cloud Build to trigger a deployment. For Vertex AI Pipelines, understand how you can use Cloud Function event triggers to schedule the pipeline.\n  * **Understand the system design of TFX/Kubeflow.** Know system design with Kubeflow and TensorFlow. Understand that in Kubeflow Pipelines, you create every task into a component and orchestrate the components. Understand how you can run TFX pipelines on Kubeflow and how to use TFX components and TFX libraries to define ML pipelines. Understand that to orchestrate ML pipelines using TFX, you can use any runtime or orchestrator such as Kubeflow or Apache Airflow. You can also run TFX on GCP using Vertex AI Pipelines.",
        "subsections": {},
        "summary": "* **Orchestration Frameworks**: \n  * *_Kubeflow Pipelines_*: Automate ML workflows using Kubeflow Pipelines, compatible with GCP and Vertex AI Pipelines.\n  * *_Vertex AI Pipelines_*: Run Kubeflow and TFX on Vertex AI Pipelines for automating ML workflows.\n  * *_Apache Airflow_*: Use Cloud Composer to schedule ML workflows.\n  * *_Cloud Composer_*: Schedule ML workflows using Cloud Composer.\n\n* **Components and Scheduling**:\n  + *_Kubeflow_*: Components include tasks, parameters, triggers, and compute needs. Schedule using Cloud Build or Cloud Function event triggers.\n  + *_Vertex AI Pipelines_*: Schedule using Cloud Function event triggers.\n\n* **System Design of TFX/Kubeflow**: \n  + *_TFX_*: Orchestrate ML pipelines using Kubeflow or Apache Airflow. Run on GCP using Vertex AI Pipelines."
      },
      "Review Questions": {
        "content": "1. You are a data scientist building a TensorFlow model with more than 100 input features, all with values between –1 and 1. You want to serve models that are trained on all available data but track your performance on specific subsets of data before pushing to production. What is the most streamlined and reliable way to perform this validation?\n     1. Use the TFX ModelValidator component to specify performance metrics for production readiness.\n     2. Use the entire dataset and treat the area under the curve receiver operating characteristic (AUC ROC) as the main metric.\n     3. Use L1 regularization to reduce the coefficients of uninformative features to 0.\n     4. Use k‐fold cross‐validation as a validation strategy to ensure that your model is ready for production.\n  2. Your team has developed an ML pipeline using Kubeflow to clean your dataset and save it in a Google Cloud Storage bucket. You created an ML model and want to use the data to refresh your model as soon as new data is available. As part of your CI/CD workflow, you want to automatically run a Kubeflow Pipelines job on GCP. How should you design this workflow with the least effort and in the most managed way?\n     1. Configure a Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a storage bucket. Use a Pub/Sub–triggered Cloud Function to start the Vertex AI Pipelines.\n     2. Use Cloud Scheduler to schedule jobs at a regular interval. For the first step of the job, check the time stamp of objects in your Cloud Storage bucket. If there are no new files since the last run, abort the job.\n     3. Use App Engine to create a lightweight Python client that continuously polls Cloud Storage for new files. As soon as a file arrives, initiate the Kubeflow Pipelines job on GKE.\n     4. Configure your pipeline with Dataflow, which saves the files in Cloud Storage. After the file is saved, you start the job on GKE.\n  3. You created an ML model and want to use the data to refresh your model as soon as new data is available in a Google Cloud Storage bucket. As part of your CI/CD workflow, you want to automatically run a Kubeflow Pipelines training job on GKE. How should you design this workflow with the least effort and in the most managed way?\n     1. Configure a Cloud Storage trigger to send a message to a Pub/Sub topic when a new file is available in a storage bucket. Use a Pub/Sub–triggered Cloud Function to start the training job on GKE.\n     2. Use Cloud Scheduler to schedule jobs at a regular interval. For the first step of the job, check the time stamp of objects in your Cloud Storage bucket to see if there are no new files since the last run.\n     3. Use App Engine to create a lightweight Python client that continuously polls Cloud Storage for new files. As soon as a file arrives, initiate the Kubeflow Pipelines job on GKE.\n     4. Configure your pipeline with Dataflow, which saves the files in Cloud Storage. After the file is saved, you can start the job on GKE.\n  4. You are an ML engineer for a global retail company. You are developing a Kubeflow pipeline on Google Kubernetes Engine for a recommendation system. The first step in the pipeline is to issue a query against BigQuery. You plan to use the results of that query as the input to the next step in your pipeline. Choose two ways you can create this pipeline.\n     1. Use the Google Cloud BigQuery component for Kubeflow Pipelines. Copy that component's URL, and use it to load the component into your pipeline. Use the component to execute queries against a BigQuery table.\n     2. Use the Kubeflow Pipelines domain‐specific language to create a custom component that uses the Python BigQuery client library to execute queries.\n     3. Use the BigQuery console to execute your query and then save the query results into a new BigQuery table.\n     4. Write a Python script that uses the BigQuery API to execute queries against BigQuery. Execute this script as the first step in your pipeline in Kubeflow Pipelines.\n  5. You are a data scientist training a TensorFlow model with graph operations as operations that perform decoding and rounding tasks. You are using TensorFlow data transform to create data transformations and TFServing to serve your data. Your ML architect has asked you to set up MLOps and orchestrate the model serving only if data transformation is complete. Which of the following orchestrators can you choose to orchestrate your ML workflow? (Choose two.)\n     1. Apache Airflow\n     2. Kubeflow\n     3. TFX\n     4. Dataflow\n  6. You are a data scientist working on creating an image classification model on Vertex AI. You are using Kubeflow to automate the current ML workflow. Which of the following options will help you set up the pipeline on Google Cloud with the least amount of effort?\n     1. Set up Kubeflow Pipelines on GKE.\n     2. Use Vertex AI Pipelines to set up Kubeflow ML pipelines.\n     3. Set up Kubeflow Pipelines on an EC2 instance with autoscaling.\n     4. Set up Kubeflow Pipelines using Cloud Run.\n  7. As an ML engineer, you have written unit tests for a Kubeflow pipeline that require custom libraries. You want to automate the execution of unit tests with each new push to your development branch in Cloud Source Repositories. What is the recommended way?\n     1. Write a script that sequentially performs the push to your development branch and executes the unit tests on Cloud Run.\n     2. Create an event‐based Cloud Function when new code is pushed to Cloud Source Repositories to trigger a build.\n     3. Using Cloud Build, set an automated trigger to execute the unit tests when changes are pushed to your development branch.\n     4. Set up a Cloud Logging sink to a Pub/Sub topic that captures interactions with Cloud Source Repositories. Execute the unit tests using a Cloud Function that is triggered when messages are sent to the Pub/Sub topic.\n  8. Your team is building a training pipeline on‐premises. Due to security limitations, they cannot move the data and model to the cloud. What is the recommended way to scale the pipeline?\n     1. Use Anthos to set up Kubeflow Pipelines on GKE on‐premises.\n     2. Use Anthos to set up Cloud Run to trigger training jobs on GKE on‐premises. Orchestrate all of the runs manually.\n     3. Use Anthos to set up Cloud Run on‐premises to create a Vertex AI Pipelines job.\n     4. Use Anthos to set up Cloud Run on‐premises to create a Vertex AI Training job.",
        "subsections": {},
        "summary": "* **Validation in TensorFlow Models**: To validate models trained with more than 100 input features, use _k-fold cross-validation_ as a validation strategy to ensure model readiness for production.\n    * Use **Cloud Storage** triggers and Pub/Sub topics to send notifications when new data is available.\n    * Implement **L1 regularization** to reduce the impact of uninformative features.\n\n* **Automating Kubeflow Pipelines**: To automate Kubeflow Pipelines with the least effort and in a managed way, use _Pub/Sub-triggered Cloud Functions_ or **Cloud Scheduler** to start jobs at regular intervals.\n    * Configure pipelines with **Dataflow**, which saves files in Cloud Storage.\n\n* **Orchestration for ML Workflows**: Choose between **Kubeflow** or **TFX** to orchestrate ML workflows, especially if data transformation is complete and model serving needs to be automated.\n    * Use **Apache Airflow** or **Cloud Build** to automate unit tests with new code pushes.\n\n* **Scaling Training Pipelines**: When security limitations prevent moving data and models to the cloud, use _Anthos_ to set up Kubeflow Pipelines on GKE on-premises for scalability."
      }
    },
    "summary": ""
  },
  "Chapter 12Model Monitoring, Tracking, and Auditing Metadata": {
    "content": "",
    "subsections": {
      "Model Monitoring": {
        "content": "You went through a great journey, from experimentation in a Jupyter Notebook to deploying a model in production, and you are now serving predictions using serverless architecture. Deployment is not the end of this workflow; rather, it is the first iteration of the machine learning model's life cycle.\n\nWhile your model might have scored high in your evaluation metrics, how do you know if it will perform well on real‐time data? What if there are massive changes in the world after you deploy, like a worldwide pandemic that changes human behavior? What if there are subtle changes to the input that are not obvious? In short, how do you know that your model works after deployment? Post‐deployment, the model may not be fit for the original purpose after some time.\n\nThe world is a dynamic place and things keep changing. However, machine learning models are trained on historical data and, ideally, recently collected data. Imagine that a model is deployed and the environment slowly but surely starts to change; your model will become more and more irrelevant as time passes. This concept is called _drift_. There are two kinds of drift: concept drift and data drift.\n\n* * *\n\nThere are two types of drift: concept drift and data drift. Know how to detect the different types of drift and methods to recover.\n\n* * *",
        "subsections": {
          "Concept Drift": {
            "content": "In general, there is a relationship between the input variables and predicted variables that we try to approximate using a machine learning model. When this relationship is not static and changes, it is called _concept drift_. This often happens because the underlying assumptions of your model have changed.\n\nA good example of this is email spam, which makes up the majority of all emails sent. As spam gets detected and filtered, spammers modify the emails to bypass the detection filter. In these cases, adversarial agents try to outdo one another and change their behavior.",
            "subsections": {},
            "summary": "* **Concept Drift**: occurs when the relationship between input variables and predicted variables changes over time\n    * _Causes_: underlying model assumptions are violated\n    * _Example_: email spam filters evolve as spammers adapt their tactics"
          },
          "Data Drift": {
            "content": "_Data drift_ refers to the change in the input data that is fed to the machine learning model compared to the data that was used to train the model. One example would be the changes in the statistical distribution, such as a model trained to predict the food preference of a customer failing if the age of the customer demography changes.\n\nAnother reason for data drift could be the change in the input schema at the source of your data. An example of this is the presence of new product labels (SKUs) not present in training data. A more subtle case is when the meaning of the columns change. For example, the meaning of the term _diabetic_ might change over time based on the medical diagnostic levels of blood sugar.\n\nThe only way to act on model deterioration such as through drift is to keep an eye on the data, called _model monitoring_. The most direct way is to monitor the input data and continuously evaluate the model with the same metrics that were used during the training phase.",
            "subsections": {},
            "summary": "* **Data Drift**: Change in input data distribution or schema over time\n    * *Example:* Model trained on customer food preference fails when demographics change\n    * *Example:* New product labels (SKUs) are added to the dataset, changing the input schema\n* **Model Monitoring**: Continuously evaluate model performance with same metrics used during training phase"
          }
        },
        "summary": "**Model Deployment Challenges**\n\n* Deploying a model in production is just the first step\n* The model's performance may degrade over time due to **_drift_**\n* Drift occurs when the environment changes, making the model less accurate\n\n* Types of Drift:\n  * _Concept Drift_: Changes in problem definition or goals\n  * _Data Drift_: Shifts in distribution of input data"
      },
      "Model Monitoring on Vertex AI": {
        "content": "Vertex AI offers model monitoring features as part of the model deployments. From the practical perspective, a model can be monitored for skew and drift:\n\n  * **Training‐serving skew:** When there is a difference in the input feature distribution in production when compared to training data, it can impact the performance of the model. You can enable this feature if and only if you have access to the original data.\n  * **Prediction drift:** When the input's statistical distribution changes in production over time, it can also impact the performance. If the original training data is not available during prediction, you can use prediction drift detection to monitor your inputs.\n\nThese two features are not exclusive; you can enable both skew and drift detection in Vertex AI. In fact, you will realize that skew and drift need to be examined for each data type, and Vertex AI provides this for categorical and numerical features. Let's look at the data types for which these are available:\n\n  * **Categorical features:** These are features that have a limited number of possible values, typically grouped by qualitative properties. Examples are color, country, and zip code. See Figure 12.1 for an example distribution.\n\n**FIGURE 12.1** Categorical features\n\n  * **Numerical values:** These are features that can take any value. Examples are price ($), speed, and distance. See Figure 12.2 for an example distribution.\n\n**FIGURE 12.2** Numerical values",
        "subsections": {
          "Drift and Skew Calculation": {
            "content": "We will show you how to calculate a baseline distribution and then use the same method to calculate the distribution at prediction time and use a comparison method. These methods will differ by the data type.\n\n  1. Calculate a baseline.\n     * **Baseline for skew detection:** The statistical distribution of the feature's values in the training data is the baseline.\n     * **Baseline for drift detection:** The statistical distribution of the features values seen in the production in the recent past is the baseline.\n\nHow these are applied depends on whether the data type is categorical or numerical. The distributions used for baseline are calculated as follows:\n\n     * **Distribution calculations for categorical features:** The count or percentage of instances of each possible value.\n     * **Distribution calculation for numerical features:** The count or percentage of the values that fall into buckets. (Full range is divided into equal‐sized buckets.)\n  2. Calculate the statistical distribution using the previous method for the latest values seen in production.\n  3. Vertex AI then compares the baseline with the latest distribution calculated in step 2. A distance measure is calculated as follows:\n     * **Categorical features:** The distance score between the baseline distribution and latest production distribution is calculated using L‐infinity distance. This is measured as the largest distance of the two vectors along any coordinate dimension.\n     * **Numerical features:** The distance score is calculated using Jensen‐Shannon divergence. (The mathematical definition of this is beyond the scope of the exam.)\n  4. When the distance score hits a configured threshold value, Vertex AI identifies it as an anomaly (skew or drift).",
            "subsections": {
              "Practical Considerations of Enabling Monitoring": {
                "content": "There are a few things to keep in mind to effectively monitor the data and at the same time make it cost‐effective:\n\n  * **Sampling rate:** Configure a _prediction request sampling rate_ to only get a sample of the production inputs.\n  * **Monitoring frequency:** Configure a _frequency_ to set the rate at which the model's logged inputs are monitored for skew or drift. This sets the monitoring window size of logged data that is analyzed in each run.\n  * **Alerting thresholds:** Configure the threshold for each feature you are interested in. When the statistical distance between the baseline and the production input feature distribution exceeds this threshold, an alert is generated. The default value is 0.3.\n  * **Multiple models in an endpoint:** When you enable and configure skew or drift detection on an endpoint, the configuration will be applied to all the models deployed behind that endpoint, including the following:\n    * Type of detection\n    * Sampling rate\n    * Monitoring frequency",
                "subsections": {},
                "summary": "* To effectively monitor data while keeping costs low:\n  * Configure a _prediction request sampling rate_ to sample production inputs.\n  * Set the **monitoring frequency** to analyze logged input data for skew or drift.\n  * Establish alert thresholds for each feature of interest above a default value of 0.3."
              }
            },
            "summary": "* _Baselines:_ \n    * **Skew detection:** Statistical distribution of training data\n    * **Drift detection:** Statistical distribution of recent production data\n* _Calculations:_ \n    * Categorical features: count or percentage of instances\n    * Numerical features: count or percentage of values in equal-sized buckets\n* _Comparison and Anomaly Detection:_ \n    * Distance measure: L‐infinity distance (categorical) / Jensen‐Shannon divergence (numerical)\n    * Threshold value triggers anomaly detection"
          },
          "Input Schemas": {
            "content": "The input values are part of the payload of the prediction requests. Vertex AI should be able to parse the input values to monitor. You can specify a schema when you configure model monitoring to help parse this input.\n\nThe input schema is automatically parsed for AutoML models, so you don't have to provide one. You must provide one for custom‐trained models that don't use a key/value input format.\n\n* * *\n\nInput values are automatically parsed for AutoML models.\n\n* * *",
            "subsections": {
              "Automatic Schema Parsing": {
                "content": "Model monitoring can usually automatically parse the input schema when enabled for skew or drift. Vertex AI model monitoring will analyze the first 1,000 requests to detect the schema. The efficacy of this automatic detection is best if the input values are in the form of key/value pairs. Here _key_ is the name of the feature and the actual value follows the colon. See the following for an example of an input (of a car) presented as key/value pairs:\n[code]\n     {\"make\":\"Ford\", \"model\":\"focus\": year: \"2011\", \"color\":\"black\"}\n\n[/code]",
                "subsections": {},
                "summary": "* **Model Monitoring Automatic Schema Detection**: Vertex AI model monitoring automatically detects the input schema when enabled for skew or drift.\n* *_Key-Value Pair Format Required_*_: The automatic detection is most effective when input values are in key/value pair format (_e.g._ \"key:value\").\n* Example: {\"make\":\"Ford\", \"model\":\"focus\": year: \"2011\", \"color\":\"black\"}"
              },
              "Custom Schema": {
                "content": "To make sure that Vertex AI correctly parses the input values, you can specify the schema in what is called an _analysis instance_. The analysis instance is expected to be in Open API schema format.\n\nHere are the details of the schema expected:\n\n  * The “type” could be one of the following:\n    * _object_ : key/value pairs\n    * _array_ : array‐like format\n    * _string_ : csv‐string\n  * _Properties_ : the type of each feature.\n  * For _array or csv_ ‐string format, specify the order of the features.\n\n* * *\n\nThere are three types of schema formats: object, array, or string.\n\n* * *\n\nHere is an example of an input to a model that predicts the resale value of a car:\n[code]\n     type: object\n     properties:\n       make:\n         type:string\n       model:\n         type:string\n       year:\n         type:number\n       color:\n         type:string\n       known_defects:\n         type: array\n         items:\n            type: string\n     required:\n     -make\n     -model\n     -year\n     -color\n\n[/code]\n\nThe previous schema is specified in `object` format. The features `make`, `model`, and `color` are string features, and `year` is a number. The feature `known_defects` is an array of strings. The last section tells you that all features are required except for `known_defects`.\n\nIn CSV format you can skip optional input features by providing empty fields. For example, say you are expecting five features: `[\"a\",\"b\",,\"d\",\"e\"]`. Notice that the third feature was skipped. The third missing feature would be understood as a “null” value.",
                "subsections": {},
                "summary": "### Specifying the Schema in an Analysis Instance\n\n* To ensure correct parsing of input values, specify the schema in an _analysis instance_ using Open API schema format.\n* The schema can be one of three formats:\n  * _object_: key/value pairs\n  * _array_: array-like format\n  * _string_: csv-string format\n* For _array or string_ format, specify the order of features.\n\n### Example Schema for Car Resale Value Prediction\n\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"make\": {\"type\": \"string\"},\n    \"model\": {\"type\": \"string\"},\n    \"year\": {\"type\": \"number\"},\n    \"color\": {\"type\": \"string\"},\n    \"known_defects\": [\n      {\"type\": \"string\"}\n    ]\n  },\n  \"required\": [\"make\", \"model\", \"year\", \"color\"]\n}\n```\n\nNote: In CSV format, skip optional input features by providing empty fields."
              }
            },
            "summary": "### Input Values for Model Monitoring\n* *_Input values are part of the prediction payload and should be specified using a schema when configuring model monitoring_*\n    + **AutoML Models**: _Input schema is automatically parsed_\n    + **Custom Models**: _Must provide input schema (key/value format)_"
          }
        },
        "summary": "* **Model Monitoring in Vertex AI**: Provides model monitoring features to detect issues like skew and drift.\n    * _Skew detection_ detects differences between training and production data, impacting model performance.\n    * _Drift detection_ monitors changes in input statistical distribution over time, affecting model performance.\n        + _Categorical features_: Limited values, typically grouped by qualitative properties (e.g., color, country, zip code).\n        + _Numerical values_: Can take any value (e.g., price, speed, distance)."
      },
      "Logging Strategy": {
        "content": "When you deploy a model for prediction, in addition to monitoring the inputs (which is to keep track of the trends in the input features), it may be useful to log the requests. In some domains (such as regulated financial verticals), logging is mandatory for all models for future audits. In other cases, it may be useful to collect monitoring data to update your training data.\n\nIn Vertex AI, you can enable prediction logs for AutoML tabular models, AutoML image models, and custom‐trained models. This can be done during either model deployment or endpoint creation.",
        "subsections": {
          "Types of Prediction Logs": {
            "content": "You can enable three kinds of logs to get information from the prediction nodes. These three types of logs are independent of each other and so can be enabled or disabled independently.",
            "subsections": {
              "Container Logging": {
                "content": "This logs the _stdout_ and _stderr_ from your prediction nodes to Cloud Logging. This is highly useful and relevant for debugging your container or the model. It may be helpful to understand the larger logging platform on GCP.",
                "subsections": {},
                "summary": "* Logs *_stdout_* and *_stderr_* from prediction nodes to Cloud Logging\n    * Helps with debugging container or model\n    * Relevant for understanding the larger logging platform on GCP"
              },
              "Access Logging": {
                "content": "This logs information such as a time stamp and latency for each request to Cloud Logging. This is enabled by default on v1 service endpoints; in addition, you can enable access logging when you deploy your model to the endpoint.",
                "subsections": {},
                "summary": "* **Cloud Logging**: Records timestamp and latency for each request to Cloud Logging\n* _Enabled by default_ on v1 service endpoints\n* *_Optional_*: Enable access logging upon deployment"
              },
              "Request‐Response Logging": {
                "content": "This logs a sample of the online prediction requests and responses to a BigQuery table. This is the primary mechanism. With this you can create more data to augment your training or test data. Either you can enable this during creation of the prediction endpoint or you can update it later.",
                "subsections": {},
                "summary": "* Logs online prediction requests and responses to a BigQuery table\n    * Primary mechanism for creating augmented training or test data\n        *_Can be enabled during endpoint creation or updated later_*"
              }
            },
            "summary": "* *_Three Types of Logs_*:\n  * *_Prediction Node Logs_* \n    _Enable to gather data from prediction node_\n  * *_Feature Value Logs_* \n    _Enable to see feature values for each prediction result_\n  * *_Model Performance Logs_* \n    _Enable to track model performance metrics_"
          },
          "Log Settings": {
            "content": "Log settings can be updated when you create the endpoint or when you deploy a model to the endpoint. It is important to be aware of the default settings for logging. If you have already deployed your model with default log settings but want to change the log settings, you must undeploy your model and redeploy it with new settings.\n\nIf you expect to see a huge number of requests for your model, in addition to scaling your deployment, you should consider the costs of logging. A high rate of “Queries per second,” or QPS, will produce a significant number of logs.\n\nHere is an example of the `gcloud` command to configure logging for an image classification model. Notice the last two lines where the logging configuration is specified.\n[code]\n     gcloud ai endpoints deploy-model _1234567890_ \\\n      --region=_us‐central1_ \\\n      --model=_model_id_12345_ \\\n      --display-name=_image_classification_ \\\n      --machine-type=_a2‐highgpu‐2g_ \\\n      --accelerator=count=2,type=nvidia-tesla-t4 \\\n     **‐‐disable‐container‐logging \\**\n     **‐‐enable‐access‐logging**\n\n[/code]",
            "subsections": {},
            "summary": "* _Logging Settings Can Be Updated When Deploying a Model_\n    * Changes to logging settings require redeployment after undeploying the model\n    * High QPS can produce significant logs, affecting costs\n* *_Configuring Logging with gcloud_* \n    * Disable container logging to reduce costs: `‐‐disable‐container‐logging`\n    * Enable access logging for logs: `‐‐enable‐access‐logging`"
          },
          "Model Monitoring and Logging": {
            "content": "Both model monitoring and request‐response logging use the same infrastructure in the backend (BigQuery table). Therefore, there are some restrictions when these two services are involved:\n\n  * If model monitoring is already enabled on an endpoint, you cannot enable request‐response logging.\n  * If request‐response logging is enabled first, and then model monitoring is enabled, the request‐response logging cannot be modified.",
            "subsections": {},
            "summary": "* **Concurrent Usage Restrictions**\n  * Model monitoring can't be enabled if request-response logging is already on.\n  * Request-response logging can't be modified once model monitoring is enabled after it."
          }
        },
        "summary": "* **Logging in Model Deployment**\n  * Monitoring inputs is crucial\n  * Logging requests may be necessary for regulatory audits or data updates\n  * Available in Vertex AI for various model types"
      },
      "Model and Dataset Lineage": {
        "content": "When you are experimenting with different types of models, you will need a methodical approach to recording the parameters of the experiments and the corresponding observations. The parameters, artifacts, and metrics of an ML experiment are called _metadata_ of the experiment. The metadata helps you to do the following:\n\n  * Detect degradation of the model after deployment.\n  * Compare the effectiveness of different sets of hyperparameters.\n  * Track the lineage of the ML artifacts, both datasets and models. Lineage helps find the source of all the artifacts and also find the descendant artifacts from a given model/dataset.\n  * Rerun an ML workflow with the same parameters.\n  * Track downstream usage of artifacts for audit purposes. This could also be used to understand what was affected by a given artifact.",
        "subsections": {
          "Vertex ML Metadata": {
            "content": "Using Vertex ML Metadata, you can record the metadata of the artifacts and query the metadata for analyzing, debugging, or auditing purposes. Vertex ML Metadata is based on the open source ML Metadata (MLMD) library that was developed by Google's TensorFlow Extended team.\n\nThe Vertex ML Metadata uses the following data model and the associated terminology:\n\n  * **Metadata store:** This is the top‐level container for all the metadata resources. It is regional and within the scope of a Google Cloud project. Usually, one metadata store is shared by the entire organization.\n  * **Metadata resources:** Vertex ML Metadata uses a graph‐like data model for representing the relationship between the resources. These resources are as follows:\n    * **Artifacts:** An artifact is an entity or a piece of data that was created by or can be consumed by an ML workflow. Datasets, models, input files, training logs, and metrics are examples.\n    * **Context:** A context is a group of artifacts and executions that can be queried. Say you are optimizing hyperparameters; each experiment would be a different execution with its own set of parameters and metrics. You can group these experiments into a context and then compare the metrics in this context to identify the best model.\n    * **Execution:** This represents a step in a machine learning workflow and can be annotated with runtime parameters. An example could be a “training” operation, with annotation about time and number of GPUs used.\n    * **Events:** An event connects artifacts and executions. Details like an artifact being the output of an execution and the input of the next execution can be captured using events. It helps you determine origin of artifact when trying to trace lineage. Figure 12.3 shows a simple graph containing events, artifacts, execution, and context.\n\n**FIGURE 12.3** Vertex Metadata data model\n\n    * **Metadataschema:** The metadataschema specifies the schema to be used by the particular types of data like artifact or execution. Type schemas are represented using OpenAPI schema objects in YAML format.",
            "subsections": {
              "Manage ML Metadataschemas": {
                "content": "Every metadata resource that is stored in Vertex ML Metadata follows a schema called `MetaDataSchema`. There are predefined schemas called _system schemas_ for the most common types of resources stored. The system schemas come under the namespace `system`. Here is an example of a predefined model system type in YAML format:\n[code]\n     title: system.**Model**\n\n     type: **object**\n\n     properties:\n      framework:\n         type: **string**\n\n         description: \"The ML framework used, eg: 'TensorFlow' or 'PyTorch'.\"\n      framework_version:\n         type: **string**\n\n         description: \"The version used, eg: '1.16' or '2.5'\"\n      payload_format:\n         type: **string**\n\n         description: \"The format of the model stored: eg: 'SavedModel' or 'pkl'\"\n\n[/code]\n\nLet's look at two of the important operations: create artifacts and lookup artifacts. The other operations (not shown here) are delete artifacts, create executions, and context management. You can use REST, the command line, or Python to do these operations, and we show the examples in Python. Here is how to create an artifact in Python:\n[code]\n     def create_artifact_sample(\n         project: str,\n         location: str,\n         uri: Optional[str] = None,\n         artifact_id: Optional[str] = None,\n         display_name: Optional[str] = None,\n         schema_version: Optional[str] = None,\n         description: Optional[str] = None,\n         metadata: Optional[Dict] = None,\n     ):\n         system_artifact_schema = artifact_schema.Artifact(\n             uri=uri,\n             artifact_id=artifact_id,\n             display_name=display_name,\n             schema_version=schema_version,\n             description=description,\n             metadata=metadata,\n         )\n         return system_artifact_schema.create(project=project, location=location,)\n\n[/code]\n\nIn this example, we are defining a new function that takes in the details like URI, artifact ID, location, display name, schema version, description, and the metadata (properties that define the metadata itself). Inside the function we are calling the artifact`_schema.Artifact` function to create a system artifact instance and in the second step calling the `create()` function to create it in the metadata store.\n\nIn the next example, we see how to query or look up an artifact like dataset or model:\n[code]\n     def list_artifact_sample(\n         project: str,\n         location: str,\n         display_name_fitler: Optional[str] = \"display_name=\\\"my_model_*\\\"\",\n         create_date_filter:  Optional[str] = \"create_time>\\\"2022-06-11\\\"\",\n     ):\n         aiplatform.init(\n             project=project,\n             location=location)\n\n         combined_filters = f\"{display_name_fitler} AND {create_date_filter}\"\n         return aiplatform.Artifact.list(filter=combined_filters)\n\n[/code]\n\nWe use a similar method here, where we define a function called `list_artifact_sample`, which does the job for us. This also has only three lines of code, where the first line is the `init()` function, and the second line creates a `combined_filters` query. In the third line we call the `aiplatform.Artifact.list()` function with the `combined_filters` string as an argument. These code blocks and more information can be found in the documentation at `https://cloud.google.com/vertex‐ai/docs/ml‐metadata/managing‐metadata`.",
                "subsections": {},
                "summary": "* **Vertex ML Metadata Operations**: \n    * Create artifacts using Python or REST/CLI with function parameters: project, location, uri, artifact_id, display_name, schema_version, description, metadata.\n    * Lookup artifacts (e.g., dataset, model) using Python or REST/CLI with filter parameters: display_name_filter and create_date_filter.\n* **Metadata Schemas**: Predefined system schemas for common resources stored under namespace `system`, such as `_Model` that includes framework, version, and payload format properties."
              },
              "Vertex AI Pipelines": {
                "content": "When you use Vertex AI Pipelines, the model metadata and artifacts are automatically stored in the metadata store for _lineage tracking_. Whenever you run a pipeline, it generates a series of artifacts. These could include dataset summaries, model evaluation metrics, metadata on the specific pipeline execution, and so on. Vertex AI Pipelines also provides a visual representation of the lineage, as shown in Figure 12.4. You can use this to understand which data the model was built on and which model version was deployed and also sort it by data schema and date.\n\n**FIGURE 12.4** Vertex AI Pipelines showing lineage",
                "subsections": {},
                "summary": "* **Lineage Tracking**: Model metadata and artifacts are stored in the metadata store for tracking a pipeline's execution.\n* _Automatically generated artifacts_ include dataset summaries, model evaluation metrics, and more.\n* Visual representation shows the pipeline's lineage, including data schema and deployment history."
              }
            },
            "summary": "* **Vertex ML Metadata**: records and queries metadata for analyzing, debugging, or auditing machine learning workflows.\n    * **Data model**: uses a graph-like structure with four main components: artifacts, context, executions, and events.\n        * _**Artifacts**_: data entities created by or consumed by an ML workflow (e.g. datasets, models).\n            * *can be part of a* **context**, which groups related experiments and metrics*\n        * _**Context**_: group of artifacts and executions that can be queried to analyze results.\n    * **Schema**: metadataschema specifies the schema for data types using OpenAPI schema objects in YAML format."
          }
        },
        "summary": "* **Importance of Metadata**: _Metadata_ provides crucial information about machine learning (ML) experiments, including parameters, observations, and artifacts, enabling effective tracking and comparison.\n* *Key benefits include:* \n    - Detecting model degradation after deployment\n    - Comparing hyperparameter effectiveness\n    - Tracking lineage for artifact provenance\n* *Purpose of metadata:* Facilitate reproducibility, audit, and understanding of ML workflows."
      },
      "Vertex AI Experiments": {
        "content": "When you are developing a ML model, the goal is to find the best model for the use case. You could experiment with various different libraries, algorithms, model architectures, hyperparameters, etc. Vertex AI Experiments helps you to keep track of these trials, and analyze the different variations.\n\nIn particular, Vertex AI Experiments helps you to:\n\n  * Track the steps of an experiment run (like preprocessing, embedding, training, etc.)\n  * Track input like algorithms, hyperparameters, datasets, etc.\n  * Track output of these steps like models, metrics, checkpoints, etc.\n\nBased on the above you can understand what works and choose your direction of exploration. The Google Cloud console provides a single pane of glass to view your experiments, where you can slice and dice the results of the experiment runs, and zoom into the results of a single run. Using the Vertex AI SDK for Python you can access experiments, experiment runs, experiment run parameters, metrics, and artifacts. When used with Vertex ML Metadata you can track artifacts and view the lineage.",
        "subsections": {},
        "summary": "* **Vertex AI Experiments**: A tool to manage ML model development, tracking trials and analyzing variations.\n* *Key Features:* \n  * Tracks experiment steps and input/output\n  * Provides a single pane of glass for viewing results in Google Cloud console\n  * Accessible through Vertex AI SDK for Python"
      },
      "Vertex AI Debugging": {
        "content": "Sometimes when training a model, you run into issues and suspect that the GPU is not being used efficiently or a permission issue restricts access to data. To debug these kinds of issues, Vertex AI allows you to directly connect to the container where your training is running. To do this, follow these steps:\n\n  1. Install the interactive Bash shell in the training container. The Bash shell comes installed in pre‐built containers.\n  2. Run the custom training where interactive shells are supported.\n  3. Make sure that the user has the right permissions. If you are using a service account, make sure the service account has the right permissions.\n  4. Set the `enableWebAccess` API field to true to enable interactive shells.\n  5. Navigate to the URI provided by Vertex AI when you initiate the custom training job.\n  6. Use the interactive shell to do the following:\n     1. Check permissions for the service account that Vertex AI uses for training code.\n     2. Visualize Python execution with profiling tools like py‐spy, which you can install using pip3.\n     3. Analyze performance of your training node using perf.\n     4. Check CPU and GPU usage.",
        "subsections": {},
        "summary": "* _Debugging Vertex AI Model Training Issues_\n    * Run custom training in an interactive Bash shell to access the container where training is running\n        * Install Bash shell in training container if not already installed\n        * Enable `enableWebAccess` API field to true for interactive shells\n    * Use interactive shell to investigate issues, including:\n        * Checking permissions for service account used by Vertex AI"
      },
      "Summary": {
        "content": "In this chapter we looked at the steps beyond building a model and deploying it. This includes monitoring a deployed model to detect if there is a performance degradation. Later we also looked at the various different logging strategies available in Vertex AI. Finally, when building several models, we looked at how to track lineage of models using Vertex ML Metadata and track using Vertex AI Experiments.",
        "subsections": {},
        "summary": "* *_Model Operations_*: Monitoring deployed models, tracking model lineage\n* *_Logging Strategies_*: Various options available in Vertex AI\n* *_Model Tracking_*: Using Vertex ML Metadata and Vertex AI Experiments"
      },
      "Exam Essentials": {
        "content": "* **Understand model monitoring.** Understand the need to monitor the performance of the model after deployment. There are two main types of degradation: data drift and concept drift. Learn how to monitor continuously for these kinds of changes to input.\n  * **Learn logging strategies.** Logging after deployment is crucial to be able to keep track of the deployment, including the performance, as well as create new training data. Learn how to use logging in addition to monitoring the models in Vertex AI.\n  * **Understand Vertex ML Metadata.** ML metadata helps you to track lineage of the models and other artifacts. Vertex ML Metadata is a managed solution for storing and accessing metadata on GCP. Learn the data model as well as the basic operations of creating and querying metadata.",
        "subsections": {},
        "summary": "* **Model Monitoring**: Continuously monitor model performance after deployment, focusing on **data drift** and **concept drift**, to detect changes in input data.\n    * Use logging strategies to track model performance and create new training data.\n    * Leverage Vertex ML Metadata for tracking model lineage and artifacts."
      },
      "Review Questions": {
        "content": "1. You spend several months fine‐tuning your model and the model is performing very well in your evaluations based on test data. You have deployed your model, and over time you notice that the model accuracy is low. What happened and what should you do? (Choose two.)\n     1. Nothing happened. There is only a temporary glitch.\n     2. You need to enable monitoring to establish if the input data has drifted from the train/test data.\n     3. Throw away the model and retrain with a higher threshold of accuracy.\n     4. Collect more data from your input stream and use that to create training data, then retrain the model.\n  2. You spend several months fine‐tuning your model and the model is performing very well in your evaluations based on test data. You have deployed your model and it is performing well on real‐time data as well based on an initial assessment. Do you still need to monitor the deployment?\n     1. It is not necessary because it performed very well with test data.\n     2. It is not necessary because it performed well with test data and also on real‐time data on initial assessment.\n     3. Yes. Monitoring the model is necessary no matter how well it might have performed on test data.\n     4. It is not necessary because of cost constraints.\n  3. Which of the following are two types of drift?\n     1. Data drift\n     2. Technical drift\n     3. Slow drift\n     4. Concept drift\n  4. You trained a regression model to predict the longevity of a tree, and one of the input features was the height of the tree. When the model is deployed, you find that the average height of trees you are seeing is two standard deviations away from your input. What type of drift is this?\n     1. Data drift\n     2. Technical drift\n     3. Slow drift\n     4. Concept drift\n  5. You trained a classification model to predict fraudulent transactions and got a high F1 score. When the model was deployed initially, you had good results, but after a year, your model is not catching fraud. What type of drift is this?\n     1. Data drift\n     2. Technical drift\n     3. Slow drift\n     4. Concept drift\n  6. When there is a difference in the input feature distribution between the training data and the data in production, what is this called?\n     1. Distribution drift\n     2. Feature drift\n     3. Training‐serving skew\n     4. Concept drift\n  7. When statistical distribution of the input feature in production data changes over time, what is this called in Vertex AI?\n     1. Distribution drift\n     2. Prediction drift\n     3. Training‐serving skew\n     4. Concept drift\n  8. You trained a classification model to predict the number of plankton in an image of ocean water taken using a microscope to measure the amount of plankton in the ocean. When the model is deployed, you find that the average number of plankton is an order of magnitude away from your training data. Later, you investigate this and find out it is because the magnification of the microscope was different in the training data. What type of drift is this?\n     1. Data drift\n     2. Technical drift\n     3. Slow drift\n     4. Concept drift\n  9. What is needed to detect training‐serving skew? (Choose two.)\n     1. Baseline statistical distribution of input features in training data\n     2. Baseline statistical distribution of input features in production data\n     3. Continuous statistical distribution of features in training data\n     4. Continuous statistical distribution of features in production data\n  10. What is needed to detect prediction drift? (Choose two.)\n     1. Baseline statistical distribution of input features in training data\n     2. Baseline statistical distribution of input features in production data\n     3. Continuous statistical distribution of features in training data\n     4. Continuous statistical distribution of features in production data\n  11. What is the distance score used for categorical features in Vertex AI?\n     1. L‐infinity distance\n     2. Count of the number of times the categorical value occurs over time\n     3. Jensen‐Shannon divergence\n     4. Normalized percentage of the time the categorical values differ\n  12. You deployed a model on an endpoint and enabled monitoring. You want to reduce cost. Which of the following is a valid approach?\n     1. Periodically switch off monitoring to save money.\n     2. Reduce the sampling rate to an appropriate level.\n     3. Reduce the inputs to the model to reduce the monitoring footprint.\n     4. Choose a high threshold so that alerts are not sent too often.\n  13. Which of the following are features of Vertex AI model monitoring? (Choose three.)\n     1. Sampling rate: Configure a prediction request sampling rate.\n     2. Monitoring frequency: Rate at which model's inputs are monitored.\n     3. Choose different distance metrics: Choose one of the many distance scores for each feature.\n     4. Alerting thresholds: Set the threshold at which alerts will be sent.\n  14. Which of the following is _not_ a correct combination of model building and schema parsing in Vertex AI model monitoring?\n     1. AutoML model with automatic schema parsing\n     2. Custom model with automatic schema parsing with values in key/value pairs\n     3. Custom model with automatic schema parsing with values not in key/value pairs\n     4. Custom model with custom schema specified with values not in key/value pairs\n  15. Which of the following is _not_ a valid data type in the model monitoring schema?\n     1. String\n     2. Number\n     3. Array\n     4. Category\n  16. Which of the following is _not_ a valid logging type in Vertex AI?\n     1. Container logging\n     2. Input logging\n     3. Access logging\n     4. Request‐response logging\n  17. How can you get a log of a sample of the prediction requests and responses?\n     1. Container logging\n     2. Input logging\n     3. Access logging\n     4. Request‐response logging\n  18. Which of the following is a _not_ a valid reason for using a metadata store?\n     1. To compare the effectiveness of different sets of hyperparameters\n     2. To track lineage\n     3. To find the right proportion of train and test data\n     4. To track downstream usage of artifacts for audit purposes\n  19. What is an artifact in a metadata store?\n     1. Any piece of information in the metadata store\n     2. The train and test dataset\n     3. Any entity or a piece of data that was created by or can be consumed by an ML workflow\n     4. A step in the ML workflow that can be annotated with runtime parameters\n  20. Which of the following is _not_ part of the data model in a Vertex ML metadata store?\n     1. Artifact\n     2. Workflow step\n     3. Context\n     4. Execution",
        "subsections": {},
        "summary": "* **Concept drift**: Model performance changes over time due to changes in input data distribution, feature values, or concept definition.\n    * It occurs when the relationship between input features and output variables changes, making it necessary to retrain or update the model.\n    * _Example: A regression model trained on tree heights is deployed to predict longevity based on new data with different height distributions._\n\n* **Monitoring**: Continuous evaluation of a deployed model's performance to detect drifts and ensure its accuracy.\n\n    * Monitoring includes metrics like sampling rate, monitoring frequency, distance scores, and alerting thresholds.\n    * _Example: Periodically switching off monitoring or reducing the sampling rate can help reduce costs._"
      }
    },
    "summary": ""
  },
  "Chapter 13Maintaining ML Solutions": {
    "content": "",
    "subsections": {
      "MLOps Maturity": {
        "content": "Organizations go through a journey starting with experimenting with machine learning technology and then progressively bringing the concepts of continuous integration/continuous deployment (CI/CD) into machine learning. This application of DevOps principles to machine learning is called MLOps.\n\nWhile there are similarities between MLOps and DevOps, there are some key differences.\n\nWe have found that organizations first start experimenting with machine learning by manually training models and then bring automation to the process using pipelines; they later enter into a transformational phase as they fully automate. These three phases that they journey through use different technologies and reflect their “AI readiness.”\n\nBefore we look at each of these phases in detail, let's first look at the steps in ML:\n\n  1. Data\n     1. **Extraction:** Collect data from different sources, aggregate the data, and make it available for the ML process downstream.\n     2. **Analysis:** Perform exploratory data analysis (EDA) on the data collected to understand the schema, statistical distributions, and relationships. Identify the feature engineering and data preparation requirements that will have to be performed.\n     3. **Preparation:** Apply transformations and feature engineering on the data. Split the data into train, test, and validation for the ML task.\n  2. Model\n     1. **Training:** Set up ML training using the input data, and predict the output. Experiment with different algorithms and hyperparameters to identify the best‐performing model.\n     2. **Evaluation:** Evaluate the model using a holdout set and assess the quality of the model based on predefined metrics.\n     3. **Validation:** Validate if the model is qualified for deployment, that is, if the metrics meet a certain baseline performance criterion.\n  3. Deployment\n     1. **Serving:** Deploy the model to serve online or batch predictions. For online predictions, create and maintain a RESTful endpoint for the model, and provision to scale based on demand. This could also include making batch predictions.\n     2. **Monitor:** Continuously monitor the model after deployment to detect anomalies, drift, and skew.\n\nWe looked at all of these steps in the previous chapters, and so you should be familiar with them. The level of automation of these steps defines the maturity of the MLOps. Google characterizes MLOps as having three levels: MLOps level 0 (manual phase), MLOps level 1(strategic automation phase), and MLOps level 2 (CI/CD automation, transformational phase).",
        "subsections": {
          "MLOps Level 0: Manual/Tactical Phase": {
            "content": "Organizations that start to experiment with ML are in this phase. In this phase, the focus is to build proof of concepts and test out some AI/ML use cases. The idea is to validate some ideas of how a business can be improved by using ML.\n\nIn this phase, there is usually an individual or a team that is experimenting and training models. The output of their effort is the model, which is handed off to the release or deployment team using a model registry as shown in Figure 13.1. The model is then picked up by the deployment team and will be deployed to serve predictions.\n\n**FIGURE 13.1** Steps in MLOps level 0",
            "subsections": {
              "Key Features of Level 0": {
                "content": "Let's look at the key points of the process in this tactical phase:\n\n  1. **Manual:** There is no automation in any of the steps. Each of the tasks is manually performed, including data analysis, preparation, training, and validation. It is not surprising to see most of these steps done in a Jupyter Notebook of a data scientist.\n  2. **Division between ML and MLOps:** The data scientists and engineers that serve the model are completely separated, and the only point of contact is the model handoff. Even the handoff could be storing the model file in a storage location to be picked up by the deployment team, or a slightly more mature team could use a model registry for the handoff. This division negatively affects the ability to detect training‐serving skew.\n  3. **Manual iterations:** Since training is manual in this phase, there is no regularity in retraining, and so the process doesn’t scale to more than a few models.\n  4. **Continuous integration:** Testing usually happens only as part of the Jupyter Notebooks, but otherwise CI is not even a consideration here.\n  5. **Continuous deployment:** These models are not frequently updated, so CD is not given much thought.\n  6. **Deployment for prediction service:** Typically, the deployment here is a microservice of a model, which is consumed through a REST API, as opposed to deploying an entire ML system.\n  7. **Monitoring:** Typically, this process does not consider monitoring or model degradation.",
                "subsections": {},
                "summary": "* **Tactical Phase Process**\n  * No automation in manual data analysis, preparation, training, and validation\n  * Data scientists and engineers are completely separated, with only a handoff of the model as point of contact\n    * _Lack of collaboration and communication between teams_\n  * Manual iterations, limited to a few models due to lack of regular retraining\n  * No consideration for continuous integration or deployment"
              },
              "Challenges": {
                "content": "The most important challenge in this phase is the model degradation. Well‐trained models frequently underperform in real life due to differences between training data and real data. The only way to mitigate these problems is to actively monitor the quality of predictions, to frequently retrain the models, and to frequently experiment with new algorithms or implementations of the model to leverage improvements in technology.",
                "subsections": {},
                "summary": "* *_Model Degradation_*: well-trained models often underperform in real-life scenarios due to differences between training and real data.\n* To mitigate this, *actively monitor predictions*, *retrain models frequently*, and *experiment with new algorithms*."
              }
            },
            "summary": "* **Phase Description**: Organizations experiment with ML to validate business improvements.\n    * Focus on building proof of concepts and testing AI/ML use cases\n    * Validate ideas for ML adoption in businesses\n* _Key Players_: Individual or team experimenting with training models\n    * Models are handed off to release/deployment teams using a model registry\n    * Models are deployed to serve predictions"
          },
          "MLOps Level 1: Strategic Automation Phase": {
            "content": "Organizations in this phase usually have identified business objectives and have prioritized ML to solve some problems. This phase involves the use of pipelines to automate continuous training of the model, and continuous delivery of the model prediction service. There are new services required to achieve this, including automated data and model validation steps, pipeline triggers, the Feature Store, and metadata management, as shown in Figure 13.2.\n\n**FIGURE 13.2** MLOps Level 1 or strategic phase\n\nIn this phase, there is infrastructure to share artifacts between teams, and there is a clear distinction between development and production environments. The development of ML models happens in an orchestrated environment. The source code for the data pipelines is stored in a central repository and model training is automated.",
            "subsections": {
              "Key Features of MLOps Level 1": {
                "content": "This phase has some distinct features that make it more mature than level 0, and we will list them here:\n\n  1. **Experimentation:** Each step in the experimentation phase is orchestrated. Since the transition between the steps is automated, it increases the speed of iterating through experiments.\n  2. **Continuous training:** The model is trained automatically in production, triggered by new data.\n  3. **Experiment‐operational symmetry:** The code used to create the experimentation pipeline is also used in the production environment, which aligns well with the best practices of unifying MLOps.\n  4. **Modular components:** Each of the components used to create the pipelines should be reusable, composable, and potentially shareable.\n  5. **Continuous delivery:** Since the production pipeline is automated, new models can be built on fresh data and delivered for prediction service.\n  6. **Pipeline deployment:** In level 1, you deploy a pipeline that is executed to create a model.",
                "subsections": {},
                "summary": "* **Level 1 Characteristics**: This phase has distinct features compared to Level 0, including:\n    * *_Automated experimentation_*: Each step in the process is orchestrated for faster iteration.\n    * *_Continuous training_*: The model is trained automatically in production on new data.\n    * *_Modular components_*: Components are reusable, composable, and potentially shareable."
              },
              "Challenges": {
                "content": "In this phase, the expectation is that the team manages only a few pipelines. Also, new pipelines are manually deployed. Pipelines are triggered mainly when there are changes to the data. This is not ideal when you are deploying models based on new ML ideas. For example, this method is good for retraining the model with new features. However, if you want to use technologies that are not part of the existing pipeline, then it has to be manually deployed. To be able to do that, you need to create a CI/CD setup to automate build/test/deploy ML pipelines.",
                "subsections": {},
                "summary": "* *_Pipeline Management_*: Team manages few pipelines, with new ones deployed manually.\n* *_Triggering Pipelines_*: Mainly triggered by changes to data; not ideal for new model deployments.\n* *_CI/CD Setup_*: Required for automation of build, test, and deploy ML pipelines."
              }
            },
            "summary": "* **MLOps Level 1 Phase**: Organized phase with prioritized ML to solve business objectives, using pipelines for continuous training and delivery\n    * **Key Services**:\n        * Automated data and model validation steps\n        * Pipeline triggers\n        * Feature Store\n        * Metadata management\n    * _Characteristics_: Shared infrastructure, clear dev/prod separation, automated source code management"
          },
          "MLOps Level 2: CI/CD Automation, Transformational Phase": {
            "content": "This is the level of maturing expected of an organization that is in the transformational phase, where it uses AI to innovate and provide agility. Typically, you can expect ML experts to be part of the product teams and across various business units. Datasets are usually accessible across the silos, and ML projects shared between product groups. As shown in Figure 13.3, the entire pipeline goes through the CI/CD automation (not just the model). So, if you want to update your library from TensorFlow 2.1 to TensorFlow 2.5, it will go through the CI/CD automation steps naturally and the process doesn't have to be done manually.\n\n**FIGURE 13.3** MLOps level 2, the transformational phase",
            "subsections": {
              "Key Features of Level 2": {
                "content": "The striking feature of moving to level 2 is the complete adoption of the CI/CD model. Let's look at some of the features in detail:\n\n  1. **Pipeline:** The first stage is the development and experimentation of new ML algorithms, possibly using new libraries. The output is the source code for the pipeline that is shared using a code repository. From that point on, automation takes over, from building pipeline code, running various tests on the pipeline code, creating artifacts for the pipeline code (like packages, executables, etc.), later deploying this pipeline, and producing the model artifacts. The trained model is automatically deployed for continuous delivery. After deployment monitoring is enabled, the monitored data is collected to create more training data and training is automatically triggered.\n  2. **Testing:** Testing is a must for continuous integration of the pipeline, starting from testing the feature engineering logic, model training convergence checks, checking errors in model output (like NaN values), validating the artifacts created at each stage, and integration between each of the pipeline components.\n  3. **Delivery:** To be able to provide continuous delivery, you need to verify the compatibility of the model with the software and hardware infrastructure in the serving environment. Test the prediction service for conforming to the API schema, load test, and so on. In addition, based on the need for a particular model, you could also use blue/green deployment, canary deployment, or a shadow deployment policy before rolling out completely.\n\nTo summarize, maintaining an ML solution is not just training a model and deploying a single model behind an API. It means the ability to automate the retraining, testing, and deployment of the models. Since the field of machine learning is seeing rapid improvements in technology almost on a daily basis, it is important to have the ability to manage this change.\n\nHowever, not every organization has to move to level 2 at the start of its journey. It depends on the level of maturity of using AI/ML, and as the organization thinking matures, the automation follows.\n\nIn the previous chapters we talked extensively about retraining models and even continuous training and deployment methodology. There are some common scenarios and practical problems that happen in that journey, and we will address two of them as design patterns in the next section.",
                "subsections": {},
                "summary": "* **Level 2 Adoption:** Adopting CI/CD model for ML pipeline automation\n    * Automates retraining, testing, and deployment of models\n    * Enables continuous delivery and monitoring\n    * Facilitates adaptability to rapid changes in ML technology"
              }
            },
            "summary": "* _Transformational Phase:_\n\t+ AI-driven innovation with agility\n\t+ ML experts in product teams and business units\n\t+ Datasets accessible across silos\n\t+ CI/CD automation for model updates and pipelines"
          }
        },
        "summary": "* _MLOps Journey_\n  * **Phase 1:** Manual Training to Automation (*MLOps Level 0*)\n  * **Phase 2:** Strategic Automation with Pipelines (*MLOps Level 1*)\n  * **Phase 3:** Full Automation and Transformation (*MLOps Level 2*)"
      },
      "Retraining and Versioning Models": {
        "content": "You have deployed a model and are monitoring the predictions for drift. Now, we know that model performance can degrade over time, which can be measured from the monitoring data. However, at what point do you retrain the model? This is not just a question out of curiosity, but one that has to be answered to configure the continuous training pipeline.\n\nVertex AI Model Monitoring offers you the ability to monitor for drift detection and training‐serving skew. As you monitor the performance, it is also important to collect this real data. This will be used for evaluation of the model as well as for creation of new training datasets. Now, when do we use this training dataset to train a new model? Or what is our retraining frequency?\n\n* * *\n\nWhen you enable monitoring, have measures to collect the data for the creation of fresh, new training and test data.\n\n* * *",
        "subsections": {
          "Triggers for Retraining": {
            "content": "Model performance usually degrades over time. To fix this we need to retrain the model. The idea is to trigger a retrain pipeline based on a policy. Here are a couple of available policies:\n\n  * **Absolute threshold:** You can set an absolute threshold for retraining. For example, whenever accuracy falls below 90 percent, you can trigger. Determining what this absolute threshold is would depend on your use case.\n  * **Rate of degradation:** You can trigger retraining whenever there is a sudden dip in performance. For example, if you see a drop of more than 2 percent in accuracy in a day, you can trigger a retraining.\n\nNow considering the preceding policies, there are a few main points to think about:\n\n  * **Training costs:** There is a cost to training models. Frequently training models could be expensive. So it is important to make sure the policy does not frequently trigger retraining.\n  * **Training time:** Some models train in a few minutes, but others take weeks to train. The deployed model could degrade further when waiting for the newly trained model.\n  * **Delayed training:** If the threshold is too low, the model could degrade too much before the new model is deployed. This not only affects the performance of the ML service, it also shows inconsistency in the ML performance, which may not be tolerated by the end user.\n  * **Scheduled training:** A much simpler policy could be a retraining on a fixed schedule. This would incorporate new data on a regular basis and the costs are predictable.\n\n* * *\n\nRetraining policy should take into account the cost of retraining, the time to retrain, and also the degradation in performance.\n\n* * *",
            "subsections": {},
            "summary": "### Retraining Model Policies\n\n* **Absolute Threshold**: Trigger retraining when accuracy falls below a set threshold (e.g. 90%).\n* _Rate of Degradation_: Trigger retraining with sudden dips in performance (e.g. >2% drop in accuracy per day).\n\n### Considerations for Retraining Policy\n* * Training Costs: Minimize frequent retuning to reduce costs *\n* * Training Time: Balance wait time vs degraded performance *\n* * Scheduled Training: Simple, predictable approach incorporating new data on a regular basis"
          },
          "Versioning Models": {
            "content": "When you are building multiple models and sharing them with other teams, it is important to have some way to find them. One method is to use metadata to identify artifacts; however, users external to the organization access a deployed model using an API, and to them the metadata store might not be accessible. This is where versioning can be used.\n\nThe problem with having multiple models is that, when users are accessing through an API, they expect a certain performance and behavior from the model. If there is a sudden change in behavior from the same API, it causes unexpected disruption. However, there is a need for the model to update or change. For example, say there is a model to identify objects and humans in an image and it accurately detects all the human faces. Later, say you have updated the model and it now has the ability to also detect dogs and other pets; this could cause a disruption. In this case, the user needs the ability to choose the older model.\n\nThe solution is to use model versioning. Model versioning allows you to deploy an additional model to the existing model, with the ability to select it using a version ID. In this case, both the models can be accessed by the end user by specifying the version. This solves the problem of backward compatibility; that is, this works well for models that have the same inputs and outputs.\n\nFor models that behave in a different way (say a model has the ability to provide model explanations), they can be deployed as a new model (and not a new version of an existing model).\n\nIn both cases, there should be the ability to access these models using REST endpoints, separately and conveniently for the users. Enabling monitoring on all the deployed model versions allows you the ability to compare the different versions.",
            "subsections": {},
            "summary": "* **Model Versioning**: allows multiple models to coexist with the same API, enabling users to choose between different versions of a model\n    * _Solves backward compatibility issues_\n    * _Allows for deployment of new models as separate entities, not just updates to existing ones_\n    * _Provides monitoring and comparison capabilities across deployed model versions_"
          }
        },
        "summary": "### **Model Drift Detection and Retraining**\n\n* *_Monitoring_*: Track model performance over time using Vertex AI Model Monitoring.\n* *_Drift detection_*: Identify when model performance degrades and retrain is necessary.\n* *_Retraining frequency_*: Determine optimal interval for retraining based on evaluation of collected real data."
      },
      "Feature Store": {
        "content": "Feature engineering is an important factor in the ability to build good ML models. However, it has been observed that, practically, feature engineering is more time‐consuming than experimenting with ML models. For this reason, a valuable feature that has been engineered provides a huge value for the entire ML solution.\n\nSo, feature engineering is a huge investment for any organization. In order to optimize their models, many teams work on creating new features. At times these features would have been valuable to other teams as well. Unfortunately, sharing these features is tricky and so the same feature engineering tasks are done over and over again. This creates several problems:\n\n  * **Non‐reusable:** Features are created ad hoc and not intended to be used by others. Each team creates a feature with the purpose of only using it themselves. These ad hoc features are not automated in pipelines and are sometimes derived from other expensive data preprocessing pipelines.\n  * **Governance:** Diversity of sources of the methods of creating these features creates a very complex situation for data governance.\n  * **Cross‐collaboration:** Due to the ad hoc nature of these features not being shared, more divisions between teams are created and they continue to go separate ways.\n  * **Training and serving differences:** When features are built ad hoc and not automated, this creates differences between training data and serving data and reduces the effectiveness of the ML solution.\n  * **Productizing features:** While these ad hoc features are useful during experimentation, they cannot be productized because of the lack of automation and the need for low‐latency retrieval of the features.",
        "subsections": {
          "Solution": {
            "content": "The solution is to have a central location to store the features as well as the metadata about the features that can be shared between the data engineers and ML engineers. This also allows the application of the software engineering principles of versioning, documentation, and access control of these features.\n\nFeature stores also has two key features: the ability to process large feature sets quickly and the ability to access the features with low latency for real‐time prediction and batch access for training time and batch predictions.\n\nFeast is an open‐source Feature Store created by Google and Gojek that is available as a software download. Feast was designed with Redis and Google Cloud services BigQuery and Apache Beam. Google Cloud also offers a managed service called Vertex AI Feature Store that allows you to scale dynamically based on your needs.",
            "subsections": {},
            "summary": "* *_Feature Store_*: Central location for storing features and metadata, enabling data sharing between teams and applying software engineering principles\n    * Enables fast processing of large feature sets and low-latency access for real-time predictions and batch access\n* **Feast**: Open-source Feature Store created by Google and Gojek, built with Redis and Google Cloud services"
          },
          "Data Model": {
            "content": "We will now discuss the data model used by the Vertex AI Feature Store service. It uses a time‐series model to store all the data, which enables you to manage the data as it changes over time. All the data in Vertex AI Feature Store is arranged in a hierarchy with the top level called a featurestore. This featurestore is a container that can have one or more entity types, which represents a certain type of feature. In your entity type you can store similar or related features.\n\n  * Featurestore → EntityType → Feature\n\nAs an example, Table 13.1 has data of baseball batters. The first step is to create a featurestore called baseballfs.\n\n**TABLE 13.1** Table of baseball batters\n\nRow | player_id | Team | Batting_avg | Age\n---|---|---|---|---\n1 | player_1 | RedSox | 0.289 | 29\n2 | player_2 | Giants | 0.301 | 32\n3 | player_3 | Yankees | 0.241 | 35\n\nYou can then create an EntityType called batters and map the column header player_id to that entity. You can then add team, batting_avg, and age as features to this EntityType.\n\nHere player_1 and player_2 are entities in this EntityType. Entities must always be unique and must always be of type String.\n\nRedSox, Giants, and Yankees are featurevalues in the featurestore.",
            "subsections": {},
            "summary": "* **Vertex AI Feature Store Data Model**\n    * Uses a time-series model to store data, enabling dynamic management\n        * **Data Hierarchy**: Featurestore > EntityType > Feature\n    * **Entity Types**: Container for similar or related features"
          },
          "Ingestion and Serving": {
            "content": "Vertex AI Feature Store supports both batch and streaming ingestion. A typical method to do this is to have the feature stored in BigQuery and then ingested into the Feature Store.\n\nFor retrieving features from the Feature Store, there are two methods: batch and online. The batch method is used for the model training phase and the online method is used for online inference. When you request data from time t, the Vertex AI Feature Store returns values at or before time t.",
            "subsections": {},
            "summary": "* **Feature Ingestion**: Vertex AI Feature Store supports both batch and streaming ingestion from BigQuery\n* **Data Retrieval Methods**:\n  * Batch: Used for model training phase\n  * Online: Used for online inference, returning data up to a specified time t"
          }
        },
        "summary": "* Feature engineering is a valuable investment in building good ML models\n    * Can lead to non-reusable, ad-hoc features that create problems across teams\n        + Lack of governance and sharing\n        + Division between teams\n    * Leads to training and serving differences, reducing ML solution effectiveness"
      },
      "Vertex AI Permissions Model": {
        "content": "It is essential to manage access to various different resources like datasets, models, and so on and also to provide permission to perform various operations like training, deploying, and monitoring. Identity and Access Management (IAM) is the mechanism to enforce that. Refer to Chapter 6, “Building Secure ML Pipelines,” for a discussion of IAM.\n\n* * *\n\nRevisit GCP's IAM fundamentals before diving into the Vertex AI permissions model.\n\n* * *\n\nYou may already have experience with GCP's IAM model—for example, in Compute Engine or Google Cloud Storage—which will come in handy.\n\nIt is important to emphasize following best practices to use IAM security. The following are some of the high‐level points:\n\n  * **Least privilege:** Restrict the users and applications to do only what is necessary.\n  * **Manage service accounts and service account keys:** Actively manage these security assets and periodically check them.\n  * **Auditing:** Enable audit logs and use cloud logging roles.\n  * **Policy management:** Use a higher‐level check to make sure the policies are being implemented at every level.\n\nFor more details on IAM best practices, refer to this URL:\n\n`https://cloud.google.com/blog/products/identity-security/iam-best-practice-guides-available-now`\n\nWe looked at general IAM best practices, and now we will look at some special cases for Vertex AI.",
        "subsections": {
          "Custom Service Account": {
            "content": "When you run a Vertex AI training job, it uses one of several _service accounts_ that Google automatically creates for your Google Cloud Project. However, these service accounts might have more permissions than required, like access to BigQuery and GCS. It is better to use custom service accounts with just the right set of permissions.",
            "subsections": {},
            "summary": "* Use custom service accounts instead of default ones to avoid unnecessary permissions\n    * Custom service accounts provide exactly the required permissions for your Vertex AI training job\n    * This ensures security and reduces potential risks"
          },
          "Access Transparency in Vertex AI": {
            "content": "To verify who is accessing your content, and what is being accessed, you need logs. In many domains, there are legal and compliance requirements for such logging. Importantly, it also provides you with access logs that capture the actions of Google personnel in your project. There are two types of access logs. _Cloud Audit logs_ are logs of users from your organization. _Access Transparency logs_ are logs of Google personnel. Most services are supported in this, but there are some features that are not covered.\n\nFor a full list of services supported, refer to this URL:\n\n`https://cloud.google.com/vertex-ai/docs/general/access-transparency`",
            "subsections": {},
            "summary": "* **Cloud Audit Logs**: Capture user activity from your organization\n* _Access Transparency Logs_: Capture actions of Google personnel in your project\n* Note: [Check the supported services](https://cloud.google.com/vertex-ai/docs/general/access-transparency) for a full list."
          }
        },
        "summary": "* **Understanding IAM**: Identity and Access Management (IAM) is crucial for managing access to resources like datasets, models, and enabling operations like training, deploying, and monitoring.\n* * *\n* **GCP IAM Fundamentals**: Revisit GCP's IAM fundamentals before diving into the Vertex AI permissions model. Existing experience with GCP's IAM model will be helpful.\n* **Best Practices**: Follow these best practices for using IAM security:\n  * _Least Privilege_: Restrict users and applications to only necessary actions\n  * _Service Account Management_: Actively manage service accounts and keys, and enable audit logs\n  * _Policy Management_: Implement policies at every level to ensure consistency"
      },
      "Common Training and Serving Errors": {
        "content": "In the following sections, we will look at some of the errors you frequently see during training and serving. Knowledge of the types of errors will help you debug problems effectively. While the errors are dependent on the framework used, we will use TensorFlow as the framework in this discussion.",
        "subsections": {
          "Training Time Errors": {
            "content": "During the training phase, the most relevant errors are seen when you run `Model.fit()`.Errors happen when the following scenarios occur:\n\n  1. Input data is not transformed or not encoded.\n  2. Tensor shape is mismatched.\n  3. Out of memory errors occur because of instance size (CPU and GPU).",
            "subsections": {},
            "summary": "* *_Common Errors During Model Training_*\n  * *_Preprocessing Issues_*: *Input data not transformed or encoded.*\n  * *_Tensor Shape Mismatch_*\n  * *_Out-of-Memory Errors_*: Due to large instance sizes on CPU/GPU."
          },
          "Serving Time Errors": {
            "content": "The serving time errors are seen only during deployment and the nature of the errors is also different. The typical errors are as follows:\n\n  1. Input data is not transformed or not encoded.\n  2. Signature mismatch has occurred.\n\nRefer to this URL for a full list of TensorFlow errors:\n\n`www.tensorflow.org/api_docs/python/tf/errors`",
            "subsections": {},
            "summary": "* _TensorFlow Serving Errors_ \n    * Occur only during deployment\n    * Different nature compared to regular TF errors\n    * Examples:\n        * Input data not transformed or encoded (*not transformed*)\n        * Signature mismatch occurred (*)"
          },
          "TensorFlow Data Validation": {
            "content": "To prevent and reduce these errors, you can use TensorFlow Data Validation (TFDV). TFDV can analyze training and serving data as follows:\n\n  1. To compute statistics\n  2. To infer schema\n  3. To detect anomalies\n\nRefer here for full documentation:\n\n`https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell`",
            "subsections": {},
            "summary": "* **TensorFlow Data Validation (TFDV)**: Analyzes training and serving data to prevent and reduce errors.\n    * _Detects anomalies_ in data\n    * _Infer schema_ from data to ensure correct structure\n    * _Computes statistics_ on data for quality control"
          },
          "Vertex AI Debugging Shell": {
            "content": "Vertex AI provides an interactive shell to debug training for both pre‐built containers and custom containers. You can use an interactive shell to inspect the training container to debug problems in your training code or the Vertex AI configuration. Using that you can do the following:\n\n  1. Run tracing and profiling tools.\n  2. Analyze GPU utilization.\n  3. Validate IAM permissions for the container.\n\nRefer to this URL for full documentation:\n\n`https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell`",
            "subsections": {},
            "summary": "* _Vertex AI provides an interactive shell to debug training_\n    * **Debugging Tools:** Run tracing and profiling tools, Analyze GPU utilization\n    * *_Validate IAM permissions_* for the container."
          }
        },
        "summary": "* _Common Errors in TensorFlow Training and Serving_\n* **Types of Errors**:\n  * _Data-related errors_ \n  * _Model-related errors_ \n  * _Framework-related errors_"
      },
      "Summary": {
        "content": "In this chapter we looked at the long‐term maintenance of a ML application. ML operations or MLOps is based on CI/CD principles of maintaining software applications. During this process we look at how to automate training, deployment, and monitoring. Retraining policy of models is an important concept that has to be balanced between model quality and cost of training. Another important problem that arises in large enterprises is the inability to share features between departments, which causes lots of inefficiencies. To solve this problem the idea of feature store was invented, which can be implemented either using open source software or Vertex AI Feature Store.",
        "subsections": {},
        "summary": "* **_MLOps Overview_**\n  * Automates training, deployment, and monitoring of ML applications\n  * Balances model quality with retraining policy and cost considerations\n\n* *_Feature Store Concept_*\n  * Solves inefficiencies caused by feature sharing across departments\n  * Implemented using open source software or Vertex AI Feature Store"
      },
      "Exam Essentials": {
        "content": "* **Understand MLOps maturity.** Learn different levels of maturity of MLOps and how it matches with the organizational goals. Know the MLOps architecture at the experimental phase, then a strategic phase where there is some automation, and finally a fully mature CI/CD‐inspired MLOps architecture.\n  * **Understand model versioning and retraining triggers.** A common problem faced in MLOps is knowing when to trigger new training. It could be based on model degradation as observed in model monitoring, or it could be time‐based. When retraining a model, learn how to add it as a new version or a new model.\n  * **Understand the use of feature store.** Feature engineering is an expensive operation, so the features generated using those methods are more useful if shared between teams. Vertex AI Feature Store is a managed service, and Feast is an open source feature store by Google.",
        "subsections": {},
        "summary": "* _MLOps Maturity Levels_\n  * Experimental phase: basic automation\n  * Strategic phase: some automation\n  * Mature phase: CI/CD-inspired architecture\n* **Model Versioning and Retraining Triggers**\n  * Trigger retraining based on model degradation or time intervals\n  * Add new version or model when retraining\n* _Feature Store Use_\n  * Share features across teams to reduce engineering cost\n  * Use managed services like Vertex AI Feature Store or open-source solutions like Feast"
      },
      "Review Questions": {
        "content": "1. Which of the following is _not_ one of the major steps in the MLOps workflow?\n     1. Data processing, including extraction, analysis, and preparation\n     2. Integration with third‐party software and identifying further use cases for similar models\n     3. Model training, testing, and validation\n     4. Deployment of the model, monitoring, and triggering retraining\n  2. You are on a small ML team in a very old retail organization, and the organization is looking to start exploring machine learning for predicting daily sales of products. What level of MLOps would you implement in this situation?\n     1. No MLOps, will build ML models ad hoc\n     2. MLOps level 0\n     3. MLOps level 1\n     4. MLOps level 2\n  3. You are a data scientist working as part of an ML team that has experimented with ML for its online fashion retail store. The models you build match customers to the right size/fit of clothes. Organization has decided to build this out, and you are leading this effort. What is the level of MLOps you would implement here?\n     1. No MLOps, will build ML models ad hoc\n     2. MLOps level 0\n     3. MLOps level 1\n     4. MLOps level 2\n  4. You have been hired as an ML engineer to work in a large organization that works on processing photos and images. The team creates models to identify objects in photos, faces in photos, and the orientation of photos (to automatically turn) and also models to adjust the colors of photos. The organization is also experimenting with new algorithms that can automatically create images from text. What is the level of MLOps you would recommend?\n     1. No MLOps, ad hoc because they are using new algorithms\n     2. MLOps level 0\n     3. MLOps level 1\n     4. MLOps level 2\n  5. What problems does MLOps level 0 solve?\n     1. It is ad hoc building of models so it does not solve any problems.\n     2. It automates training so building models is a repeatable process.\n     3. Model training is manual but deployment is automated once there is model handoff.\n     4. It is complete automation from data to deployment.\n  6. Which of these statements is **false** regarding MLOps level 1 (strategic phase)?\n     1. Building models becomes a repeatable process due to training automation.\n     2. Model training is triggered automatically by new data.\n     3. Trained models are automatically packaged and deployed.\n     4. The pipeline is automated to handle new libraries and algorithms.\n  7. You are part of an ML engineering team of a large organization that has started using ML extensively across multiple products. It is experimenting with different algorithms and even creating its own new ML algorithms. What should be its MLOps maturity level to be able to scale?\n     1. Ad hoc is the only level that works for the organization because it is using custom algorithms.\n     2. MLOps level 0.\n     3. MLOps level 1.\n     4. MLOps level 2.\n  8. In MLOps level 1 of maturity (strategic phase), what is handed off to deployment?\n     1. The model file\n     2. The container containing the model\n     3. The pipeline to train a model\n     4. The TensorFlow or ML framework libraries\n  9. In MLOps level 0 of maturity (tactical phase) what is handed off to the deployment?\n     1. The model file\n     2. The container containing the model\n     3. The pipeline to train a model\n     4. The TensorFlow or ML framework libraries\n  10. What triggers building a new model in MLOps level 2?\n     1. Feature store\n     2. Random trigger\n     3. Performance degradation from monitoring\n     4. ML Metadata Store\n  11. What should you consider when you are setting the trigger for retraining a model? (Choose two.)\n     1. The algorithm\n     2. The frequency of triggering retrains\n     3. Cost of retraining\n     4. Time to access data\n  12. What are reasonable policies to apply for triggering retraining from a model monitoring data? (Choose two.)\n     1. The amount of prediction requests to a model\n     2. Model performance degradation below a threshold\n     3. Security breach\n     4. Sudden drop in performance of the model\n  13. When you train or retrain a model, when do you deploy a new version (as opposed to deploy as a new model)?\n     1. Every time you train a model, it is deployed as a new version.\n     2. Only models that have been uptrained from pretrained models get a new version.\n     3. Never create a new version, always a new model.\n     4. Whenever the model has similar inputs and outputs and is used for the same purpose.\n  14. Which of the following are good reasons to use a feature store? (Choose two.)\n     1. There are many features for a model.\n     2. There are many engineered features that have not been shared between teams.\n     3. The features created by the data teams are not available during serving time, and this is creating training/serving differences.\n     4. The models are built on a variety of features, including categorical variables and continuous variables.\n  15. Which service does Feast _not_ use?\n     1. BigQuery\n     2. Redis\n     3. Gojek\n     4. Apache Beam\n  16. What is the hierarchy of the Vertex AI Feature Store data model?\n     1. Featurestore ‐ > EntityType ‐> Feature\n     2. Featurestore ‐ > Entity ‐> Feature\n     3. Featurestore ‐ > Feature ‐> FeatureValue\n     4. Featurestore ‐> Entity ‐ > FeatureValue\n  17. What is the highest level in the hierarchy of the data model of a Vertex AI Feature Store called?\n     1. Featurestore\n     2. Entity\n     3. Feature\n     4. EntityType\n  18. You are working in a small organization and dealing with structured data, and you have worked on creating multiple high‐value features. Now you want to use these features for machine learning training and make these features available for real‐time serving as well. You are given only a day to implement a good solution for this and then move on to a different project. Which options work best for you?\n     1. Store the features in BigQuery and retrieve using the BigQuery Python client.\n     2. Create a Feature Store from scratch using BigQuery, Redis, and Apache Beam.\n     3. Download and install open‐source Feast.\n     4. Use Vertex AI Feature Store.\n  19. Which of these statements is false?\n     1. Vertex AI Feature Store can ingest from BigQuery.\n     2. Vertex AI Feature Store can ingest from Google Cloud Storage.\n     3. Vertex AI Feature Store can even store images.\n     4. Vertex AI Feature Store serves features with low latency.\n  20. Which of these statements is true?\n     1. Vertex AI Feature Store uses a time‐series model to store all data.\n     2. Vertex AI Feature Store cannot ingest from Google Cloud Storage.\n     3. Vertex AI Feature Store can even store images.\n     4. Vertex AI Feature Store cannot serve features with low latency.",
        "subsections": {},
        "summary": "* **MLOps is not about ad hoc building of models**: * MLOps level 0 is ad hoc, where models are built without automation or standardization. \n    * It automates training so building models is a repeatable process.\n    * Model training is manual but deployment is automated once there is model handoff.\n    * It is complete automation from data to deployment.\n\n* **Level of MLOps for predicting daily sales**: \n    * _No MLOps_, will build ML models ad hoc\n    * MLOps level 0\n\n* **Level of MLOps for size/fit matching**: \n    * No MLOps, will build ML models ad hoc\n    * MLOps level 1\n\n* **Level of MLOps for objects in photos and image adjustment**: \n    * No MLOps, ad hoc because they are using new algorithms\n    * MLOps level 2\n\n* **MLOps level 0 problems**: \n    * It is ad hoc building of models so it does not solve any problems.\n\n* **False statement about MLOps level 1 (strategic phase)**: \n    * Model training is triggered automatically by new data.\n    * The pipeline is automated to handle new libraries and algorithms.\n\n* **MLOps maturity level for custom algorithms**: \n    * _Ad hoc_ is the only level that works for the organization because it is using custom algorithms\n    * MLOps level 2\n\n* **Triggered handoff in MLOps level 1**: \n    * The model file\n\n* **Triggered handoff in MLOps level 0**: \n    * The container containing the model\n\n* **What triggers building a new model in MLOps level 2**: \n    * Performance degradation from monitoring\n\n* **Consider when setting trigger for retraining a model**: \n    * The frequency of triggering retrains\n    * Cost of retraining\n\n* **Policies to apply for triggering retraining from data monitoring**: \n    * Model performance degradation below a threshold\n    * Sudden drop in performance of the model\n\n* **Deploying new version vs. new model**: \n    * Whenever the model has similar inputs and outputs and is used for the same purpose.\n\n* **Good reasons to use Feature Store**:\n    * The features created by the data teams are not available during serving time, and this is creating training/serving differences.\n    * Models are built on a variety of features, including categorical variables and continuous variables\n\n* **Service that Feast does not use**: \n    * Gojek"
      }
    },
    "summary": ""
  },
  "Chapter 14BigQuery ML": {
    "content": "",
    "subsections": {
      "BigQuery – Data Access": {
        "content": "There are three ways of accessing the data. The most common method is using the web console to write a SQL query (shown in Figure 14.1). The results of the query are displayed below the query editor.\n\nThe second method is to run the same query in a Jupyter Notebook, by using the magic command `%%bigquery`, as shown in Figure 14.2. The figure shows the execution and the query result from BigQuery being run on a Jupyter Notebook running Vertex AI Workbench.\n\n**FIGURE 14.1** Running a SQL query in the web console\n\n**FIGURE 14.2** Running the same SQL query through a Jupyter Notebook on Vertex AI Workbench\n\nThe third method is to use a Python API to run the same query in Jupyter Notebook using the Python API. See the following code for reference. The first part is importing the BigQuery library and then creating a client. Pass the query as a string and the results are captured in a Pandas DataFrame.\n[code]\n     from google.cloud import bigquery\n     import pandas\n     client = bigquery.Client(location=\"us-central1\",project=\"projectname\")\n     query = \"\"\"\n         SELECT * FROM `projectname.dataset1.table1` LIMIT 10\n     \"\"\"\n     query_job = client.query(\n         query,\n         location=\"us-central1\",\n     )\n     df = query_job.to_dataframe()\n\n[/code]",
        "subsections": {},
        "summary": "* **Accessing Data**\n    * Using the web console: write a SQL query, execute and display results below the editor.\n    * Running in Jupyter Notebook with `%%bigquery` magic command\n        * Execute query, display execution and result from BigQuery\n    * Using Python API: import library, create client, pass query as string to get results in Pandas DataFrame"
      },
      "BigQuery ML Algorithms": {
        "content": "BigQuery ML (previously called BQML) allows you to create machine learning models using standard SQL queries. You can create models and train, test, validate, and predict using models with only SQL queries. You don't have to write any Python code to use machine learning in BigQuery. Moreover, it is completely serverless.\n\n* * *\n\nBigQuery ML is a completely serverless method to train and predict.\n\n* * *",
        "subsections": {
          "Model Training": {
            "content": "To create a model, the keyword to use is `CREATE MODEL`. This statement is similar to the `CREATE TABLE DDL` statement. When you run a query with the `CREATE MODEL` statement, a query job is generated and processes the query. Similar to `CREATE MODEL`, you also have `CREATE MODEL IF NOT EXISTS` and `CREATE OR REPLACE MODEL`, commands with names that are intuitive to help us reuse model names for our convenience.\n[code]\n     CREATE MODEL modelname1\n        OPTIONS(model_type='linear_reg', input_label_cols=['label_col'])\n     AS SELECT * FROM table1\n\n[/code]\n\nIn the preceding SQL command, after the keyword, you must provide two options, model_type and input_label_cols. The model type specifies what kind of model you are trying to build. There are regression, classification, and time‐series models available for you to choose from. See Table 14.1 for the full list of models available today. The second option is input_label_cols, which identifies the target column in the data provided below.\n\nFinally, the last part of the SQL command (“SELECT * FROM table1) identifies the tables you are going to use for training. Notice that it appears as a simple selection query, which means the query result is being passed to the training job. In this line, you can select some columns, select some rows, join multiple tables to generate a result, and so on to create your training dataset. The only restriction is to make sure that the target column exists and there are enough rows to train a model.\n\n**TABLE 14.1** Models available on BigQuery ML\n\nModel\nCategory | Model Type | Description\n---|---|---\nRegression | LINEAR_REG, BOOSTED_TREE_REGRESSOR, DNN_REGRESSOR, AUTOML_REGRESSION | To predict a real value\nClassification | LOGISTIC_REG, BOOSTED_TREE_CLASSIFIER, DNN_CLASSIFIER, DNN_LINEAR_COMBINED_CLASSIFIER, AUTOML_CLASSIFIER | To predict either a binary label or multiple labels\nDeep and wide models | DNN_LINEAR_COMBINED_REGRESSOR,\nDNN_LINEAR_COMBINED_CLASSIFIER | Deep and wide models used for recommendation systems and personalization\nClustering | KMEANS | Unsupervised clustering models\nCollaborative filtering | MATRIX_FACTORIZATION | For recommendations\nDimensionality reduction | PCA, AUTOENCODER | Unsupervised preprocessing step\nTime‐series forecasting | ARIMA_PLUS | Forecasting\nGeneral | TENSORFLOW | Generic TensorFlow model\n\nIf you take a look at the list in Table 14.1, there are several kinds of available models. The expected ones are in linear regression, classification, and clustering and are easy to define using SQL. However, as you go down the list, you may see DNN, which stands for _deep neural network_. In BigQuery ML, you have the complete flexibility to define and train DNN models by passing the right parameters in the options section. See Figure 14.3 for the full list of options for `DNN_CLASSIFIER` and `DNN_REGRESSOR`. These models are built using TensorFlow estimators. Notice that you have all the flexibility you need to build the model of your choice.\n\n**FIGURE 14.3** SQL options for `DNN_CLASSIFIER` and `DNN_REGRESSOR`\n\nWe created a model for our small dataset, and it was complete in a few minutes:\n[code]\n     CREATE OR REPLACE MODEL\n         `test.creditcard_model1`\n     OPTIONS(model_type='logistic_reg',\n             input_label_cols=['defaultpaymentnextmonth'])\n     AS\n     SELECT * FROM `test.creditcardtable`\n\n[/code]\n\nWhen we train a classification model, we can view the results of the training, the iterations, and also the evaluations, including aggregate metrics, score threshold, ROC curve, PR curves, and the confusion matrix. These are calculated automatically.",
            "subsections": {},
            "summary": "* **Create Model Statement**: `CREATE MODEL` statement creates a query job to process the query.\n    * `_model type_ specifies the model to build (regression, classification, time-series)_\n    * `_input label cols_ identifies the target column in the data_\n\n* **Available Models**:\n  * Regression: LINEAR_REG, BOOSTED_TREE_REGRESSOR, DNN_REGRESSOR, AUTOML_REGRESSION\n  * Classification: LOGISTIC_REG, BOOSTED_TREE_CLASSIFIER, DNN_CLASSIFIER, DNN_LINEAR_COMBINED_CLASSIFIER, AUTOML_CLASSIFIER\n\n* **Training a Model**: \n    * _view results of training, iterations, and evaluations (aggregate metrics, score threshold)_\n    * _confusion matrix is calculated automatically_"
          },
          "Model Evaluation": {
            "content": "However, it is recommended to use a separate dataset not seen by the model for evaluating the model using the keyword `ML.EVALUATE`.\n[code]\n     SELECT * FROM\n     ML.EVALUATE(MODEL `projectid.test.creditcard_model1`,\n       ( SELECT * FROM `test.creditcardtable`))\n\n[/code]\n\nThis gave us the result shown in Figure 14.4 in a few seconds, which shows the query and the results below it in the web interface.\n\n* * *\n\nSupervised and unsupervised learning model evaluations work differently.\n\n* * *\n\n**FIGURE 14.4** Query showing results of model evaluation",
            "subsections": {},
            "summary": "* To evaluate a model, use `ML.EVALUATE` with a separate unseen dataset.\n* *_Recommended practice_* to prevent overfitting.\n    * This is typically done using a test dataset.\n        + Example: `ML.EVALUATE` on `projectid.test.creditcard_model1` with `test.creditcardtable`."
          },
          "Prediction": {
            "content": "The `ML.PREDICT` function is used for prediction in BigQuery ML. You can pass an entire table to predict and the output will be a table with all the input columns and the same number of rows, along with two new columns, _predicted_ <label_column_name>_ and _predicted_ <label_column_name>_probs_. Here the <label_column_name> is the name of the label column in the training data. There are some differences.\n\nHere is the example SQL for making predictions from the model we created:\n[code]\n     select * from\n      ML.PREDICT (MODEL `dataset1.creditcard_model1`,\n               (select * FROM `dataset1.creditcardpredict` limit 1))\n\n[/code]\n\nHere we are specifying the model `creditcard_model1` that was created earlier and selecting one row from a table called `creditcardpredict` and passing it to the `ML.PREDICT` function. The `select` in the first line allows me to select only certain columns from the output.\n\nHere is the same example with only the predictions selected:\n[code]\n     select\n            predicted_defaultpaymentnextmonth,\n            predicted_defaultpaymentnextmonth_probs\n       from\n      ML.PREDICT (MODEL `dataset1.creditcard_model1`,\n           (select  * FROM `dataset1.creditcardtable` limit 1))\n\n[/code]\n\nThe result of the preceding query is shown in Figure 14.5. Notice that the predictions probability is shown for each label.\n\n**FIGURE 14.5** Query results showing only the predictions",
            "subsections": {},
            "summary": "* **Prediction with ML.PREDICT function**: passes a table to predict, producing output with _predicted_ <label_column_name>_ and _predicted_ <label_column_name>_probs_ columns.\n    * Passes a model and input data to `ML.PREDICT` function\n        *_Example_*\n            `select * from ML.PREDICT (MODEL 'dataset1.creditcard_model1', (select * FROM 'dataset1.creditcardpredict' limit 1))`\n* **Output format**: table with input columns, _predicted_ <label_column_name>_ and _predicted_ <label_column_name>_probs_ columns\n    * Includes probability for each label"
          }
        },
        "summary": "* **BigQuery ML**: allows you to create machine learning models using standard SQL queries without writing Python code\n    * is a completely serverless method to train and predict models\n        * does not require any additional infrastructure or setup"
      },
      "Explainability in BigQuery ML": {
        "content": "Explainability is important to debug models and improve transparency, and in some domains it is even a regulatory requirement. In BigQuery, you can get global feature importance values at the model level or you can get explanations for each prediction. These are also accessed using SQL functions.\n\nTo have explanations at the global level, you must set `enable_global_explain=TRUE` during training. Here is the sample SQL for our previous example:\n[code]\n     CREATE OR REPLACE MODEL\n            `model1`\n\n     OPTIONS(model_type=’logistic_reg’,\n             enable_global_explain=TRUE,\n             input_label_cols=[‘defaultpaymentnextmonth’]) AS SELECT\n       *\n     FROM ` dataset1.creditcardtable`\n\n[/code]\n\nAnd after the model has trained, you can query the model's global explanations, which are returned as a table (Figure 14.6); each row contains the input features with the floating‐point number representing the importance.\n[code]\n     SELECT  *\n     FROM\n     ML.GLOBAL_EXPLAIN(MODEL ` model1`)\n\n[/code]\n\nThe numbers next to the features represent the impact of a feature on making the predictions. The higher the attribution, the higher the relevance to model and vice versa. However, note that the attributions are not normalized (they do not add up to 1). See Table 14.2.\n\n**FIGURE 14.6** Global feature importance returned for our model\n\n**TABLE 14.2** Model types\n\nModel Type | Explainability Method | Description\n---|---|---\nLinear and logistic regression | Shapley values and standard errors, p‐values | This is the average of all the marginal contributions to all possible coalitions.\nBoosted Trees | Tree SHAP, Gini‐based feature importance | Shapley values optimized for decision tree–based models.\nDeep Neural Network and Wide‐and‐Deep | Integrated gradients | A gradients‐based method to efficiently compute feature attributions with same axiomatic properties as Shapley.\nArima_PLUS | Time‐series decomposition | Decompose into multiple components if present in the time series.\n\nThere is a computational cost to adding explainability to predictions. This is especially true for methods like Shapley where the complexity increases exponentially with the number of features.\n\nIn the following SQL code, we are using EXPLAIN_PREDICT instead of the PREDICT function. We are selecting all columns from a table called dataset1.credit_card_test and only one row. The result is shown in Figure 14.7.\n\n**FIGURE 14.7** Prediction result\n[code]\n     SELECT  *\n     FROM  ML.EXPLAIN_PREDICT(\n     MODEL `creditcard_model1`,\n         (\n           SELECT * FROM\n             `dataset1.credit_card_test` limit 1\n         ),\n     STRUCT(5 AS top_k_features))\n\n[/code]\n\nIn our case, predicted value is –1.281. Now let us look at the top five features that were reported as part of the query result (Figure 14.8). One thing to notice is that these individual feature contributions are two orders of magnitude lower than the baseline.\n\n**FIGURE 14.8** Top feature attributions for the prediction\n\nAfter you have analyzed the explanations, you may also want to record the global explanations as part of your metadata. At a model level, you may always set aside a dataset just to run explanations on. Store the feature attributions for every prediction along with the predictions.",
        "subsections": {},
        "summary": "* **Enable explainability**: Set `enable_global_explain=TRUE` during training using SQL\n```sql\nCREATE OR REPLACE MODEL\n    `model1`\nOPTIONS(model_type='logistic_reg', enable_global_explain=TRUE, input_label_cols=['defaultpaymentnextmonth']) AS SELECT * FROM `dataset1.creditcardtable`\n```\n* **Query global explanations**: Use `ML.GLOBAL_EXPLAIN(MODEL 'model1')` to get a table with feature importance values\n* **Explainability methods**: Available for different model types, including:\n  * Linear and logistic regression: Shapley values and standard errors, p-values\n  * Boosted Trees: Tree SHAP, Gini-based feature importance\n  * Deep Neural Network and Wide-and-Deep: Integrated gradients"
      },
      "BigQuery ML vs. Vertex AI Tables": {
        "content": "BigQuery ML deals with tables and so does Vertex AI. These two products have similarities, but there are key differences because they cater to different kinds of users. BigQuery is a serverless data warehouse, where users are SQL experts and think in terms of tables, joins, `GROUP‐BY` statements, and so on. Some of the BigQuery customers have written thousands of queries, and some of them are very complex queries, are automated using BigQuery_scheduled queries, and use visualization tools like Looker and Looker Studio.\n\nThe Vertex AI customer is very familiar with Kubeflow and most proficient in Java or Python. This data scientist uses Jupyter Notebooks on a daily basis and mainly uses Pandas DataFrames to manipulate the data. This machine learning engineer is building interesting new neural networks, and sometimes custom TensorFlow operations, and thinking about using TPUs. This user wants fine‐grained control over the flow of the data and the training process.\n\nIn summary, although the two products have largely similar features, the interface and the method to train and predict are very different and are aimed at different audiences.\n\n* * *\n\nIf you have a question where the user is an analyst or business user, you want to consider BigQuery. On the other hand, if you have machine learning engineers, consider Vertex AI.\n\n* * *",
        "subsections": {},
        "summary": "**BigQuery vs. Vertex AI**\n=====================================\n\n### **Similarities and Differences**\n\n*   Both products deal with tables but cater to different types of users:\n    *   **BigQuery**: Serverless data warehouse for SQL experts, focusing on table operations and joins.\n    *   **Vertex AI**: More advanced machine learning features for data scientists familiar with Java or Python, focusing on fine-grained control over data and training processes.\n\n*   BigQuery is ideal for analytics and visualization tasks, while Vertex AI is suited for building complex neural networks and custom TensorFlow operations.\n\n*   Key differences lie in the user interface and approach to training and prediction:\n    *   **BigQuery**: Automated queries, visualization tools like Looker Studio.\n    *   **Vertex AI**: Customizable workflows, Jupyter Notebooks, Pandas DataFrames.\n\n### **Choosing Between BigQuery and Vertex AI**\n\n*   For analytics and business users: Use BigQuery.\n*   For machine learning engineers: Use Vertex AI."
      },
      "Interoperability with Vertex AI": {
        "content": "Although Vertex AI and BigQuery ML are very distinct products, they have been designed to interoperate at every point in the machine learning pipeline. There are at least six integration points that make it easy to use both products together seamlessly.",
        "subsections": {
          "Access BigQuery Public Dataset": {
            "content": "BigQuery has more than 200 public datasets that are made available to the general public through the Google Cloud Public Datasets Program. Google pays for the storage of the datasets and you can access them through your GCP project. You pay only for the queries that you run on these datasets.\n\nYou can also access these datasets from Vertex AI to train your ML models or to augment your existing data. For example, say you are predicting the traffic conditions in a location; you can combine your dataset with BigQuery's public weather dataset to improve your model.",
            "subsections": {},
            "summary": "* **BigQuery Public Datasets**: Over 200 publicly available datasets stored by Google Cloud and accessible through GCP projects\n* **Query Costs**: Pay only for queries run on these datasets, no upfront storage costs\n* *_Access via Vertex AI_*: Train ML models or augment existing data with public datasets like weather data to improve model accuracy"
          },
          "Import BigQuery Data into Vertex AI": {
            "content": "When you create a dataset in Vertex AI, you provide the source URL to start with. You can directly provide a BigQuery URL that points to a BigQuery dataset to create a Vertex AI dataset. You can do this in just a few steps in the console or like this in Python:\n[code]\n     from google.cloud import aiplatform\n     dataset = aiplatform.TabularDataset.create(\n         display_name=\"my-tabular-dataset\",\n         bq_source=\"bq://project.dataset.table_name\",\n     )\n\n[/code]\n\nNotice that we are not exporting the data from BigQuery and then importing into Vertex AI. Thanks to this integration, you can now seamlessly connect to data in BigQuery.",
            "subsections": {},
            "summary": "* You can create a dataset in Vertex AI by providing a source URL that points to a BigQuery dataset.\n* This allows for seamless connection to data in BigQuery without exporting and importing the data.\n* Use `bq://project.dataset.table_name` as the source URL, like this: `bq://my-project.my-dataset.my-table`"
          },
          "Access BigQuery Data from Vertex AI Workbench Notebooks": {
            "content": "When you use a Jupyter Notebook from a managed notebook instance in Vertex AI Workbench, you can directly browse your BigQuery dataset, run SQL queries, or download into a Pandas DataFrame.\n\nThis is highly useful for data scientists who use Jupyter Notebooks for exploratory analysis, create visualizations, experiment with machine learning models, or perform feature engineering with different datasets.",
            "subsections": {},
            "summary": "* **Integration Benefits**: Directly browsing BigQuery dataset, running SQL queries, and downloading data into Pandas DataFrame\n* *_Enhanced Data Analysis_*: Useful for data scientists performing exploratory analysis, creating visualizations, and experimenting with machine learning models\n* *Streamlined Workflow*: Simplifies working with multiple datasets in a Jupyter Notebook"
          },
          "Analyze Test Prediction Data in BigQuery": {
            "content": "This feature is similar to the dataset creation integration but works in the other direction. When you train a model, you provide a train and test dataset, and as part of the process you get the predictions for the test dataset. You have the ability to export this directly to BigQuery. This is useful when you want to further analyze the test predictions using various SQL methods.",
            "subsections": {},
            "summary": "* _Model Prediction Export_\n  * **Train and Test Data Export**\n    *_Directly to BigQuery_*\n      + Utilizes model training data for analysis\n      + Enables SQL-based predictions analysis"
          },
          "Export Vertex AI Batch Prediction Results": {
            "content": "When you are making batch predictions in Vertex AI, you can directly point to a BigQuery table as the input data and, in addition, send the predictions back to BigQuery to be stored as a table. This is very useful if you have standardized your MLOps using Vertex AI, but the data is in BigQuery.",
            "subsections": {},
            "summary": "* You can make batch predictions in **Vertex AI** directly from a *_BigQuery_* table.\n* Predictions can also be sent back to BigQuery to be stored as a table.\n* This feature integrates well with standardized MLOps using Vertex AI."
          },
          "Export BigQuery Models into Vertex AI": {
            "content": "When you build a model in BigQuery, you can export the model from BigQuery into GCS and then import it into Vertex AI. This gives you complete freedom to train it and use it anywhere. Going forward, if you use the Vertex AI Model Registry, you can register your BigQuery ML models directly into it. This step saves you the additional work of exporting the model files into GCS and makes it seamless. Both BigQuery inbuilt models and TensorFlow models are supported. Currently there is a limitation on exporting ARIMA_PLUS models, XGBoost models, and models that use a transform.",
            "subsections": {},
            "summary": "* **Model Export**: You can export BigQuery ML models to GCS for import into Vertex AI.\n    * **Model Registry Integration**: Upcoming feature allows direct registration of BigQuery ML models in the Vertex AI Model Registry.\n        * **Supported Models**: Both BigQuery inbuilt and TensorFlow models are supported, except ARIMA_PLUS, XGBoost, and transform-based models."
          }
        },
        "summary": "* **Integration Points**: Vertex AI and BigQuery ML integrate through:\n  * Data ingestion\n  * Model training\n  * Model deployment\n  * Model monitoring\n  * Data labeling\n  * Experiment management"
      },
      "BigQuery Design Patterns": {
        "content": "There are several situations in data science and machine learning that recur frequently enough to be considered a pattern, and there are some clever well‐thought‐out solutions to address them, called design patterns. Because BigQuery ML is a revolutionary technology that approaches machine learning from a novel perspective, there have been some elegant new solutions to some of the old problems.",
        "subsections": {
          "Hashed Feature": {
            "content": "This solution addresses three problems faced by categorical variables:\n\n  * Incomplete vocabulary: The input data might not have the full set of the values that a categorical variable could take. This creates a problem if the data is fed directly into a ML model.\n  * High cardinality: Zip code is an example of a categorical variable with high cardinality, which creates some scaling issues in ML.\n  * Cold start problem: There could be new values added to the categorical variable that might not have existed in the training dataset—for example, the creation of a new employee ID when a person joins.\n\nOne method to deal with this problem is to transform this high cardinal variable into a low cardinal domain by hashing. This can be done very easily in BigQuery like this:\n[code]\n     ABS(MOD(FARM_FINGERPRINT(zipcode), numbuckets))\n\n[/code]\n\nThis uses a hashing function called FarmHash, a family of hashing algorithms that are deterministic, well‐distributed, and available in a number of programming languages.",
            "subsections": {},
            "summary": "* *_Categorical Variable Issues_*: Incomplete vocabulary, high cardinality, and cold start problem can affect ML models\n    * *_Solution_*:\n        * Transform high cardinal variables using hashing to reduce dimensionality \n            + *_Example_*: `ABS(MOD(FARM_FINGERPRINT(zipcode), numbuckets))`"
          },
          "Transforms": {
            "content": "Sometimes inputs to models are modified or enhanced or engineered before feeding into the model like the hashing example. There is valuable information in the transformations applied to the inputs. The transformations applied to the inputs to the training dataset must also be applied to the inputs if the model is deployed in production. So the code to transform is part of the pipeline when you are making the predictions. BigQuery has an elegant solution to this called the TRANSFORM clause that is part of the CREATE_MODEL function:\n[code]\n     CREATE OR REPLACE MODEL m\n       TRANSFORM(ML.FEATURE_CROSS(STRUCT(f1, f2)) as cross_f,\n                 ML.QUANTILE_BUCKETIZE(f3) OVER() as buckets,\n                 label_col)\n       OPTIONS(model_type=’linear_reg’, input_label_cols=['label_col'])\n     AS SELECT * FROM t\n\n[/code]\n\nIn the preceding example SQL, we are creating a “feature cross” transform over f1 and f2, and the second transform is a quantile bucketizing the feature f3. These transformations will be applied to the input fields f1, f2, and f3 to create new features called “cross_f” and “buckets”. BigQuery ML will then automatically add this transform code to the model that is built. Therefore, during prediction, when you pass inputs f1, f2, and f3, it will automatically perform these transformations before sending the inputs to the model.\n\nBigQuery offers many such useful transforms like POLYNOMIAL_EXPAND, FEATURE_CROSS, NGRAMS, QUANTILE_BUCKET, HASH_BUCKETIZE, MIN_MAX_SCALER and STANDARD_SCALER.\n\nFor more details on the `TRANSFORM` clause, refer to the documentation:\n\n`https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-preprocessing-functions`\n\nThe caveat for this design pattern is that these models with transforms will not work outside BigQuery ML, say, if you export the model to Vertex AI.",
            "subsections": {},
            "summary": "* **Transforms in BigQuery ML Models**: The `TRANSFORM` clause applies transformations to input fields before feeding into a model.\n    * These transformations must be applied when deploying the model in production.\n    * Applies transformations such as feature cross, quantile bucketizing, and scaling.\n        * Available transforms include `POLYNOMIAL_EXPAND`, `FEATURE_CROSS`, `NGRAMS`, `QUANTILE_BUCKET`, `HASH_BUCKETIZE`, `MIN_MAX_SCALER` and `STANDARD_SCALER`."
          }
        },
        "summary": "* _BigQuery ML introduces novel solutions to common data science and machine learning challenges_\n* These solutions are based on well-thought-out design patterns\n* _Elegant new approaches to tackling frequent problems in the field_"
      },
      "Summary": {
        "content": "BigQuery is an important service that revolutionized the use of ML in the SQL community. BigQuery ML democratized machine learning and made it available to many more people.\n\nIn this chapter we saw how to use SQL to perform all actions of a ML pipeline. We also learned how to apply transformations to input values directly using SQL, which reduces the time to create models. Although BigQueryML is a separate service, it is highly interoperable with Vertex AI. Lastly we also saw some interesting design patterns which are unique to BigQuery ML.",
        "subsections": {},
        "summary": "* _BigQuery ML simplifies machine learning_ using SQL, making it more accessible to many users.\n* *_Interoperability with Vertex AI_* allows for seamless collaboration between services.\n* _Unique design patterns_ enable efficient model creation and data transformation."
      },
      "Exam Essentials": {
        "content": "* **Understand BigQuery and ML.** Learn the history of BigQuery and the innovation of bringing machine learning into a data warehouse and to data analysis and anyone familiar with SQL. Learn how to train, predict, and provide model explanations using SQL.\n  * **Be able to explain the differences between BigQuery ML and Vertex AI and how they work together.** These services offer similar features but are designed for different users. BigQuery ML is designed for analysts and anyone familiar with SQL, and Vertex AI is designed for ML engineers. Learn the various different integration points that make it seamless to work between the two services.\n  * **Understand BigQuery design patterns.** BigQuery has elegant solutions to recurring problems in machine learning. Hashing, transforms, and serverless predictions are easy to apply to your ML pipeline.",
        "subsections": {},
        "summary": "* **BigQuery and Machine Learning Overview**\n    * Learn about BigQuery's history and the innovation of bringing machine learning into data analysis\n    * Understand how to train, predict, and explain models using SQL\n* **BigQuery ML vs Vertex AI**\n    * _Differences between BigQuery ML and Vertex AI_\n        - *Learn about the design differences between these two services*\n    * _Integration Points for Seamless Collaboration_\n        - *Understand how to work together with both services*\n* _BigQuery Design Patterns for Machine Learning_\n    * Learn about easy solutions like hashing, transforms, and serverless predictions"
      },
      "Review Questions": {
        "content": "1. You work as part of a large data analyst team in a company that owns a global footwear brand. The company manufactures in South Asia and distributes all over the globe. Its sales were affected during the COVID‐19 pandemic and so was distribution. Your team has been asked to forecast sales per country with new data about the spread of the illness and a plan for recovery. Currently your data is on‐prem and sales data comes from all over the world weekly. What will you use to forecast?\n     1. Use Vertex AI AutoML Tables to forecast sales as this is a distributed case.\n     2. User Vertex AI AutoML Tables with custom models (TensorFlow) because this is a special case due to COVID‐19.\n     3. Use BigQuery ML, experiment with a TensorFlow model and DNN models to find the best results.\n     4. Use BigQuery ML with ARIMA_PLUS, and use the BigQuery COVID‐19 public dataset for trends.\n  2. You are part of a startup that rents bicycles, and you want to predict the amount of time a bicycle will be used and the distance it will be taken based on current location and userid. You are part of a small team of data analysts, and currently all the data is sitting in a data warehouse. Your manager asks you to quickly create a machine learning model so that they can evaluate this idea. Your manager wants to show this prototype to the CEO to improve sales. What will you choose?\n     1. Use a TensorFlow model on Vertex AI tables to predict time and distance.\n     2. Use the advanced path prediction algorithm in Google Maps.\n     3. Use BigQuery ML.\n     4. Use a Vertex AI custom model to get better results because the inputs include map coordinates.\n  3. You are a data analyst for a large video sharing website. The website has thousands of users that provide 5‐star ratings for videos. You have been asked to provide recommendations per user. What would you use?\n     1. Use BigQuery classification `model_type`.\n     2. Use a Vertex AI custom model to build a collaborative filtering model and serve it online.\n     3. Use the matrix factorization model in BigQuery ML to create recommendations using explicit feedback.\n     4. Use Vertex AI AutoML for matrix factorization.\n  4. You are a data analyst and your manager gave you a TensorFlow SavedModel to use for a classification. You need to get some predictions quickly but don't want to set up any instances or create pipelines. What would be your approach?\n     1. Use BigQuery ML and choose TensorFlow as the model type to run predictions.\n     2. Use Vertex AI custom models, and create a custom container with the TensorFlow SavedModel.\n     3. TensorFlow SavedModel can only be used locally, so download the data onto a Jupyter Notebook and predict locally.\n     4. Use Kubeflow to create predictions.\n  5. You are working as a data scientist in the finance industry and there are regulations about collecting and storing explanations for every machine learning prediction. You have been tasked to provide an initial machine learning model to classify good loans and loans that have defaulted. The model that you provide will be used initially and is expected to be improved further by a data analyst team. What is your solution?\n     1. Use Kubeflow Pipelines to create a Vertex AI AutoML Table with explanations.\n     2. Use Vertex AI Pipelines to create a Vertex AI AutoML Table with explanations and store them in BigQuery for analysts to work on.\n     3. Use BigQuery ML, and select “classification” as the model type and enable explanations.\n     4. Use Vertex AI AutoML Tables with explanations and store the results in BigQuery ML for analysts.\n  6. You are a data scientist and have built extensive Vertex AI Pipelines which use Vertex AI AutoML Tables. Your manager is asking you to build a new model with data in BigQuery. How do you want to proceed?\n     1. Create a Vertex AI pipeline component to download the BigQuery dataset to a GCS bucket and then run Vertex AI AutoML Tables.\n     2. Create a new Vertex AI pipeline component to train BigQuery ML models on the BigQuery data.\n     3. Create a Vertex AI pipeline component to execute Vertex AI AutoML by directly importing a BigQuery dataset.\n     4. Create a schedule query to train a model in BigQuery.\n  7. You are a data scientist and have built extensive Vertex AI Pipelines which use Vertex AI AutoML Tables. Your manager is asking you to build a new model with a BigQuery public dataset. How do you want to proceed?\n     1. Create a Vertex AI pipeline component to download the BigQuery dataset to a GCS bucket and then run Vertex AI AutoML Tables.\n     2. Create a new Vertex AI pipeline component to train BigQuery ML models on the BigQuery data.\n     3. Create a Vertex AI pipeline component to execute Vertex AI AutoML by directly importing the BigQuery public dataset.\n     4. Train a model in BigQuery ML because it is not possible to access BigQuery public datasets from Vertex AI.\n  8. You are a data scientist, and your team extensively uses Jupyter Notebooks. You are merging with the data analytics team, which uses only BigQuery. You have been asked to build models with new data that the analyst team created in BigQuery. How do you want to access it?\n     1. Export the BigQuery data to GCS and then download it to the Vertex AI notebook.\n     2. Create an automated Vertex AI pipeline job to download the BigQuery data to a GCS bucket and then download it to the Vertex AI notebook.\n     3. Use Vertex AI managed notebooks, which can directly access BigQuery tables.\n     4. Start using BigQuery console to accommodate the analysts.\n  9. You are a data scientist, and your team extensively uses Vertex AI AutoML Tables and pipelines. Your manager wants you to send the predictions of new test data to test for bias and fairness. The fairness test will be done by the analytics team that is comfortable with SQL. How do you want to access it?\n     1. Export the test prediction data from GCS and create an automation job to transfer it to BigQuery for analysis.\n     2. Move your model to BigQuery ML and create predictions there.\n     3. Deploy the model and run a batch prediction on the new dataset to save in GCS and then transfer to BigQuery.\n     4. Add the new data to your AutoML Tables test set, and configure the Vertex AI tables to export test results to BigQuery.\n  10. You are a data scientist, and your team extensively uses Vertex AI AutoML Tables and pipelines. Your manager wants you to send predictions to test for bias and fairness. The fairness test will be done by the analytics team that is comfortable with SQL. How do you want to access it?\n     1. Export the test prediction data from GCS and create an automation job to transfer it to BigQuery for analysis.\n     2. Move your model to BigQuery ML and create predictions there.\n     3. Deploy the model and run a batch prediction on the new dataset to save in GCS and then transfer to BigQuery.\n     4. Deploy the model and run a batch prediction on the new dataset to export directly to BigQuery.\n  11. You are a data scientist, and your team extensively uses Vertex AI AutoML Tables and pipelines. Another team of analysts has built some highly accurate models on BigQuery ML. You want to use those models also as part of your pipeline. What is your solution?\n     1. Run predictions in BigQuery and export the prediction data from BigQuery into GCS and then load it into your pipeline.\n     2. Retrain the models on Vertex AI tables with the same data and hyperparameters.\n     3. Load the models in the Vertex AI model repository and run batch predictions in Vertex AI.\n     4. Download the model and create a container for Vertex AI custom models and run batch predictions.\n  12. You are a data analyst and working with structured data. You are exploring different machine learning options, including Vertex AI and BigQuery ML. You have found that your model accuracy is suffering because of a categorical feature (zipcode) that has high cardinality. You do not know if this feature is causing it. How can you fix this?\n     1. Use the hashing function `(ABS(MOD(FARM_FINGERPRINT(zipcode),buckets))` in BigQuery to bucketize.\n     2. Remove the input feature and train without it.\n     3. Don't change the input as it affects accuracy.\n     4. Vertex AI tables will automatically take care of this.\n  13. You are a data analyst working with structured data in BigQuery and you want to perform some simple feature engineering (hashing, bucketizing) to improve your model accuracy. What are your options?\n     1. Use the BigQuery TRANSFORM clause during CREATE_MODEL for your feature engineering.\n     2. Have a sequence of queries to transform your data and then use this data for BigQuery ML training.\n     3. Use Data Fusion to perform feature engineering and then load it into BigQuery.\n     4. Build Vertex AI AutoML Tables which can automatically take care of this problem.\n  14. You are part of a data analyst team working with structured data in BigQuery but also considering using Vertex AI AutoML. Which of the following statements is wrong?\n     1. You can run BigQuery ML models in Vertex AI AutoML Tables.\n     2. You can use BigQuery public datasets in AutoML Tables.\n     3. You can import data from BigQuery into AutoML.\n     4. You can use SQL queries on Vertex AI AutoML Tables.\n  15. Which of the following statements is wrong?\n     1. You can run SQL in BigQuery through Python.\n     2. You can run SQL in BigQuery through the CLI.\n     3. You can run SQL in BigQuery through R.\n     4. You can run SQL in BigQuery through Vertex AI.\n  16. You are training models on BigQuery but also use Vertex AI AutoML Tables and custom models. You want flexibility in using data and models and want portability. Which of the following is a bad idea?\n     1. Bring TensorFlow models into BigQuery ML.\n     2. Use TRANSFORM functionality in BigQuery ML.\n     3. Use BigQuery public datasets for training.\n     4. Use Vertex AI Pipelines for automation.\n  17. You want to standardize your MLOps using Vertex AI, especially AutoML Tables and Vertex AI Pipelines, etc., but some of your team is using BigQuery ML. Which of the following is incorrect?\n     1. Vertex AI Pipelines will work with BigQuery.\n     2. BigQuery ML models that include TRANSFORM can also be run on AutoML.\n     3. BigQuery public datasets can be used in Vertex AI AutoML Tables.\n     4. You can use BigQuery and BigQuery ML through Python from Vertex AI managed notebooks.\n  18. Which of these statements about BigQuery ML is incorrect?\n     1. BigQuery ML supports both supervised and unsupervised models.\n     2. BigQuery ML supports models for recommendation engines.\n     3. You can control the various hyperparameters of a deep learning model like dropouts in BigQuery ML.\n     4. BigQuery ML models with TRANSFORM clause can be ported to Vertex AI.\n  19. Which of these statements about comparing BigQuery ML explanations is incorrect?\n     1. All BigQuery ML models provide explanations with each prediction.\n     2. Feature attributions are provided both at the global level and for each prediction.\n     3. The explanations vary by the type of model used.\n     4. Not all models have global explanations.\n  20. You work as part of a large data analyst team in a company that owns hundreds of retail stores across the country. Their sales were affected due to bad weather. Currently your data is on‐prem and sales data comes from all across the country. What will you use to forecast sales using weather data?\n     1. Use Vertex AI AutoML Tables to forecast with previous sales data.\n     2. User Vertex AI AutoML Tables with a custom model (TensorFlow) and augment the data with weather data.\n     3. Use BigQeury ML, and use the Wide‐and‐Deep model to forecast sales for a wide number of stores as well as deep into the future.\n     4. Use BigQuery ML with ARIMA_PLUS, and use the BigQuery public weather dataset for trends.",
        "subsections": {},
        "summary": "* You will use *_BigQuery ML_* to forecast sales per country using new data about the spread of the illness and a plan for recovery because it can handle distributed cases and provides better results compared to other options.\n    * *_Vertex AI AutoML Tables_* are not suitable for this case due to its distributed nature, \n    * *_Vertex AI Pipelines_* would be complex to set up with new data, \n    * *_BigQuery ML_* is the most efficient option given the complexity of sales data distribution.\n\n* You will use *_BigQuery ML_* and *_TensorFlow model_* to predict time and distance for bicycle rentals because it can handle map coordinates and provide more accurate results.\n    * *_Vertex AI Pipelines_* would be complex, \n    * *_Vertex AI AutoML Tables_* are not suitable for custom models,\n    * *_Google Maps_* algorithm is limited in this scenario.\n\n* You will use *_BigQuery ML_* with a _*_Matrix factorization model_* to create recommendations using explicit feedback because it provides the most accurate results.\n    * *_Vertex AI AutoML Tables_* do not support matrix factorization,\n    * *_Vertex AI Pipelines_* would be complex, \n    * *_Vertex AI custom models_* are not necessary in this scenario.\n\n* You will use *_BigQuery ML_* to get quick predictions without setting up instances or pipelines because it can handle saved models.\n    * *_Vertex AI custom models_* require more setup,\n    * *_Vertex AI Pipelines_* would be complex, \n    * *_Jupyter Notebooks_* are not suitable for bigquery.\n\n* You will use *_BigQuery ML_* to provide machine learning predictions with explanations for good loans and defaulted loans because it supports classification and provides explanations.\n    * *_Kubeflow Pipelines_* are not necessary in this scenario,\n    * *_Vertex AI AutoML Tables_* do not support explanations,\n    * *_Vertex AI Pipelines_* would be complex, \n\n* You will use *_BigQuery ML_* to build a model with BigQuery public dataset because it provides flexible and portable solutions.\n    * *_Vertex AI AutoML Tables_* are limited in this scenario,\n    * *_Vertex AI Pipelines_* require more setup, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will use *_BigQuery ML_* or *_Vertex AI Pipelines_* to send predictions to test for bias and fairness because it can handle BigQuery public datasets.\n    * *_Exporting data to GCS_* is not necessary,\n    * *_Moving model to BigQuery ML_* provides more flexibility, \n    * *_Deploying model in Vertex AI_* would be complex.\n\n* You will use *_BigQuery Pipelines_* to import and train a model from another team on BigQuery ML because it provides flexible and portable solutions.\n    * *_Vertex AI AutoML Tables_* are limited in this scenario,\n    * *_Using TRANSFORM_* is not necessary, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will use *_BigQuery Pipelines_* to train a model on BigQuery public dataset because it provides flexible and portable solutions.\n    * *_Vertex AI AutoML Tables_* are limited in this scenario,\n    * *_Using TRANSFORM_* is not necessary, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will use *_BigQuery ML_* or *_Vertex AI Pipelines_* to send predictions to test for bias and fairness because it can handle BigQuery public datasets.\n    * *_Exporting data to GCS_* is not necessary,\n    * *_Moving model to BigQuery ML_* provides more flexibility, \n    * *_Deploying model in Vertex AI_* would be complex.\n\n* You will use *_Vertex AI Pipelines_* or *_BigQuery ML_* to load and use models from another team on BigQuery.\n    * *_Using TRANSFORM_* is not necessary, \n    * *_Exporting data to GCS_* is not necessary,\n    * *_Vertex AI AutoML Tables_* are limited in this scenario.\n\n* You will avoid using *_Vertex AI Pipelines_* or *_Vertex AI AutoML Tables_* with *_TensorFlow models_* because they require more setup and are less efficient.\n    * *_BigQuery ML_* provides better results, \n    * *_Using TRANSFORM_* is not necessary, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will avoid using *_Vertex AI Pipelines_* or *_Vertex AI AutoML Tables_* with *_BigQuery public datasets_* because they limit the flexibility of these solutions.\n    * *_BigQuery ML_* provides better results, \n    * *_Using TRANSFORM_* is not necessary, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will use *_Vertex AI Pipelines_* or *_BigQuery ML_* to handle categorical features with high cardinality in BigQuery ML.\n    * *_Using TRANSFORM_* provides better results,\n    * *_Vertex AI AutoML Tables_* are limited in this scenario, \n    * *_Exporting data to GCS_* is not necessary.\n\n* You will use *_Vertex AI Pipelines_* or *_BigQuery ML_* to perform simple feature engineering (hashing, bucketizing) on structured data in BigQuery.\n    * *_Data Fusion_* provides better results,\n    * *_Vertex AI AutoML Tables_* are limited in this scenario, \n    * *_Using TRANSFORM_* is not necessary.\n\n* You will avoid using *_BigQuery ML_* for comparing explanations of machine learning models because it does not provide the same level of control.\n    * *_Vertex AI Pipelines_* provide more control, \n    * *_Vertex AI AutoML Tables_* are limited in this scenario,\n    * *_Using TRANSFORM_* is not necessary.\n\n* You will use *_BigQuery ML_* to forecast sales per country using weather data because it provides better results compared to other options.\n    * *_Vertex AI AutoML Tables_* are not suitable for this case due to its complexity, \n    * *_Vertex AI Pipelines_* would be complex, \n    * *_Google Maps_* algorithm is limited in this scenario."
      }
    },
    "summary": ""
  },
  "AppendixAnswers to Review Questions": {
    "content": "",
    "subsections": {
      "Chapter 1: Framing ML Problems": {
        "content": "1. A, B, D. First understand the use case, and then look for the details such as impact, success criteria, and budget and time frames. Finding the algorithm comes later.\n  2. B. Hyperparameters are variables that cannot be learned. You will use a hyperparameter optimization (HPO) algorithm to automatically find the best hyperparameters. This is not considered when you are trying to match a business case to an ML problem.\n  3. B. The input data is time‐series data and predicting for next 7 days is typical of a forecasting problem.\n  4. B. A prediction has only two outputs: either valid or not valid. This is binary classification. If there are more than two classes, it is multiclass classification. Linear regression is predicting a number. Option C is popular with support tickets to identify clusters of topics but cannot be used in this case.\n  5. C. When you are trying to identify an object across several frames, this is video object tracking. Option A is factually incorrect. Option B is for images, not video. Scene detection or action detection classifies whether an “action” has taken place in video, a different type of problem, so option D is also wrong.\n  6. D. Topic modeling is an unsupervised ML problem. Given a set of documents, it would cluster them into groups and also provide the keywords that define each cluster.\n  7. C. Precision is a metric for unbalanced classification problems.\n  8. A. The root‐mean‐squared error (RMSE) is the best option if you are trying to reduce extreme errors.\n  9. A. RMSE, MAPE, and R2 are regression metrics. Accuracy is the only classification metric here.\n  10. C. We can eliminate RMSE because it is a regression metric. Accuracy is also wrong because it is a poor metric for imbalanced (1:100) datasets. So, the correct answer is either precision or recall. In this case, a false negative could cause severe problems later on, so we want to choose a metric that minimizes false negatives. So, the answer is recall.\n  11. B. “No labeled data” means you cannot have supervised learning or semi‐supervised learning. Reinforcement learning is when an agent actively explores an environment (like a robot), which is not relevant here. Only unsupervised learning can be applied to purely unlabeled data.\n  12. C. The Like button here is the explicit feedback that users provide on content and can be used for training. Collaborative filtering is the class of algorithm that can be used for recommendations such as in this case.\n  13. B. Option B is the bad idea because you need to update the data related to the new products. The idea of a “golden dataset” exists, but in this case, the dataset needs to be updated.\n  14. D. Use supervised learning when you have labeled data. Use unsupervised learning when you have unlabeled data. Use semi‐supervised learning when you have a mix. There is no such thing as hyper‐supervised learning.\n  15. A, D. Option A is absolutely true and is done throughout the industry. Option B is incorrect because it is done frequently in practice. Option C is partially true because it may amplify errors, but that does not mean you never feed one. Option D is correct because there is an entire class of models that help in transforming data for downstream prediction.\n  16. D. Whenever dealing with customer data and sensitive data, it is important to test your model for biases and apply responsible AI practices.\n  17. C. More testing data is not going to achieve much here. But that does not mean we cannot do anything. You can't always remove all the fields that may cause bias because some details might be hidden in other fields. The correct answer is to use model interpretability and explanations.\n  18. C. The model was deployed properly. Most Android phones can handle deep learning models very well. We cannot say much about the metric because it is unknown. This fun Android app could be used by a wide variety of people and was possibly not tested on a representative sample dataset.\n  19. B, D. There are many kinds of private data, not just photographs. Scans are also private data. There should always be concerns when using sensitive data.\n  20. B. While you can use the data creatively, there is always a privacy concern when dealing with customer data. Option A is true because you usually recommend other products at checkout. Option C is true because changes in user behavior and in the product catalog require retraining. Option D is true because you can use important information about products, like similar products or complementary products, to sell more.",
        "subsections": {},
        "summary": "* **Understanding the Problem**: Identify the problem type (classification, regression, forecasting) and relevant data characteristics (e.g., time-series, unlabeled)\n    * _Use appropriate ML algorithm_\n    * _Consider impact, success criteria, budget, and time frames_\n\n* **Hyperparameters and Optimization**: Use hyperparameter optimization (HPO) to find best parameters for a model.\n    * _Avoid overfitting by optimizing parameters_\n    * _Choose metric that minimizes false negatives (e.g., recall)_"
      },
      "Chapter 2: Exploring Data and Building Data Pipelines": {
        "content": "1. D. Oversampling is the way to improve the imbalanced data class.\n  2. A. The model performed poorly on new patient data due to label leakage because you are training the model on hospital name.\n  3. A. Monitoring the model for skew and retraining will help with data distribution.\n  4. B. Model retraining will help with data distribution and minimizing data skew.\n  5. B. Downsample the majority data with unweighting to create 10 percent samples.\n  6. D. Transforming data before splitting for testing and training will avoid data leakage and will lead to better performance during model training.\n  7. C. Removing features with missing values will help because the dataset has columns with missing values.\n  8. A, B, and D. All of the options describe reasons for data leakage except option C, removing features with missing values.",
        "subsections": {},
        "summary": "* *_Data leakage_*: Training model on hospital name causing poor performance on new patient data\n* *_Improvement methods_*:\n\t+ Downsample majority data to 10% samples (option D)\n\t+ Transform data before splitting for testing and training (option E, not explicitly listed but implied by option D)\n\t+ Remove features with missing values (option C)"
      },
      "Chapter 3: Feature Engineering": {
        "content": "1. C. With one hot encoding you can convert categorical features to numeric features. Moreover, not all algorithms works well on categorical features.\n  2. B. Normalizing the data will help convert the range into a normalized format and will help converge the model.\n  3. C. For imbalanced datasets, AUC PR is a way to minimize false positives compared to AUC ROC.\n  4. B. Since the model is performing well with training data, it is a case of data leakage. Cross‐validation is one of the strategies to overcome data leakage. We covered this in Chapter 2.\n  5. A, B. With TensorFlow data, prefectching and interleaving are techniques to improve processing time.\n  6. A. Use a tf.data.Dataset.prefetch transformation.\n  7. C. We will get one feature cross of binned latitude, binned longitude, and binned roomsPerPerson.\n  8. A. Cloud Data Fusion is the UI‐based tool for ETL.\n  9. A. TensorFlow Transform is the most scalable way to transform your training and testing data for production workloads.\n  10. D. Since the model is underperforming with production data, there is a training‐serving skew. Using a tf.Transform pipeline helps prevent this skew by creating transformations separately for training and testing.",
        "subsections": {},
        "summary": "* **Data Preprocessing**\n  * _Hot encoding_ converts categorical features to numeric features\n  * Normalizing data helps converge the model\n* **Model Evaluation and Training**\n  * AUC PR is used for imbalanced datasets to minimize false positives\n  * Data leakage occurs when models perform well on training data; use cross-validation to overcome this\n* **Data Processing with TensorFlow**\n  * Prefetching and interleaving improve processing time with `tf.data.Dataset`\n  * Use `tf.data.Dataset.prefetch` transformation to optimize data loading"
      },
      "Chapter 4: Choosing the Right ML Infrastructure": {
        "content": "1. C. Always start with a pretrained model and see how well it solves your problem. If that does not work, you can move to AutoML. Custom models should always be the last resort.\n  2. C. “Glossary” is a feature that is intended to solve this exact problem. If you have some terms that need to be translated in a certain way, you can create a list of these in a XML document and pass it to Google Translate. Choose the Advanced option and not Basic. Whenever these specific words/phrases appear, it will replace them with your translation from the glossary.\n  3. D. It is true that there is no “translated subtitle” service; however, you can combine two services to suit your needs. Option A is wrong because there is no AutoML in Vertex AI today. Options B and C are possible but should not be the first step.\n  4. A. This is a classification problem. Using the AutoML Edge model type is the right approach because the model will be deployed on the edge device. While both Coral.ai and Android app deployment are right, if you want to go to market quickly, it is better to go with Android application using ML Kit.\n  5. A. You get the error “not found” when a GPU is not available in the selected region. Not all regions have all GPUs. If you have insufficient quota, you will get the error “Quota ‘NVIDIA_V100_GPUS’ exceeded.”\n  6. D. Option A is wrong because n1‐standard‐2 is too small for GPUs, and option B is wrong because it is still using CPUs. Option D is better because it is better to go with 1 TPU than 8 GPUs, especially when you don't have any manual placements.\n  7. D. “Recommended for you” is intended for home pages, which brings attention to the most likely products based on current trends and user behavior. “Similar items” is based on product information only, which helps customers choose between similar products. “Others you may like” is the right choice for content based on the user's browsing history. “Frequently bought together” is intended to be shown at checkout when they can quickly add more into their cart.\n  8. B. “Frequently bought together” is intended to be shown at checkout when customers can quickly add more into their cart. “Recommended for you” is intended for home pages, which brings attention to the most likely product. “Similar items” is based on product information only, which helps customers choose between similar products. “Others you may like” is the right choice for showing content based on the user's browsing history.\n  9. A. When you want the customer to “engage more,” it means you want them to spend more time in the website/app browsing through the products. “Frequently bought together” is intended to be shown at checkout when customers can quickly add more into their cart. “Recommended for you” is intended for home pages and brings attention to the most likely product, and “Similar items” is based on product information only, which helps customers to choose between similar products. “Others you may like” is the right choice for showing content based on the user's browsing history.\n  10. C. When you do not have browsing data, or “user events,” you have to create recommendations based on project catalog information only. The only model that does not require “user information” in this list is “Similar items,” which shows the products that are similar to the one the user is currently viewing.\n  11. B. The objective of “click‐through rate” is based on the number of times the user clicks and follows a link, whereas the “revenue per order” captures the effectiveness for a recommendation being made at checkout.\n  12. D. Option A is wrong because there is no AutoML for this today. Currently there is no pretrained API available for this on GCP. A Vertex AI custom job is the most appropriate.\n  13. B. Option A is wrong because the Natural Language API does not accept voice. While options C and D are also correct, these are custom models that will take time.\n  14. C. TPUs do not support custom TensorFlow operations. GPUs are the best options here.\n  15. A. Only A2 and N1 machine series support GPUs. Option C is wrong because you cannot have 3 GPUs in an instance.\n  16. A. Pushing a large model to an Android device without hardware support might slow the device significantly. Using devices with Edge TPU installed is the best answer here.\n  17. A, C. You cannot have TPU and GPU in a single instance. You would not usually go for a cluster of TPU VMs.\n  18. C. If you have a sparse matrix, TPUs will not provide the necessary efficiency.\n  19. C. TPUs are not used for high‐precision predictions.\n  20. C, D. Options A and B increase the size of the instance without identifying the root cause. The question mentions that model has been already deployed on a big instance (32‐core). The next step should be to identify the root cause of the latency, so Option C is a correct choice. Also, checking the code to see if it is single‐threaded is correct, because it is not always a hardware problem; it could be a configuration issue or a software issue of being single threaded code.",
        "subsections": {},
        "summary": "* **AutoML**: Always start with pre-trained models and use AutoML (or custom models) as the last resort.\n  * *_Glossary_*: Create a list of terms to translate in an XML document for Google Translate's Advanced option.\n* **Vertex AI**: \n  + Use Edge model type for classification problems, deployed on edge devices. *_ML Kit_* is recommended for quick market deployment.\n  + When using GPUs, ensure sufficient quota and region availability. n1-standard-2 is insufficient for GPUs.\n* **Cloud Recommendations**:\n  * *_Recommended for you_*: Showcased on home pages, based on current trends and user behavior.\n  + *_Frequently bought together_*: Shown at checkout, helping customers quickly add more to their cart.\n  + *_Others you may like_*: Based on browsing history, suitable for content recommendations.\n* **Customer Engagement**:\n  * *_Engage more_* means increasing customer time spent browsing products. *_Frequently bought together_* is shown at checkout.\n+ *_Recommended for you_* and *_Similar items_* are for home pages and product information, respectively.\n+ *_Others you may like_* shows content based on user's browsing history.\n* **Recommendation Models**:\n  * When no browsing data exists, create recommendations using project catalog information only. *_Similar items_* does not require user information.\n* **AutoML Performance Metrics**:\n  + *_Click-through rate_* measures the effectiveness of a recommendation being made at checkout.\n+ *_Revenue per order_* captures the effectiveness of a recommendation in terms of sales.\n* **Vertex AI Custom Jobs**: Recommended for pre-trained APIs unavailable today, such as AutoML."
      },
      "Chapter 5: Architecting ML Solutions": {
        "content": "1. B. The question is asking for the simplest solution, so we do not need Memorystore and Bigtable as the latency requirement is 300ms@p99. The best and simplest way to handle this is using App Engine to deploy the applications and call the model endpoint on the Vertex AI Prediction.\n  2. B. Bigtable is designed for very low latency reads of very large datasets.\n  3. A. To preprocess data you will use Dataflow, and then you can use the Vertex AI platform for training and serving. Since it's a recommendation use case, Cloud BigQuery is the recommended NoSQL store to manage this use case storage at scale and reduce latency.\n  4. A. Since you want to minimize the infrastructure overhead, you can use the Vertex AI platform for distributed training.\n  5. A, C. With Document AI, you can get started quickly because it's a solution offering built on the top layer of your AI stack with the least infrastructure heavy lifting needed by you to set up. Cloud Storage is the recommended data storage solution to create a document data lake.\n  6. A. When the question asks for retraining, look for a pipeline that can automate and orchestrate the task. Kubeflow Pipelines is the only option here that can help automate the retraining workflow.\n  7. D. Kubeflow Pipelines is the only choice that comes with an experiment tracking feature. See `www.kubeflow.org/docs/components/pipelines/concepts/experiment`.\n  8. C. You can use a Cloud Storage trigger to send a message to a Pub/Sub topic and create a Cloud Function that can trigger the GKE training jobs. See `https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#triggering-and-scheduling-kubeflow-pipelines`.\n  9. A. Use Kubeflow experiments for training and executing experiments.\n  10. A. You can use the batch prediction functionality because the data is aggregated at the end of the day and you do not need prediction in near real time.\n  11. B. TensorFlow’s BigQueryClient uses the Storage API to efficiently read data directly out of BigQuery storage. The tf.data.datasetreader is used to create a input data pipeline and not to load data efficiently into tensorflow from BigQuery.\n  12. A, B, C. You can use all three connectors to connect to various framework datasets in BigQuery. Refer to Table 5.3 in this chapter.\n  13. A. A Vertex AI–managed dataset is the best way to organize and manage your data for training and prediction in the Vertex AI platform.\n  14. A, B. You should avoid storing data in block and file storage for ML use cases due to latency issues.",
        "subsections": {},
        "summary": "* **Vertex AI**: For recommendation and model endpoint deployment with App Engine\n    * Handles large datasets with low latency reads\n* **Kubeflow Pipelines**: Automates retraining, tracking experiments, and triggering GKE jobs\n    * Recommended for automating retraining workflow"
      },
      "Chapter 6: Building Secure ML Pipelines": {
        "content": "1. B. You would use the federated learning technique and deploy the ML model on the device where the data is stored.\n  2. B. Setting up different resources in separate projects can help separate the use of resources.\n  3. D. Masking replaces the value with surrogate characters such as # and the asterisk.\n  4. A, B, C. You need a service account key and authentication with GOOGLE_APPLICATION_CREDENTIALS to use APIs. You also need to provide Vertex AI IAM access to the service account role created by the Vertex AI Workbench compute instance.\n  5. B, C. Cloud DLP can help redact and mask the PII, and VPC security control can manage data access.\n  6. A. You need to use a Vertex AI–managed notebook, which will take care of auto shutdown of idle instances automatically.\n  7. B. Cloud Healthcare API helps de‐identify PHI data from an FHIR‐type dataset.\n  8. A. Using architecture best practices, you can stream the files to Google Cloud and use Cloud Dataflow to write it to BigQuery. You can bulk‐scan the tables using DLP API.",
        "subsections": {},
        "summary": "* **Federated Learning**: Deploy ML model on device where data is stored\n    * _Use of resources separated in different projects_\n* **Authentication and Authorization_*\n  * Requires service account key, GOOGLE_APPLICATION_CREDENTIALS, and Vertex AI IAM access\n  * _Uses Cloud DLP for redaction and VPC security control for data management_\n* **Cloud Computing Benefits_*\n  * Uses Vertex AI–managed notebook with auto shutdown of idle instances\n  * Utilizes Cloud Healthcare API for de‐identifying PHI data and Cloud Dataflow for bulk scanning"
      },
      "Chapter 7: Model Building": {
        "content": "1. C. The question talks about a future change (6 months later) in data distribution causing the model to perform poorly. This sounds like training‐serving skew or possibly prediction drift. The recommended best practice in either case is to monitor for it and retrain when necessary. `https://developers.google.com/machine‐learning/guides/rules‐of‐ml#training‐serving_skew`.\n  2. C. The model is already memorizing the training data, as seen from the good performance on training data but poor performance on validation data. Doubling the number of neurons will only make this worse, which rules out answer D. A 20% dropout would help the model generalize without drastically increasing training time. So, answer is C. Also L1 and L2 regularization will not help in this scenario.\n  3. B. The size of the images is too large to fit the GPUs. Changing batch size will help resolve the out of memory error.\n  4. D. In case of multiclass classification problems, we use sparse categorical cross‐entropy.\n  5. D. Oscillating loss curves indicate that the model is repeatedly overcorrecting. This is usually due to the learning rate being too high. Increasing the learning rate will only make this worse. See `https://developers.google.com/machine‐learning/testing‐debugging/metrics/interpretic#1.‐my‐model‐wont‐train`.\n  6. B. The image classification model is a deep learning model. You minimize the loss of deep learning models to get the best model. So comparing loss performance for each model on validation data is the correct answer.\n  7. B. In order to minimize the training time without impacting the accuracy, you need to modify the batch size of the model because the model will train faster due to increase in batch size per epoch. This will lead to less time to train the same model.\n  8. B. Since the data is one‐hot encoded, you will use categorical cross‐entropy.\n  9. B, D. Since the model is converging in training and not while testing, it is an overfitting problem, which can be resolved by regularizing it with L2.\n  10. C. Since there is a bias and variance trade‐off, you need to make sure that while training a model, both of the parameters are considered.\n  11. D, E. L1 is used for feature selection and k‐means is a clustering algorithm.\n  12. B. When you have limited data, you can use the data augmentation techniques covered in this chapter.\n  13. B. Sigmoid is the activation function used for binary classification problems.\n  14. B. This is an example of tuning hyperparameters.\n  15. A. MirroredStrategy uses a single machine. Refer to Table 7.1.",
        "subsections": {},
        "summary": "* **Concept 1**: Model performance degrades over time due to data distribution changes, requiring monitoring and retraining.\n\n* **Model Issue**: The model is memorizing training data, requiring dropout and regularization to improve generalization.\n    * _Using dropout (20%) can help the model generalize without increasing training time_\n    * _L2 regularization may also be beneficial_\n\n* **GPU Issue**: Image size is too large for GPUs, requiring a change in batch size to resolve out-of-memory error.\n\n* **Loss Function**: For multiclass classification problems, use sparse categorical cross-entropy.\n\n* **Overcorrection**: High learning rates can cause oscillating loss curves; consider using lower learning rates instead.\n\n* **Batch Size**: Increasing batch size can reduce training time without impacting accuracy.\n\n* **One-Hot Encoding**: Use categorical cross-entropy since data is one-hot encoded.\n\n* **Overfitting**: Convergence in training but not testing indicates overfitting, which can be resolved by L2 regularization.\n\n* **Bias-Variance Trade-off**: Consider both bias and variance when training a model to ensure optimal performance.\n\n* **L1/L2**: L1 is for feature selection, while k-means is a clustering algorithm."
      },
      "Chapter 8: Model Training and Hyperparameter Tuning": {
        "content": "1. A. In order to train TensorFlow code with less overhead from on‐premises to cloud, you can use the custom training option on Vertex AI.\n  2. A. Since the question asks for the least manual intervention and less computation time, BigQuery SQL is the easiest way to do that compared to other options.\n  3. C. In order to evaluate the model metric while the job is running, you need to enable an interactive shell.\n  4. A, B, C. With Vertex AI hyperparameter tuning, you can configure the number of trials and the search algorithm as well as range of parameters.\n  5. B. You need a Cloud Dataproc connector to transform the data from PySpark to Spark SQL.\n  6. A. Using a Vertex AI training custom container is the most managed way to set up training for any framework.\n  7. B. For a large dataset, you can directly use Bigtable to train a TensorFlow model using a Bigtable connector.\n  8. A. Pub/Sub with Cloud Dataflow is the most managed way of ingesting streaming data in Google Cloud.\n  9. A. From Pub/Sub, you can preprocess data in Dataflow and send it for ML training in Vertex AI, storing it in BigQuery. Store the results back into BigQuery after training and visualize in Data/Looker Studio.\n  10. C. You can directly access BigQuery tables using BigQuery magic in a Vertex AI notebook and then use the BigQuery client to convert it into a DataFrame.\n  11. B. From Pub/Sub you can preprocess data in Dataflow and send it for ML training in BigQuery ML. Store the results back into BigQuery after training and visualize in Data/Looker Studio.\n  12. A. Using an established text classification model, and to have full control on the code, you will use custom Vertex AI training.\n  13. A, B. You can use both an interactive shell and TF Profiler to track metrics.\n  14. A. In order to monitor performance, you need TensorFlow Profiler.\n  15. A. The question is asking for a cost‐effective approach. So the answer is option A.\n  16. B. The What‐If Tool helps visualize TF models for regression and classification and LIT is for NLP models.",
        "subsections": {},
        "summary": "* **Cloud AI Training Options**: \n    * _Vertex AI_: Custom training with managed setup, suitable for least manual intervention.\n    * *_BigQuery ML_*: Cost-effective approach for machine learning tasks.\n        * Can use Bigtable for large datasets and TF Profiler for performance monitoring.\n\n* **Ingesting Data**:\n    * Pub/Sub with Cloud Dataflow is the most managed way to ingest streaming data."
      },
      "Chapter 9: Model Explainability on Vertex AI": {
        "content": "1. B. Shapley values provide the informative, or important, features in a model.\n  2. B. For image data, integrated gradient is the preferred method.\n  3. A, B, and C. Vertex Explainable AI supports custom TensorFlow models and AutoML Tables and images.\n  4. A, B, E. Sampled Shapley, integrated gradients, and XRAI are three techniques for feature attribution.\n  5. A. Since this model is a nondifferentiable model, you can use sampled Shapley. Nondifferentiable models include nondifferentiable operations in the TensorFlow graph, such as operations that perform decoding and rounding tasks. To get feature attributions for nondifferentiable models, use the sampled Shapley method.\n  6. B, C. Both integrated gradients and XRAI are attribution techniques that are supported.\n  7. A. You can use local kernel explanations because the Explainable AI SDK is available in user‐managed notebooks.\n  8. A. Vertex example–based explanations can help with detecting misclassifications in the predictions.",
        "subsections": {},
        "summary": "* **Explainable AI**: Helps understand important features and model decisions\n    * Supports custom models, AutoML, and various attribution techniques (Integrated Gradient, Sampled Shapley, XRAI)\n    * Techniques vary by model type: nondifferentiable models use sampled Shapley, while differentiable models can use integrated gradients or XRAI"
      },
      "Chapter 10: Scaling Models in Production": {
        "content": "1. D. Since the features are dynamic and it is a low‐latency serving requirement, you would choose Bigtable for dynamic feature lookup. Moreover, you are also going to implement caching predictions in a datastore for a faster lookup.\n  2. A. You can use a BigQuery table for batch prediction with Vertex AI.\n  3. A. You can set up a notification with Pub/Sub and Cloud Function when your model predicts the user account balance drops below a certain threshold.\n  4. A. Pub/Sub to a Cloud Function for notification is the most suitable architecture.\n  5. A. Creating a daily batch prediction job will require minimal effort.\n  6. A. Creating a daily batch prediction job will require minimal effort using the schedule function in Vertex AI managed notebooks.\n  7. D. Since the question is asking about a solution with the least latency, you need to select a datastore, which will provide the least latency. Option D talks about using Cloud Bigtable for writing and reading the user navigation context. This is a classic architecture pattern discussed in topic online predictions.\n  8. A. The question is asking for a solution which requires the least effort to setup. Embed the client on the website, deploy the gateway on App Engine, and then deploy the model using Vertex AI Prediction.",
        "subsections": {},
        "summary": "* **Low-latency Solution**: Use Cloud Bigtable for dynamic feature lookup due to its low latency requirement\n    * Implement caching predictions in a datastore for faster lookups\n* **Batch Prediction with Minimal Effort**: \n    * Create a daily batch prediction job using schedule function in Vertex AI managed notebooks\n        * Requires minimal setup and effort\n* **Notification Architecture**:\n    * Pub/Sub to Cloud Function for notification is the most suitable architecture\n* **Efficient Architecture Pattern**: Use Cloud Bigtable for writing and reading user navigation context"
      },
      "Chapter 11: Designing ML Training Pipelines": {
        "content": "1. A. You can use the TFX Evaluator or TFX ModelValidator component to create performance benchmarks for a model in production. Evaluator performs deep analysis of the training results and helps you validate your exported models, ensuring that they are “good enough” to be pushed to production.\n  2. A. For the setup that is the most managed and requires the least effort, you would use event‐based Cloud Storage triggers to schedule Vertex AI Pipelines.\n  3. A. For the most managed setup that requires the least effort, you would use event‐based Cloud Storage triggers to schedule a Kubeflow Pipelines job on GKE.\n  4. A, D. You can load the Kubeflow BigQuery component URL `https://github.com/kubeflow/pipelines/blob/master/components/gcp/bigquery/query/sample.ipynb` and query BigQuery in Kubeflow Pipelines.\n  5. A, B. With TFX, you can use either Apache Airflow or Kubeflow to orchestrate the TensorFlow pipeline.\n  6. B. Setting up Kubeflow Pipelines using Vertex AI is the most managed way as it will require the least effort to set up because it's serverless.\n  7. C. Using Cloud Build, you can automate the testing of Kubeflow Pipelines.\n  8. A. You can set up Kubeflow Pipelines using GKE on‐premises with Anthos.",
        "subsections": {},
        "summary": "* **Kubeflow Pipelines Setup**\n  * Use event-based Cloud Storage triggers to schedule pipelines for minimal setup effort\n  * Alternatively, use Kubeflow Pipelines with GKE for a managed solution\n* **Orchestration Options**\n  * TFX supports orchestration with Apache Airflow or Kubeflow\n* **Automation and Testing**\n  * Cloud Build automates testing of Kubeflow Pipelines\n* **BigQuery Integration**\n  * Load the Kubeflow BigQuery component to query data in BigQuery"
      },
      "Chapter 12: Model Monitoring, Tracking, and Auditing Metadata": {
        "content": "1. B, D. Option A is wrong because it is very clear that the data has drifted away from the training data. This may or may not be a temporary problem. Option C is wrong because retraining a model with higher accuracy is not going to solve this problem because you have not updated the training data. Option B and then followed by option D is the right approach.\n  2. C. Option A is wrong; the performance of the model on test data is irrelevant after deployment because the input data might change. Option B is wrong because the initial assessment is not sufficient. Option D is wrong because, although monitoring costs money, it is not a reason to not monitor the model.\n  3. A, D. Data drift is when the distribution of the input data changes. Concept drift is when the relationship between the input and predicted value changes.\n  4. A. Data drift is when the distribution of the input data changes. Here the distribution “height” feature of the training data and the real data are two standard deviations apart, which is significant.\n  5. D. Concept drift is when the relationship between the input and predicted value changes. In this case, most likely the fraudsters have changed their modus operandi and are using techniques to evade detection. You have to collect new data and retrain.\n  6. C. This is the definition of training‐serving skew.\n  7. B. This is the definition of prediction drift.\n  8. A. Data drift is when the distribution of the input data changes. Here the number of plankton in training data and the real data are an order of magnitude different, which is significant.\n  9. A, D. For training‐serving skew, you need a statistical distribution of input features of the training data for reference, and then you compare that with the continuous statistical distribution of the inputs in production.\n  10. B, D. For prediction drift, you need a baseline statistical distribution of input features of the production data for reference, and then you compare that with the continuous statistical distribution of the inputs in production over time.\n  11. A. L‐infinity distance, or Chebyshev distance, is the greatest distance between two vectors.\n  12. B. Sampling rate is an option to tune to reduce the amount of data that is consumed by a service.\n  13. A, B, D. Option C is wrong because currently this is not a configuration; there is only one distance metric in Vertex AI.\n  14. C. If schema is not specified for custom models, the values may not be parsed properly. Option A is valid because AutoML models do not require schema. Option B is valid because custom models were the input is in key:value pairs are fine. Option D is valid because a schema is specified.\n  15. D. Options A and B are valid because string and number are supported. Option C is valid because you can define an array of strings or numbers. There is no data type called “category.”\n  16. B. While the wording “input logging” may sound intuitive, that is not a type of logging on Vertex AI. Container logging, access logging, and request‐response logging are valid.\n  17. D. Request‐response logging gives you a sample saved in a BigQuery table.\n  18. C. Options A, B, and D valid as they are the primary usage for a metadata store. Option C does not make sense here.\n  19. C. Option C is the definition of an artifact. Option A is wrong because not all information on a metadata store is an artifact. Option B is only partially true, because an artifact is not limited to the train and test dataset. Option D is the definition of _execution_.\n  20. B. Artifact, context, and execution are valid elements in the data model.",
        "subsections": {},
        "summary": "* **Data Drift**: The distribution of input data changes over time.\n* *_Concept Drift_*: The relationship between input and predicted value changes over time.\n* **Distance Metrics**: Used to measure differences between datasets, including L-infinity distance and Chebyshev distance."
      },
      "Chapter 13: Maintaining ML Solutions": {
        "content": "1. B. Option B is not part of MLOps. While it is a valid endeavor to look for other use cases to solve, it is not part of the MLOps workflow. Options A, C, and D are the major steps in MLOps.\n  2. B. This is an old organization that is just starting to explore and experiment with machine learning, so the best approach is to go with level 0. Option A is wrong because building ad hoc models is not a disciplined approach for any organization. Options C and D are for more mature situations.\n  3. C. Option A is wrong because ad hoc model building does not suit this level of maturity. Option B is wrong because MLOps level 0 is for organizations just experimenting and running proof of concepts. MLOps level 1 (strategic) is the right level for this organization. Also, since we are dealing with only one model, it fits this level. Option D is wrong because that is usually intended for organizations dealing with dozens or hundreds of models and experimenting with different algorithms and technologies.\n  4. D. Option A is wrong because ad hoc model building does not suit this level of maturity. Option B is wrong because MLOps level 0 is for organizations just experimenting and running proof of concepts. Option C is also wrong because MLOps level 1 (strategic) is usually for organizations that operate a few models. MLOps level 2 is the right level because they are dealing with many models in parallel and experimenting with different algorithms and technologies.\n  5. C. Option A is wrong because, although model training might be claimed as ad hoc, it does solve some problems. Option B is wrong because MLOps level 0 does not automate training. Option D is wrong because level 0 does not automate the whole process. Option C is factually correct.\n  6. D. Option D is not handled in MLOps level 1 but only in level 2. All other options, A, B and C are valid in level 1.\n  7. D. This organization is in an advanced stage of machine learning usage, and that level of maturity should also be reflected in the MLOps, so level 2 is the correct answer.\n  8. C. At this level, the data scientist experiments with the model and then creates a pipeline to generate the model. This pipeline code is submitted through a code repository, which will be orchestrated in the production environment.\n  9. A. At this level, the data scientist experiments with the model and then creates a model. This file is provided to the deployment team.\n  10. C. While they are important components, the Feature Store and Metadata Store do not trigger the training workflow. Also, a random trigger is not used. When a model degrades over time, as seen in monitoring data, it is triggered.\n  11. B, C. Retraining a model has infrastructure costs, and the more frequently a model is retrained, the more these costs will increase. So, options B and C are correct. The algorithm has no bearing on triggering retraining. Time to access data is not a factor in this at all.\n  12. B, D. When model monitoring is enabled, we evaluate the performance of the model at regular intervals. Retraining could be triggered if there is a sudden drop in performance or if performance degrades below a threshold. So, the correct answers are options B and D. The number of predictions or a security breach should not trigger retraining.\n  13. D. Generally, when you train/retrain a model that has the same inputs/outputs and is expected to be used in the same way as the previous version, to avoid disruption downstream, it is recommended to create a new version. Option A is wrong because if you a train a model that does completely different things, it shouldn’t be deployed as a new version of an existing model. Option B is wrong because, while uptrained models are usually deployed as new versions, it is not the only case. Option C is wrong because versioning of new models is a common design pattern.\n  14. B, C. When teams are siloed, they tend to create the same or similar features repeatedly and that creates redundancies. It is better to create the features once and store them for future use or even for others to use. Option A is wrong because the number of features one model uses is not a good reason for creating a featurestore. Similarly, option D is wrong because the number of types of inputs to a model is not a reason to use a featurestore.\n  15. C. Feast uses BigQuery, Redis, and Apache Beam. It does not use Gojek, which is a contributor to the Feast open source project.\n  16. A. Featurestore is the high‐level container and contains EntityType, which contains many Features.\n  17. A. Featurestore is the highest‐level container and contains EntityType, which can contain many Features.\n  18. D. Option A is wrong because you cannot do real‐time serving from BigQuery. Option B is not a good solution because it takes a long time to create a Feature Store. Option C would work, but since you have been given only a day and asked to move on to a different project, you should make it easy to maintain and hand off. So, a managed service is preferred here. Option D is the correct answer; Vertex AI Feature Store is a managed Feature Store that fits this use case best.\n  19. C. Vertex AI Feature Store cannot store images. All of the other statements are true.\n  20. A. Vertex AI Feature Store uses a time‐series model to store data. This helps it to retrieve historical data to help with consistency of downstream ML workflows.",
        "subsections": {},
        "summary": "* _MLOps Levels_\n    * Level 0: Organizations just experimenting and running proof of concepts\n    * Level 1 (Strategic): Organizations operating a few models, dealing with different algorithms and technologies\n    * Level 2: Advanced organizations dealing with many models in parallel and experimenting with new technologies"
      },
      "Chapter 14: BigQuery ML": {
        "content": "1. D. Option A is wrong because this is not a distributed use case (sales might be distributed, but data and compute are not). Option B is wrong because you are part of a data analyst team and may not have TensorFlow expertise. Option C is correct but is missing ARIMA_PLUS, the main algorithm for forecasting. Option D is better because you use ARIMA_PLUS and also leverage COVID‐19 public datasets.\n  2. C. Options A and D are wrong because you are part of a small team of data analysts and may not have expertise for custom models or TensorFlow models. The key point is that your manager wants it quickly. Also, the inputs, including map coordinates, are not relevant. Option B is wrong because there is no such feature on Google Maps today. Option C is the correct answer because BigQuery lets you create models with SQL and that can be done quickly.\n  3. C. The BigQuery ML matrix factorization model type is intended for this use. There is even a tutorial in the Google documentation to do this. Option A is not suited because recommendations are a special type of classification. Option B is technically correct but not suited here because the question specifies that you are a data analyst. Option D is wrong because there is no AutoML for matrix factorization today.\n  4. A. Option A is the correct answer because analysts prefer BigQuery and it is quick. Option B is technically possible but not the best suited solution for data analysts. Option C is factually incorrect. Option D is wrong because the question states that you don't want to create pipelines.\n  5. C. Options A and B are also possible, but creating pipelines is overkill for an “initial model.” Option A is also wrong because the next stage is usage by analysts who prefer BigQuery. Option D is also feasible but not the simplest and not elegant. Option C is a simple and elegant solution and works for analysts as well.\n  6. C. Option A is possible but downloading and then creating a Vertex AI dataset is unnecessary when you can directly import it from BigQuery. Options B and D are wrong because the question specifies that you have built extensive pipelines with AutoML Tables, so we don't have to disrupt that. Option C is the solution that fits this situation.\n  7. C. Option A is possible, but downloading and then creating a Vertex AI dataset is unnecessary when you can directly import it from BigQuery. Option B is wrong because the question specifies that you have built extensive pipelines with AutoML Tables, so you don't have to disrupt that. Option C is the solution that fits this situation. Option D is factually wrong. You _can_ access BigQuery public datasets from Vertex AI.\n  8. C. Option A is possible but unnecessarily exports out of BigQuery. Option B is the same as option A but is only automated with a pipeline job. Option D is not convenient for your team. Option C is correct because it effectively uses a nice feature of Vertex AI managed notebooks.\n  9. D. Option D is the best answer because it effectively uses a Vertex AI and BigQuery integration that solves this exact problem. Option A is wrong because it is less efficient. Generally avoid data transfers. Option B is wrong because moving models is not necessary here. Option C is also possible but then again involves a data transfer.\n  10. D. Option D is the best answer because it effectively uses a Vertex AI and BigQuery integration that solves this exact problem. Option C is similar to option D but less effective so not correct here. It is better to avoid data transfers if possible. Option B is wrong because moving models is not necessary here. Option A is also possible but then again involves a data transfer.\n  11. C. Option A is possible, but it involves a data transfer from BigQuery to GCS, which is unnecessary. Option B is wrong because retraining the model is redundant. Option D is wrong because this is the long route to solving this problem. Option C is the correct answer because it elegantly uses one of the six integrations of BigQuery with Vertex AI.\n  12. A. This is a common problem seen in datasets, and the design pattern is to use a hashing function. BigQuery has an elegant solution provided in option A. Option B is wrong because removing a feature is counterproductive. Option C is wrong because we can transform this feature to improve accuracy. Option D is wrong because, although Vertex AI has some transformations, it doesn’t have this level of advanced features engineering.\n  13. A. This is a common problem seen in datasets, and the design pattern is described in option A. Option B is wrong because these transformations are now separated from the model, which causes problems during prediction. Option C is wrong because we don't have to leave BigQuery for these simple transformations. Option D is wrong because although Vertex AI has some transformations, it does have this level of advanced feature engineering.\n  14. D. Option D is factually incorrect. Options A, B, and C are correct.\n  15. D. Option D is factually wrong. Options A, B, and C are factually correct.\n  16. B. Option B is the correct answer because you cannot port BigQuery models that have a TRANSFORM clause, which affects model portability. Regarding option D, Vertex AI pipelines has inbuilt components that support BigQuery operations.\n  17. B. The statement in option B is incorrect because you cannot port BigQuery models that have a TRANSFORM clause. The other statements are correct.\n  18. D. The statement in option D is incorrect. The other statements are true.\n  19. A. The statement in option A is incorrect because you have to enable explanations while training to get explanations. The other statements are true.\n  20. D. Options A and B are wrong because this is a data analyst team and BigQuery is better suited for them. Option C is wrong because Wide‐and‐Deep models are used for recommending engine type problems, not forecasting. Option D is the best fit.",
        "subsections": {},
        "summary": "* **Concept 1**: The correct approach to using ARIMA_PLUS for forecasting is by leveraging COVID-19 public datasets.\n    * Using ARIMA Plus effectively involves utilizing BigQuery and public datasets for improved accuracy.\n    * Avoiding custom models or TensorFlow expertise, as well as unnecessary data transfers, are key considerations in this context.\n\n* **Concept 2**: The best solution for using a hashing function to improve dataset design is option A, which provides an elegant BigQuery-based solution.\n    * Removing features and leaving transformations outside of the model can cause prediction issues.\n    * Advanced feature engineering requires the level of capabilities offered by options A, B, and C.\n\n* **Concept 3**: Vertex AI's managed notebooks provide a convenient solution for integrating with BigQuery.\n    * Using option D is preferred because it effectively uses the integration to solve the problem without unnecessary data transfers or model retraining.\n\n* **Concept 4**: The best approach for porting models between platforms is by utilizing BigQuery and Vertex AI integrations.\n    * Options A, B, and C are factually correct in their descriptions of these integrations.\n\n* **Concept 5**: The feature engineering process can be complex, but using options that fit within the platform's capabilities can simplify it.\n    * Using option D is preferred because it elegantly uses one of BigQuery's six integrations with Vertex AI.\n\n* **Concept 6**: The best solution for integrating data models between platforms involves using features that are native to each platform.\n    * Option B is correct in stating that you cannot port BigQuery models with TRANSFORM clauses, but other options may still be viable.\n\n* **Concept 7**: The feature engineering process can be complex, but using features that fit within the platform's capabilities can simplify it.\n    * Option D is factually incorrect regarding the capability to access BigQuery public datasets from Vertex AI.\n\n* **Concept 8**: The best approach for model deployment involves utilizing the platform's native features.\n    * Using option C effectively uses a nice feature of Vertex AI managed notebooks without unnecessary data transfers or retraining models.\n\n* **Concept 9**: The most efficient solution is to use the platform's native integrations and capabilities.\n    * Option D is preferred because it effectively uses a Vertex AI and BigQuery integration that solves the problem, avoiding unnecessary data transfers.\n\n* **Concept 10**: The best approach for model deployment involves utilizing the platform's native features and integrations.\n    * Option C is similar to option D but less effective; option A is also possible without data transfer, making it more efficient.\n\n* **Concept 11**: The most efficient solution is to utilize BigQuery's integration capabilities with Vertex AI.\n    * Using option D elegantly uses one of BigQuery's integrations with Vertex AI, avoiding unnecessary data transfers or retraining models.\n\n* **Concept 12**: The feature engineering process can be complex, but using options that fit within the platform's capabilities simplifies it.\n    * Option A provides an elegant solution for this problem by utilizing a hashing function.\n\n* **Concept 13**: The feature engineering process can be complex, but using features that fit within the platform's capabilities can simplify it.\n    * Options A, B, and C are correct in describing the design pattern for handling this type of issue in BigQuery datasets.\n\n* **Concept 14**: The best solution for this problem involves utilizing BigQuery's native feature engineering capabilities.\n    * Option D is factually incorrect regarding this aspect.\n\n* **Concept 15**: The most efficient solution is to use the platform's native features and integrations.\n    * Options A, B, and C are correct in describing how to address this issue effectively.\n\n* **Concept 16**: The best approach for model deployment involves utilizing BigQuery's native capabilities.\n    * Option B states that you cannot port BigQuery models with TRANSFORM clauses, making it a viable solution.\n\n* **Concept 17**: The feature engineering process can be complex, but using options that fit within the platform's capabilities simplifies it.\n    * Options A and D are incorrect in their statements regarding model portability.\n\n* **Concept 18**: This statement is factually incorrect; other options are true.\n    * Option D states something that is not true about BigQuery public datasets access from Vertex AI.\n\n* **Concept 19**: The feature engineering process can be complex, but using features that fit within the platform's capabilities simplifies it.\n    * Options A and B are wrong due to incorrect statements regarding enabling explanations during training.\n\n* **Concept 20**: The most efficient solution is to use BigQuery for forecasting and recommend Wide-and-Deep models instead of ARIMA Plus.\n    * Option D is the best fit because it effectively utilizes a Vertex AI and BigQuery integration that solves this problem."
      }
    },
    "summary": ""
  },
  "Index": {
    "content": "",
    "subsections": {
      "A": {
        "content": "* access logging, 247\n  * AdaGrad optimization, 42\n  * Adam optimization, 42\n  * AEAD (Authenticated Encryption with Associated Data), 104\n  * AI (artificial intelligence)\n    * best practices, 13–14\n    * fairness, 13\n    * interpretability, 13\n    * intro, 2\n    * model explanations, 13\n    * privacy, 13\n    * security, 14\n\n  * AI/ML stack, 86\n  * ANNs (artificial neural networks), 126\n  * Apache Airflow, 228–229\n  * Apache Beam, 28\n  * asynchronous predictions, 95–96\n  * asynchronous training, 123\n  * AUC (area under the curve), 11\n  * AUC ROC (Area Under the Curve Receiver Operating Characteristic), 11, 46\n  * AUC‐PR curve, 12, 46\n  * augmentation on the fly, 132\n  * automation, 91\n    * review question answers, 304–305\n\n  * AutoML\n    * CCAI (Contact Center AI), 69–70\n    * compared to others, 58–60\n    * Dialogflow, 69–70\n    * Document AI, 69\n    * images, 66–67\n    * _versus_ others, 87\n    * Retail AI, 68\n    * review question answers, 302–304\n    * tables/structured data\n      * BigQuery ML, 64\n      * Vertex AI tables, 64–65\n    * text, 67\n    * Vertex AI, 60\n    * video, 66–67",
        "subsections": {},
        "summary": "* **Access Logging**\n*   `_What:_` Logging and monitoring system access to data and models\n*   `_Key Concepts:__\n    * AdaGrad optimization\n    * Adam optimization\n    * AEAD (Authenticated Encryption with Associated Data)\n    * AI/ML stack components\n    * _best practices for AI & ML security_"
      },
      "B": {
        "content": "* bar plots, 21–22\n  * batch data, collecting, 146–147\n  * batch prediction, 94\n    * input data, 212–213\n\n  * BigQuery, 28, 64, 87, 88\n    * Dataproc and, 148\n    * encryption and, 104\n    * Jupyter Notebooks, 280–281\n    * _versus_ others, 87\n    * Pub/Sub and, 146\n    * Python API, 281\n    * SQL queries, 280\n    * tools for reading data, 88\n\n  * BigQuery Data Transfer Service, 146\n    * BigQuery integration, 155–156\n\n  * BigQuery ML\n    * data\n      * import to Vertex AI, 290\n      * test prediction, 290\n      * Vertex AI Workbench Notebooks, 290\n    * design\n      * hashed feature, 291\n      * transforms, 291–292\n    * DNNs (deep neural networks), 283\n    * explainability in, 286–288\n    * export to Vertex AI, 291\n    * Jupyter Notebooks, 285\n    * ML.evaluate keyword, 284\n    * model creation, 282\n    * model training, 282–284\n    * models, 283\n    * predictions, 285–286\n    * public dataset access, 289\n    * review question answers, 313–314\n    * Vertex AI, prediction results export, 290\n    * Vertex AI tables comparison, 289\n\n  * BigQuery Omni, 235\n  * BigQuery REST API, 88\n  * BigQuery Spark connector, 149\n  * Bigtable, 90–91\n  * binary classification, 7, 9\n  * bivariate analysis, data visualization, 20\n  * bucketing, 42",
        "subsections": {},
        "summary": "* **BigQuery and Data Visualization**: summary of key points about using BigQuery for data visualization, including bivariate analysis and data visualization techniques.\n    * _Data Visualization Techniques_\n      * *_Visualizing Bivariate Data_*: plots like bar plots to display relationships between variables\n      * *_Data Visualization Tools_*:\n        * *Jupyter Notebooks*\n        * *SQL queries*\n\n* **BigQuery ML**: summary of key points about using BigQuery for machine learning, including design and model training.\n    * *_Model Training_*\n      * *_Hashed Features_*: used in DNNs\n      * *_Transformations_*: applied to data before training\n    * *_Exporting Models_*: to Vertex AI Workbench Notebooks"
      },
      "C": {
        "content": "* caching architecture, 206\n  * categorical data, 41\n  * categorical values, mapping\n    * embedding, 44\n    * feature hashing, 44\n    * hybrid of hashing and vocabulary, 44\n    * integer encoding, 43\n    * label encoding, 43\n    * one‐hot encoding, 43\n    * OOV (out of vocab), 43\n\n  * CCAI (Contact Center AI), 69–70\n  * CI/CD pipeline, 230\n  * class imbalance, 44–45\n    * AUC ROC (Area Under the Curve Receiver Operating Characteristic), 46\n    * classification threshold, 45\n    * false negative, 45\n    * false positive, 45\n    * true negative, 45\n    * true positive, 44\n\n  * classification, 7\n    * binary, 7, 9\n    * multiclass, 7\n    * prediction classes, 9–10\n\n  * classification threshold, 45\n  * classification threshold invariance, 11\n  * client‐side encryption, 105\n  * clipping, 26\n  * Cloud Bigtable, 149\n  * Cloud Build, 215\n  * Cloud Composer, 149, 229\n  * Cloud Data Fusion, 51, 148\n  * Cloud Dataflow, 147–148\n  * Cloud Dataprep, 149\n  * Cloud Dataproc, 148–149\n    * BigQuery, 148\n    * BigQuery Spark connect, 149\n    * Cloud Bigtable, 149\n    * Pub/Sub Lite Spark, 149\n\n  * Cloud Run, 215\n  * Cloud Scheduler, 215\n  * Cloud Storage, Dataproc and, 148\n  * clustering, 8\n  * CNNs (convolutional neural networks), 126–127\n  * concept drift, 178, 242\n  * confusion matrix, 9\n  * container logging, 247\n  * containers\n    * custom, model training, 166–168\n    * prebuilt, model training and, 163–165\n\n  * correlation\n    * negative, 24\n    * positive, 24\n    * zero, 24\n\n  * CT (continuous training) pipeline, 230\n  * custom ML models\n    * compared to others, 58–60\n    * CPU, 71\n    * GPUs (graphics processing units), 70\n      * ALUs (arithmetic logic units), 71\n      * restrictions, 71–72\n      * virtual CPUs, 72\n    * _versus_ others, 87\n    * TPUs (Tensor Processing Units), 72–73\n      * advantages, 73\n      * Cloud TPU model, 74",
        "subsections": {},
        "summary": "* **Caching Architecture**: \n    * Mapping of categorical values: embedding, feature hashing, one-hot encoding\n    * Techniques for imbalanced data: AUC ROC, classification threshold, true positives/negatives\n\n* **Cloud Services**:\n    * BigQuery, Cloud Data Fusion, Cloud Dataflow\n    * Cloud Composer, Cloud Dataproc, Cloud Run\n        * Custom container logging and ML models with GPUs/TPUs\n        * Comparison of hardware: CPU, GPU, TPU"
      },
      "D": {
        "content": "* DAG (directed acyclic graph), 93\n  * data\n    * missing, 32\n    * review question answers, 301–302\n    * semi‐structured, model training and, 145\n    * structured\n      * modeling training and, 145\n      * regression and, 7\n    * unstructured, model training and, 145\n\n  * data augmentation, 132\n    * augmentation on the fly, 132\n\n  * data cleaning, 25\n  * data collection\n    * batch data, 146–147\n    * BigQuery, 88\n    * Bigtable, 90–91\n    * Cloud Composer, 149\n    * Cloud Data Fusion, 148\n    * Cloud Dataflow, 147–148\n    * Cloud Dataprep, 149\n    * Cloud Dataproc, 148–149\n    * Datastore, 90–91\n    * GCS (Google Cloud Storage), 88\n    * Memorystore, 90–91\n    * model training, 146–147\n    * NoSQL data store, 90–91\n    * privacy implications, 113–117\n    * review question answers, 304–305\n    * sensitive data removal, 116–117\n    * streaming data, 146–147\n    * Vertex AI, managed datasets, 89\n    * Vertex AI Feature Store, 89\n\n  * data compatibility. _See_ data transformation\n  * data constraints, validation, 27–28\n  * data drift, 178, 243\n  * data lakes, 28\n  * data leakage, 33–34\n  * data parallelism, 122\n    * asynchronous training, 123\n    * synchronous training, 123\n\n  * data quality. _See also_ data transformation; quality of data\n  * data reliability. _See_ reliability of data\n  * data sampling, 29\n  * data skew, 25\n  * data splitting, 31\n    * online systems, 31\n\n  * data transformation\n    * bucketing, 42\n    * Cloud Data Fusion, 51\n    * data compatibility and, 40\n    * data quality and, 40\n    * Dataprep by Trifacta, 51\n    * dimensionality, 44\n    * feature selection, 44\n    * inside models, 41\n    * mapping categorical values\n      * embedding, 44\n      * feature hashing, 44\n      * hybrid of hashing and vocabulary, 44\n      * integer encoding, 43\n      * label encoding, 43\n      * one‐hot encoding, 43\n      * OOV (out of vocab), 43\n    * mapping numeric values\n      * bucketing, 42\n      * normalizing, 42\n    * normalizing, 42\n    * pretraining, 40–41\n    * structured data\n      * categorical data, 41\n      * numeric data, 41\n    * TensorFlow Transform\n      * library, 49–51\n      * tf.data API, 49\n      * TFX (TensorFlow Extended), 49–51\n\n  * data validation\n    * TFDV (TensorFlow Data Validation), 27–28, 272\n    * TFX (TensorFlow Extended) platform, 27–28\n\n  * data visualization\n    * bar plots, 21–22\n    * bivariate analysis, 20\n    * box plots\n      * outliers, 20–21\n      * quartiles, 20–21\n      * whiskers, 20–21\n    * line plots, 21\n    * scatterplots, 22\n    * univariate analysis, 20\n\n  * Dataflow, Pub/Sub and, 146\n  * Dataprep by Trifacta, 51\n  * datasets\n    * imbalanced data, 29–31\n    * model training, Vertex AI, 163\n    * review question answers, 310–311\n    * sampling\n      * oversampling, 29\n      * undersampling, 29\n    * splitting data, 31\n    * test datasets, 29\n    * training datasets, 29\n    * validation datasets, 29\n    * Vertex AI managed datasets, 89\n\n  * Datastore, 90–91\n  * Datastream, 146\n  * debugging, Vertex AI, 272\n  * development workflow, 223\n  * Dialogflow, 69\n    * Agent Assist, 70\n    * CCAI, 70\n    * insights, 70\n    * virtual agent, 70\n\n  * DICOM (Digital Imaging and Communications in Medicine), 116\n  * distributed training, model training, 168–169\n  * DLP (Data Loss Prevention) API, 104, 114–115\n  * DNNs (deep neural networks), 126\n    * BigQuery ML, 283\n\n  * Docker images, custom containers, 166–168\n  * Document AI, 69\n  * DP (differential privacy), Vertex AI and, 112\n  * dynamic reference features, 203–204\n    * architecture, 205",
        "subsections": {},
        "summary": "* **Data Management**\n  * _Data Quality_: ensuring accuracy, completeness, and consistency of data.\n  * _Data Validation_: checking data for errors, inconsistencies, and compliance with rules.\n  * _Data Transformation_: preparing data for analysis by converting formats, handling missing values.\n\n* **Model Training and Deployment**\n  * _Distributed Training_: training models on large datasets across multiple machines.\n  * _Vertex AI_: a cloud-based platform for building, deploying, and managing machine learning models.\n  * _BigQuery ML_: a service for building, training, and deploying machine learning models in BigQuery."
      },
      "E": {
        "content": "* EDA (exploratory data analysis), 20\n    * visualization\n      * bar plots, 21–22\n      * bivariate analysis, 20\n      * box plots, 20–21\n      * line plots, 21\n      * scatterplots, 22\n      * univariate analysis, 20\n\n  * edge inference, 76\n  * Edge TPU, 76\n  * embedding, 44\n  * encryption\n    * BigQuery and, 104\n    * client‐side, 105\n    * FPE (Format‐Preserving Encryption), 113\n    * at rest, 104–105\n    * server‐side, 105\n    * tokenization, 113\n    * in transit, 105\n    * in use, 105\n\n  * explainability. _See also_ Vertex Explainable AI\n    * BigQuery ML, 286–288\n    * global, 188\n    * local, 188",
        "subsections": {},
        "summary": "* **Exploratory Data Analysis**: \n  * Visualization techniques: bar plots, bivariate analysis, box plots, line plots, scatterplots, and univariate analysis\n  * Edge inference and edge computing concepts (Edge TPU)\n* **Encryption and Explainability**:\n  * Encryption methods: at rest, client-side, FPE, global, in transit, in use, tokenization, BigQuery integration\n  * Explainability techniques: BigQuery ML, local explainability"
      },
      "F": {
        "content": "* false negative, 45\n  * false positive, 45\n  * feature crosses, 46–48\n    * feature columns, 48\n\n  * feature engineering, 40\n    * class imbalance\n      * AUC ROC, 46\n      * classification threshold, 45\n      * false negative, 45\n      * false positive, 45\n      * true negative, 45\n      * true positive, 44\n    * data preprocessing, 40–41\n    * data transformation\n      * Cloud Data Fusion, 51\n      * data compatibility and, 40\n      * data quality and, 40\n      * Dataprep by Trifacta, 51\n      * inside models, 41\n      * pretraining, 40–41\n      * TensorFlow Transform, 49–51\n      * TFX (TensorFlow Extended), 49\n    * feature crosses, 46–48\n      * feature columns, 48\n    * predictions, 74–75\n      * deploy to Android, 76\n      * deploy to iOS devices, 76\n      * Edge TPU, 76\n      * machine types, 75–76\n      * ML Kit, 76\n      * scaling, 75\n    * review question answers, 302\n\n  * feature hashing, 44\n  * feature importance, 189\n  * federated learning, Vertex AI and, 112\n  * FHIR (Fast Healthcare Interoperability Resources), 116\n  * forecasting, 8\n  * FPE (Format‐Preserving Encryption), 113",
        "subsections": {},
        "summary": "* **Technical Terms**:\n    * False negative: 45\n    * False positive: 45\n    * Feature columns: 48\n    * Feature engineering: 40\n        * Class imbalance: \n            + AUC ROC: 46\n            + Classification threshold: 45\n        * Data preprocessing: 40–41\n* **Data Preprocessing**:\n    * Cloud Data Fusion: 51\n    * Dataprep by Trifacta: 51\n* **Predictions and Deployment**:\n    * Deploy to Android and iOS devices: 76\n    * Edge TPU: 76\n    * Machine types: 75–76\n    * ML Kit: 76"
      },
      "G": {
        "content": "* GANs (generative adversarial networks), 132\n  * GCP AI APIs, 235\n  * GCS (Google Cloud Storage), 88\n  * Github integration, 156–157\n  * GNMT (Google Neural Machine Translation), 63\n  * Google Cloud Healthcare API, 115–116\n    * DICOM (Digital Imaging and Communications in Medicine), 116\n    * FHIR (Fast Healthcare Interoperability Resources), 116\n\n  * GPUs (graphics processing units), 70\n    * ALUs (arithmetic logic units), 71\n    * model parallelism, 124–125\n    * restrictions, 71–72\n    * virtual CPUs, 72\n\n  * gradient descent, 128",
        "subsections": {},
        "summary": "* **Technical Topics**: \n  * GANs, GCP AI APIs, GCS, Github integration, GNMT, Google Cloud Healthcare API, DICOM, FHIR, GPUs (with details on ALUs, model parallelism, and restrictions)\n  * Gradient Descent \n    * Used for training models\n    * Optimizes parameters to minimize loss"
      },
      "H": {
        "content": "* host third‐party pipelines (MLFlow) on Google Cloud, 213–214\n  * hybrid cloud strategies, 235–236\n  * hybrid of hashing and vocabulary, 44\n  * hyperparameter tuning\n    * algorithm options, 170\n    * Bayesian search, 170\n    * grid search, 170\n    * importance, 170–171\n    * optimization speed, 171\n    * parameter comparison, 169\n    * random search, 170\n    * review question answers, 307–308\n    * Vertex AI, 171–174\n      * Vertex AI Vizier, 174",
        "subsections": {},
        "summary": "* **Hosting MLFlow on Google Cloud**\n  * Hosts third-party pipelines for machine learning\n  * Supports hybrid cloud strategies\n  * Integrates with hyperparameter tuning tools including *_Vertex AI_* \n    * Provides algorithm options and optimization speed features"
      },
      "I": {
        "content": "* IAM (identity and access management), 104\n    * FPE (Format‐Preserving Encryption), 113\n    * project‐level roles, 105\n    * resource‐level roles, 105\n    * Vertex AI and, 106\n    * federated learning, 112\n    * Vertex AI Workbench permissions, 106–108\n\n  * infrastructure, 86\n    * review question answers, 302–304\n\n  * inside model data transformation, 41\n  * integer encoding, 43\n  * interactive shells, 175–176",
        "subsections": {},
        "summary": "* **Identity and Access Management**: IAM system for controlling user access to resources.\n    * _Federated Learning_: machine learning approach that allows multiple parties to collaborate without sharing sensitive data.\n    * _Project-level Roles_ and _Resource-level Roles_: defining permissions at different levels for projects and resources.\n    * _Vertex AI Workbench Permissions_: managing access to Vertex AI tools.\n\n* **Infrastructure**: overview of infrastructure components.\n    * _Review Question Answers_: answers to review questions related to infrastructure."
      },
      "J": {
        "content": "* JupyterLab features, 154–155",
        "subsections": {},
        "summary": "* JupyterLab features, 154–155"
      },
      "K": {
        "content": "* k‐NN (k‐nearest neighbors) algorithm, missing data, 32\n  * Kubeflow DSL, system design, 232–233\n    * pipeline components, 233\n\n  * Kubeflow Pipelines, 92–93, 224–225, 229\n    * workflow scheduling, 230–232\n\n  * Kubernetes Engine, 87",
        "subsections": {},
        "summary": "* **K‐NN Algorithm**: \n    * **Missing Data Handling**: \n        * _Kubeflow DSL_: Pipeline components design (233)\n        * Kubeflow Pipelines: Workflow scheduling (230-232)\n        * Kubernetes Engine: _No specific mention_"
      },
      "L": {
        "content": "* label encoding, 43\n  * latency, online prediction, 96\n  * line plots, 21\n  * lineage tracking, metadata, 252\n  * LOCF (last observation carried forward), 32\n  * log scaling, 26\n  * logging\n    * log settings, 248\n    * model monitoring and, 248\n    * prediction logs\n      * access logging, 247\n      * container logging, 247\n      * request‐response logging, 248\n    * request‐response logging, 248\n    * review question answers, 310–311\n\n  * loss functions, 127–128\n  * LSTMs (long short‐term memory), 127",
        "subsections": {},
        "summary": "### Key Concepts\n\n* **Label Encoding**: \n  * _Latency_, online prediction\n  * _Line Plots_\n  * Lineage tracking and metadata\n  * LOCF (last observation carried forward)\n  * Log scaling\n\n* **Logging**\n  * Logging settings, model monitoring, and prediction logs\n    + Access logging, container logging, request-response logging\n  * Loss functions and LSTMs (_Long Short-Term Memory_)"
      },
      "M": {
        "content": "* machine types, 75\n    * QPS (queries per second), 75\n    * restrictions, 75–76\n\n  * MAE (mean absolute error), 12\n  * managed datasets, 89\n  * managed notebook, Vertex AI Workbench\n    * BigQuery integration, 155–156\n    * creation, 153–154\n    * data integration, 155\n    * Github integration, 156–157\n    * JupyterLab features, 154–155\n    * scaling up, 156, 157\n    * scheduling or executing code, 158–159\n    * _vs._ user‐managed notebooks, 152–153\n\n  * mapping categorical values\n    * embedding, 44\n    * feature hashing, 44\n    * hybrid of hashing and vocabulary, 44\n    * integer encoding, 43\n    * label encoding, 43\n    * one‐hot encoding, 43\n    * OOV (out of vocab), 43\n\n  * mapping numeric values, 42\n  * mean, 22\n    * skewed data, 25\n    * variance, 23\n\n  * Media Translation API, 63\n  * median, 22\n    * skewed data, 25\n\n  * Memorystore, 90–91\n  * metadata\n    * lineage tracking, 252\n    * review question answers, 310–311\n    * Vertex ML, 249–250\n      * artifacts, 249\n      * context, 249\n      * events, 250\n      * execution, 249\n      * metadataschema, 250\n      * schema management, 250–252\n      * Vertex AI Pipelines, 252\n\n  * metrics, model training\n    * interactive shells, 175–176\n    * TensorFlow Profiler, 177\n    * WIT (What‐If Tool), 177–178\n\n  * missing data, 32\n  * ML (machine learning), 2\n    * business use cases, 3–4\n    * classification, 7\n    * clustering, 8\n    * forecasting, 8\n    * problem types, 6\n    * problems, review question answers, 300–301\n    * regression, 7\n    * semi‐supervised learning, 7\n    * supervised learning, 6\n    * unsupervised learning, 6\n      * topic modeling, 6–7\n\n  * ML Kit, 76–77\n  * ML metrics, 8–9\n    * AUC ROC (Area Under the Curve Receiver Operating Characteristic), 11\n    * AUC‐PR curve, 12\n    * regression\n      * MAE (mean absolute error), 12\n      * RMSE (root‐mean‐squared error), 12\n      * RMSLE (root‐mean‐squared logarithmic error), 12\n    * summary, 10\n\n  * ML models\n    * AutoML\n      * CCAI (Contact Center AI), 69–70\n      * compared to others, 58–60\n      * Dialogflow, 69–70\n      * Document AI, 69\n      * images, 66–67\n      * Retail AI, 68\n      * structured data, 64–65\n      * tables, 64–65\n      * text, 67\n      * video, 66–67\n    * custom\n      * compared to others, 58–60\n      * CPU, 71\n      * GPUs (graphics processing units), 70–72\n      * TPUs (Tensor Processing Units), 72–74\n    * pretrained, 60\n      * compared to others, 58–60\n      * Natural Language AI, 62–63\n      * Speech‐to‐Text service, 63\n      * Text‐to‐Speech service, 64\n      * Translation AI, 63\n      * Video AI, 62\n      * Vision AI, 61–62\n    * review question answers, 302–304\n\n  * ML workflow, Google Cloud services, 85\n  * MLOps (machine learning operations), 222, 260–261\n    * data\n      * analysis, 261\n      * extraction, 261\n      * preparation, 261\n    * deployment\n      * monitor, 261\n      * serving, 261\n    * Level 0 Manual/Tactical phase, 261–263\n    * Level 1 Strategic Automation phase, 263–264\n    * Level 2 CI/CD Automation, Transformational phase, 264–266\n    * models\n      * evaluation, 261\n      * training, 261\n      * validation, 261\n\n  * mode, 23\n  * model building\n    * ANNs (artificial neural networks), 126\n    * batches, 129\n      * size, 129\n      * tuning batch size, 129–130\n    * bias, 133\n      * variance trade‐off, 133\n    * CNNs (convolutional neural networks), 126–127\n    * data parallelism, 122\n      * asynchronous training, 123\n      * synchronous training, 123\n    * DNNs (deep neural networks), 126\n    * epoch, 129\n    * gradient descent, 128\n    * hyperparameters, 129\n    * learning rate, 129\n      * tuning, 130\n    * loss functions, 127–128\n    * model parallelism, 123–125\n    * overfitting, 134\n    * regularization, 134–136\n      * dropout, 136\n      * exploding gradients, 135\n      * L1, 135\n      * L2, 135\n      * losses, 136\n      * ReLU units, dead, 135\n      * vanishing gradients, 135–136\n    * review question answers, 306\n    * RNNs (recurrent neural networks), 127\n    * step size, 129\n    * underfitting, 133–134\n    * variance, 133\n      * bias variance trade‐off, 133\n\n  * model deployment, 207–209\n  * model monitoring, 242\n    * concept drift, 242–243\n    * data drift, 243\n    * review question answers, 310–311\n    * Vertex AI, 243–244\n      * drift, 244–245\n      * input schemas, 245–247\n      * skew, 244–245\n\n  * Model Registry, 209\n  * model retraining, 266–267\n  * model servers\n    * deployment, 200\n    * serving time errors, 271\n    * TensorFlow, 200\n\n  * model training\n    * algorithmic correctness, testing for, 180\n    * AutoML, 161\n    * BigQuery ML, 282–284\n    * custom, 162\n    * custom containers, 166–168\n    * data\n      * semi‐structured, 145\n      * structured, 145\n      * unstructured, 145\n    * data analysis, 150–151\n    * data collection, 146–147\n      * Cloud Composer, 149\n      * Cloud Dataprep, 149\n    * data storage, 150–151\n    * datasets, Vertex AI and, 163\n    * distributed training, 168–169\n    * metrics\n      * interactive shells, 175–176\n      * TensorFlow Profiler, 177\n      * WIT (What‐If Tool), 177–178\n    * MLOps (machine learning operations), 222\n    * new data and, 230\n    * prebuilt containers, 163–165\n    * review question answers, 307–308\n    * training time errors, 271\n    * unit testing, 179\n    * updates to API call, 180\n    * Vertex AI Workbench, 109–110\n      * managed notebook, 151–159\n      * user‐managed notebook, 151–153, 159–161\n    * workflow\n      * custom jobs, 162\n      * hyperparameter tuning jobs, 162\n      * training pipelines, 162\n      * Vertex AI and, 162\n\n  * model versioning, 267–268\n  * models, testing, performance, 214–215\n  * multiclass classification, 7\n  * multicloud strategies, 235–236",
        "subsections": {},
        "summary": "* **Machine Learning Overview**: \n    * _ML (machine learning)_, defined as \"subfield of artificial intelligence\" that focuses on building algorithms to enable machines to learn from data.\n        * Business use cases: predictive maintenance, customer segmentation\n        * Types: supervised/unsupervised, classification/regression, clustering\n* **Model Building**:\n    * Custom models, built using popular frameworks like TensorFlow or PyTorch, leveraging GPU acceleration for faster training times\n        * Hyperparameters tuning, learning rate schedules, regularization techniques (e.g., dropout, L1/L2)\n    * Prebuilt containers, providing a streamlined experience for common tasks and datasets\n* **Model Deployment**:\n    * Model Registry: central hub for managing models, tracking versions, and sharing knowledge across teams\n        * AutoML capabilities enable effortless model development, without requiring extensive machine learning expertise\n    * Cloud-based infrastructure: scalable resources for large-scale deployment and inference"
      },
      "N": {
        "content": "* Naive Bayes, missing data, 32\n  * NaN data error, 42\n  * NaN values, 26, 32\n  * Natural Language AI, 62–63\n  * negative correlation, 24\n  * neural networks\n    * ANNs (artificial neural networks), 126\n    * CNNs (convolutional neural networks), 126–127\n    * data augmentation, 132\n    * DNNs (deep neural networks), 126\n    * RNNs (recurrent neural networks), 127\n\n  * normalization, 42\n  * NoSQL data store, 90–91\n  * numeric data, 41\n  * numeric values, mapping\n    * bucketing, 42\n    * normalizing, 42",
        "subsections": {},
        "summary": "* **Naive Bayes** \n  * Handles missing data with NaN errors and normalization techniques.\n  * Used in Natural Language AI applications.\n\n* **Neural Networks**\n  * Include:\n    * Artificial Neural Networks (ANNs)\n    * Convolutional Neural Networks (CNNs)\n    * Deep Neural Networks (DNNs)\n    * Recurrent Neural Networks (RNNs)"
      },
      "O": {
        "content": "* offline data augmentation, 132\n  * offline prediction, 94\n  * one‐hot encoding, 43\n  * online data augmentation, 132\n  * online prediction\n    * asynchronous\n      * poll notifications, 95–96\n      * push notifications, 95–96\n    * endpoint setup, 207\n    * latency, 96\n    * making predictions, 210\n    * model deployment, 207–209\n    * synchronous, 95\n\n  * online predictions\n    * endpoints, undeploy, 211\n    * explanation requests, 212\n\n  * OOV (out of vocab), 43\n  * optimization, hyperparameters, 159\n  * orchestration, 91\n    * frameworks, 223\n      * Apache Airflow, 228–229\n      * Cloud Composer, 229\n      * Kubeflow Pipelines, 224–225, 229\n      * Vertex AI Pipelines, 225–229\n    * review question answers, 304–305\n    * TensorFlow, 92\n    * Vertex AI pipelines, 92\n\n  * orchestrators, 94\n  * outliers\n    * clipping, 26\n    * detecting, 23\n    * handling, 26–27",
        "subsections": {},
        "summary": "* **Offline Data Augmentation**: techniques used to artificially increase dataset size before model training\n  * _One-Hot Encoding_: converts categorical data into numerical format\n* \n* **Online Prediction**:\n  * _Asynchronous and Synchronous Endpoints_: approaches for making predictions\n  * _Model Deployment_: process of deploying models in production environment"
      },
      "P": {
        "content": "* parameters, hyperparameter comparison, 169\n  * PCA (principal component analysis), 44\n  * performance testing, 214–215\n  * PHI (protected health information), 104, 113–116, 118\n  * PII (personally identifiable information), 104, 113–115, 118\n    * DLP (Data Loss Prevention), 114–115\n    * Google Cloud Healthcare API, 115–116\n\n  * pipelines\n    * Apache Airflow and, 228–229\n    * artifact lineage, 226\n    * artifacts, 226\n    * CI/CD pipeline, 230\n    * Cloud Composer and, 229\n    * CT (continuous training), 230\n    * functionalities, 222\n    * Kubeflow, 92–93\n      * workflow scheduling, 230–232\n    * Kubeflow DSL, 232–233\n    * Kubeflow Pipelines, 224–225, 229\n    * metadata, 226\n    * review question answers, 301–302, 305–306, 309\n    * TensorFlow Extended SDK, 93\n    * triggering schedules, 215–216\n    * Vertex AI Pipelines, 225–229\n      * scheduling, 232\n    * when to use, 93–94\n\n  * poll notifications, 95–96\n    * online prediction and, 95–96\n\n  * positive correlation, 24\n  * prebuilt containers, model training and, 163–165\n  * precomputing prediction, 204–207\n  * prediction, 74–75\n    * batch, 94\n      * input data, 212\n    * BigQuery ML, 285–286\n    * caching, architecture, 206\n    * deploy to Android, 76\n    * deploy to iOS devices, 76\n    * dynamic reference features, 203–204\n      * architecture, 205\n    * Edge TPU, 76\n    * lookup keys, 206–207\n    * machine types, 75–76\n    * ML Kit, 76\n    * offline, 94\n    * online\n      * A/B testing versions, 210–211\n      * asynchronous, 95–96\n      * endpoint setup, 207\n      * endpoints, undeploy, 211\n      * explanation requests, 212\n      * latency, 96\n      * making, 211\n      * model deployment, 207–209\n      * synchronous, 95\n    * precomputing, 204–207\n    * review question answers, 304–305, 308–309\n    * scaling, 75\n    * scaling prediction service, 200–203\n    * static reference features, 203–204\n      * architecture, 205\n    * TensorFlow Serving, 201–202\n    * triggering jobs, 215–216\n\n  * prediction classes, 9–10\n  * prediction logs\n    * access logging, 247\n    * container logging, 247\n    * request‐response logging, 248\n\n  * pretrained ML models, 60\n    * compared to others, 58–60\n    * Natural Language AI, 62–63\n    * review question answers, 302–304\n    * Speech‐to‐Text service, 63\n    * Text‐to‐Speech service, 64\n    * Translation AI, 63\n    * Video AI, 62\n    * Vision AI, 61–62\n\n  * pretraining data transformation, 40–41\n  * Private endpoints, Vertex AI, 110–111\n  * problem types, 6\n  * Public endpoint, Vertex AI, 110\n  * Pub/Sub, 146, 215\n  * Pub/Sub Lite, 146\n  * Pub/Sub Lite Spark connector, 149\n  * push notifications, online prediction and, 95–96",
        "subsections": {},
        "summary": "* **Machine Learning Fundamentals**\n    * _Prediction:_ Making predictions involves input data, model deployment, and scalability.\n    * *_Model Deployment:_ Deploying models to endpoints requires consideration of latency, synchronous vs. asynchronous, and triggering jobs.\n    * *_Data Preprocessing:_ Precomputing prediction involves transforming data for model training.\n\n* **Pipelines**\n    * _Cloud Composer_: A fully managed service for creating, managing, and deploying data pipelines.\n    * _Kubeflow Pipelines_: A managed pipeline service that integrates with TensorFlow Extended SDK.\n    * _Vertex AI Pipelines_: A pipeline service that enables continuous training and scheduling of models.\n\n* **Notifications**\n    * *_Poll Notifications:_ Online prediction involves receiving poll notifications for real-time updates.\n    * *_Push Notifications:_ Push notifications enable model updates without requiring manual redeployment."
      },
      "Q": {
        "content": "* quality of data, 24–27",
        "subsections": {},
        "summary": "* quality of data, 24–27"
      },
      "R": {
        "content": "* Random Forest algorithm, missing data, 32\n  * redeployment evaluation\n    * concept drift and, 178\n    * data changes trigger, 179\n    * data drift and, 178\n    * on demand, 179\n    * performance‐based trigger, 179\n    * periodic, 179\n    * when to retrain, 178–179\n\n  * regression, 7\n    * MAE (mean absolute error), 12\n    * RMSE (root‐mean‐squared error), 12\n    * RMSLE (root‐mean‐squared logarithmic error), 12\n    * structured data and, 7\n\n  * regularization, 134\n    * dropout, 136\n    * exploding gradients, 135\n    * L1, 135\n    * L2, 135\n    * losses, 136\n    * ReLU units, dead, 135\n    * vanishing gradients, 135–136\n\n  * reliability of data, 24–27\n  * request‐response logging, 248\n  * Retail AI, 68\n  * retraining\n    * concept drift and, 178\n    * data changes trigger, 179\n    * data drift and, 178\n    * on demand, 179\n    * models, 266–267\n    * performance‐based trigger, 179\n    * periodic, 179\n    * when to retrain, 178–179\n\n  * RMSE (root‐mean‐squared error), 12\n  * RMSLE (root‐mean‐squared logarithmic error), 12\n  * RNNs (recurrent neural networks), 127\n  * ROC (receiver operating characteristics), 11",
        "subsections": {},
        "summary": "* **Random Forest**: \n    * handles missing data and concept drift with deployment evaluation\n        * on-demand, periodic, performance-based triggers, and when to retrain\n    * applied in regression tasks with metrics like MAE, RMSE, and RMSLE\n    * affects reliability of data due to regularization techniques"
      },
      "S": {
        "content": "* SaaS (Software as a Service), 86\n  * scale‐invariance, 11\n  * scaling, 25–26\n    * prediction, 200–203\n    * z‐score, 26\n\n  * scatterplots, 22\n  * semi‐structured data, model training and, 145\n  * semi‐supervised learning, 7\n  * sensitive data, removing, 116–117\n  * Seq2seq+, 66\n  * server‐side encryption, 105\n  * services, 86\n  * skewed data, 25\n  * SMOTE (Synthetic Minority Oversampling Technique), 25\n  * solutions, 86, 311–313\n  * Speech‐to‐Text service, 63\n  * splitting data, 31\n  * SSL (semi‐supervised learning)\n    * limitations, 131\n    * need for, 131\n\n  * standard deviation, 23\n  * static reference features, 203–204\n    * architecture, 205\n\n  * statistics\n    * correlation\n      * negative, 24\n      * positive, 24\n      * zero, 24\n    * mean, 22\n      * variance, 23\n    * median, 22\n    * mode, 23\n    * outliers, detecting, 23\n    * standard deviation, 23\n\n  * streaming data, collecting, 146–147\n  * structured data\n    * categorical data, 41\n    * modeling training and, 145\n    * numeric data, 41\n    * regression and, 7\n\n  * supervised learning, 6\n  * synchronous training, 123\n  * system design\n    * Kubeflow DSL, 232–233\n    * review question answers, 309\n    * TFX (TensorFlow Extended), 234–235",
        "subsections": {},
        "summary": "* **Summary of Key Concepts**: \n  * SaaS, scaling, and data preparation\n  * Supervised learning, statistics, and data types (structured & semi-structured)\n  * Model training and deployment using frameworks like Kubeflow DSL and TFX\n\n    * **Key Models and Techniques**:\n      * Seq2seq+, SMOTE (Synthetic Minority Oversampling Technique), SSL (Semi-Supervised Learning)\n      * Regression, classification, categorical data analysis\n    * **Data Management**: Data splitting, server-side encryption, standard deviation calculation\n      * **Machine Learning Limitations**: need for semi-supervised learning"
      },
      "T": {
        "content": "* t‐SNE (t‐distributed stochastic neighbor embedding), 44\n  * Temporal Fusion Transformer, 66\n  * TensorFlow\n    * model serving, 200\n    * multiclass classification, 128\n    * orchestration, 92\n    * training strategies, 124–125\n\n  * TensorFlow Extended SDK, 93\n  * TensorFlow ModelServer, 201–202\n  * TensorFlow Profiler, 177\n  * TensorFlow Serving, 201–202\n  * TensorFlow Transform\n    * library, 49–51\n    * tf.data API, 49\n    * TFX (TensorFlow Extended), 49–51\n\n  * test datasets, 29\n  * testing, for performance, 214–215\n  * Text‐to‐Speech service, 64\n  * tf.data API\n    * tf.data.Dataset.cache, 49\n    * tf.data.Dataset.prefetch, 49\n\n  * TFDV (TensorFlow Data Validation), 20, 272\n    * APIs (application programming interfaces), 28\n    * exploratory data analysis phase, 28\n    * production pipeline phase, 28\n\n  * TFX (TensorFlow Extended), 27–28, 49–51\n    * system design and, 234–235\n\n  * time series data\n    * data leakage and, 33\n    * forecasting and, 8\n\n  * TLS (Transport Layer Security), 105\n  * tokenization, 113\n  * TPUs (Tensor Processing Units), 58\n  * training datasets, 29\n  * training jobs, Vertex AI, 111\n  * transfer learning, 130\n  * Translation AI, 63\n  * triggering prediction jobs, 215–216\n  * true negative, 45\n  * true positive, 44",
        "subsections": {},
        "summary": "* **TensorFlow**: \n    * _Model Serving_: 200 \n        - Orchestrated model deployment \n        - Multiclass classification \n    * _TFX (TensorFlow Extended)_: 49-51 \n        - System design and integration \n        - Library for data preparation and orchestration \n* **Key Topics**: \n    * *_Time Series Data_*: forecasting, data leakage\n    * *_Transfer Learning_*: 130 \n    * *_Tokenization_*: 113 \n* **Tools and Technologies** \n    * _tf.data API_: caching, prefetching"
      },
      "U": {
        "content": "* univariate analysis, data visualization, 20\n  * unstructured data, model training and, 145\n  * unsupervised learning, 6\n    * topic modeling, 6–7\n\n  * user‐managed notebook, Vertex AI Workbench, 151–153, 159–161",
        "subsections": {},
        "summary": "* **Univariate Analysis**: statistical analysis of individual variables\n* _Data Visualization_: graphical representation of data for understanding and insight\n* \n* **Vertex AI Workbench**: user-managed notebook for model training and unsupervised learning"
      },
      "V": {
        "content": "* validation datasets, 29\n    * TFDV (TensorFlow Data Validation), 272\n\n  * versioning models, 267–268\n  * Vertex AI\n    * APIs, 86\n    * AutoML, 86\n    * batch predictions, 212–213\n    * BigQuery ML comparison, 289\n    * data bias and fairness, 193–194\n    * debugging shell, 272\n    * endpoints, 110–111\n    * DP (differential privacy) and, 112\n    * example‐based explanations, 193\n    * experiments, 252–253\n      * review question answers, 310–311\n    * federated learning, 112\n    * IAM roles and, 106\n    * interpretability term, 189\n    * ML solution readiness, 194–195\n    * model monitoring, 243–244\n      * drift, 244–245\n      * input schemas, 245–247\n      * skew, 244–245\n    * model training\n      * custom containers, 166–168\n      * datasets, 163\n      * prebuilt containers, 163–165\n      * workflow, 162\n    * permissions\n      * Access Transparency logs, 271\n      * Cloud Audit logs, 271\n      * service accounts, custom, 270\n    * platform, 86\n    * training jobs, 111\n    * VPC network, 110\n    * Workbench, 86, 109–110\n\n  * Vertex AI AutoML, 60, 87\n  * Vertex AI Feature Store, 89\n    * data model, 269\n    * ingestion, 269–270\n    * serving, 269–270\n    * solution, 268–269\n\n  * Vertex AI Jupyter Notebooks, 88\n  * Vertex AI Model Monitoring, retraining, 266–267\n  * Vertex AI Pipelines, 28, 93–94, 225–229\n    * scheduling, 232\n\n  * Vertex AI tables, 64–65\n  * Vertex AI Workbench, 109–110\n    * IAM permissions, 106–108\n    * managed notebook, 151–159\n    * user‐managed notebook, 151–153, 159–161\n\n  * Vertex Explainable AI, 189–190\n    * explainability term, 189\n    * explanations\n      * batch, 195\n      * online, 195\n    * feature attributions\n      * differentiable models, 192\n      * integrated gradients method, 191, 192\n      * nondifferentiable models, 192\n      * Sampled Shapley method, 190–192\n      * XRAI, 191, 192\n    * feature importance, 189\n    * global explainability, 188\n    * local explainability, 188\n    * local kernel explanations, 195\n    * review question answers, 308\n\n  * Vertex ML, metadata, 249–250\n    * artifacts, 249\n    * context, 249\n    * events, 250\n    * execution, 249\n    * metadataschema, 250–252\n    * Vertex AI Pipelines, 252\n\n  * Vertex ML metadata, 92\n  * Video AI, 62\n  * Vision AI, 61–62\n  * VPC network, Vertex AI, 110",
        "subsections": {},
        "summary": "* **Vertex AI Overview**: \n  * A fully managed machine learning service for building, deploying and managing machine learning models\n  * Provides APIs, AutoML, batch predictions, and more\n\n* **Key Features**:\n  * *_Data Management_*: Vertex AI Feature Store, data ingestion, serving, solution\n  * *_Model Monitoring_*: model training, permissions, platform, training jobs\n  * *_Explainability_*: feature attributions, explanations, feature importance, review question answers"
      },
      "W": {
        "content": "* WIT (What‐If Tool), 177–178\n  * Workflow, 216\n  * workpool tasks, distributed training, 168–169",
        "subsections": {},
        "summary": "* WIT (What‐If Tool), 177–178\n  * Workflow, 216\n  * workpool tasks, distributed training, 168–169"
      },
      "Z": {
        "content": "* z‐score, 26\n  * zero correlation, 24",
        "subsections": {},
        "summary": "* z‐score, 26\n  * zero correlation, 24"
      }
    },
    "summary": ""
  },
  "Online Test Bank": {
    "content": "",
    "subsections": {
      "Register and Access the Online Test Bank": {
        "content": "To register your book and get access to the online test bank, follow these steps:\n\n  1. **1.** Go to `www.wiley.com/go/sybextestprep`. You'll see the “How to Register Your Book for Online Access” instructions.\n  2. **2.** Click \"here to register\" and then select your book from the list.\n  3. **3.** Complete the required registration information, including answering the security verification to prove book ownership. You will be emailed a pin code.\n  4. **4.** Follow the directions in the email or go to `www.wiley.com/go/sybextestprep`.\n  5. **5.** Find your book on that page and click the “Register or Login” link with it. Then enter the pin code you received and click the “Activate PIN” button.\n  6. **6.** On the Create an Account or Login page, enter your username and password, and click Login or, if you don't have an account already, create a new account.\n  7. **7.** At this point, you should be in the test bank site with your new test bank listed at the top of the page. If you do not see it there, please refresh the page or log out and log back in.",
        "subsections": {},
        "summary": "* _Register your book for online access_:\n  * Go to `www.wiley.com/go/sybextestprep` and select your book from the list\n  * Complete registration information and answer security verification\n  * Enter pin code received via email to activate account\n* _Access test bank:_\n  * Log in with new username and password\n  * Refresh page if test bank not visible, or log out and log back in"
      }
    },
    "summary": ""
  },
  "WILEY END USER LICENSE AGREEMENT": {
    "content": "",
    "subsections": {},
    "summary": ""
  }
}